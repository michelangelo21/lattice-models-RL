{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IsingModelEnv' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/michelangelo/repos/xymodel_gym/rllibtut.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/michelangelo/repos/xymodel_gym/rllibtut.ipynb#ch0000000vscode-remote?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCartPole-v0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/michelangelo/repos/xymodel_gym/rllibtut.ipynb#ch0000000vscode-remote?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mgym_xymodel:xymodel-v0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/michelangelo/repos/xymodel_gym/rllibtut.ipynb#ch0000000vscode-remote?line=3'>4</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mgym_xymodel:isingmodel-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/michelangelo/repos/xymodel_gym/rllibtut.ipynb#ch0000000vscode-remote?line=5'>6</a>\u001b[0m env\u001b[39m.\u001b[39maction_space\n",
      "File \u001b[0;32m~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py:235\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=233'>234</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=234'>235</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mmake(\u001b[39mid\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py:129\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=126'>127</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mMaking new env: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, path)\n\u001b[1;32m    <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=127'>128</a>\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspec(path)\n\u001b[0;32m--> <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=128'>129</a>\u001b[0m env \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39;49mmake(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=129'>130</a>\u001b[0m \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py:90\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=87'>88</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=88'>89</a>\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point)\n\u001b[0;32m---> <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=89'>90</a>\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs)\n\u001b[1;32m     <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=91'>92</a>\u001b[0m \u001b[39m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/repos/xymodel_gym/.venv/lib/python3.8/site-packages/gym/envs/registration.py?line=92'>93</a>\u001b[0m spec \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/repos/xymodel_gym/gym-xymodel/gym_xymodel/envs/isingmodel_env.py:15\u001b[0m, in \u001b[0;36mIsingModelEnv.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///~/repos/xymodel_gym/gym-xymodel/gym_xymodel/envs/isingmodel_env.py?line=12'>13</a>\u001b[0m \u001b[39m# states are -1 or 1\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/repos/xymodel_gym/gym-xymodel/gym_xymodel/envs/isingmodel_env.py?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlattice \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39msample() \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='file:///~/repos/xymodel_gym/gym-xymodel/gym_xymodel/envs/isingmodel_env.py?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menergy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_energy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[1;32m     <a href='file:///~/repos/xymodel_gym/gym-xymodel/gym_xymodel/envs/isingmodel_env.py?line=15'>16</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39mMultiDiscrete(SIDE_LENGTH, SIDE_LENGTH)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IsingModelEnv' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.make(\"gym_xymodel:xymodel-v0\")\n",
    "env = gym.make(\"gym_xymodel:isingmodel-v0\")\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 -4.439402371644974 -2.332258015871048 False {}\n",
      "step 1 -9.505510360002518 5.066107988357544 False {}\n",
      "step 2 -9.337292492389679 -0.16821786761283875 False {}\n",
      "step 3 -11.835821062326431 2.4985285699367523 False {}\n",
      "step 4 -9.863821566104889 -1.9719994962215424 False {}\n",
      "step 5 -8.872738033533096 -0.9910835325717926 False {}\n",
      "step 6 -7.361022233963013 -1.5117157995700836 False {}\n",
      "step 7 -6.922528386116028 -0.43849384784698486 False {}\n",
      "step 8 -9.262925148010254 2.340396761894226 False {}\n",
      "step 9 -7.991807609796524 -1.2711175382137299 False {}\n",
      "step 10 -11.338801860809326 3.346994251012802 False {}\n",
      "step 11 -4.845211863517761 -6.493589997291565 False {}\n",
      "step 12 -8.611982971429825 3.7667711079120636 False {}\n",
      "step 13 -11.165527582168579 2.5535446107387543 False {}\n",
      "step 14 -8.778212815523148 -2.3873147666454315 False {}\n",
      "step 15 -8.039074957370758 -0.7391378581523895 False {}\n",
      "step 16 -3.647136852145195 -4.391938105225563 False {}\n",
      "step 17 -9.214866518974304 5.567729666829109 False {}\n",
      "step 18 -7.825198724865913 -1.3896677941083908 False {}\n",
      "step 19 -9.51611503958702 1.6909163147211075 False {}\n"
     ]
    }
   ],
   "source": [
    "# returns an initial observation\n",
    "env.reset()\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "  observation, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "  energy = env.energy\n",
    "\n",
    "  print(\"step\", i, energy, reward, done, info)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"env\": \"CartPole-v0\",\n",
    "  # Change the following line to `“framework”: “tf”` to use tensorflow\n",
    "  \"framework\": \"torch\",\n",
    "  \"model\": {\n",
    "    \"fcnet_hiddens\": [32],\n",
    "    \"fcnet_activation\": \"linear\",\n",
    "  },\n",
    "}\n",
    "stop = {\"episode_reward_mean\": 195}\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "  num_gpus=1,\n",
    "  num_cpus=3,\n",
    "  include_dashboard=False,\n",
    "  ignore_reinit_error=True,\n",
    "  log_to_driver=False,\n",
    ")\n",
    "# execute training \n",
    "analysis = ray.tune.run(\n",
    "  \"PPO\",\n",
    "  config=config,\n",
    "  stop=stop,\n",
    "  checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00122336, 0.93029538, 0.39499548],\n",
       "       [0.31307696, 0.36893683, 0.90384494],\n",
       "       [0.9898301 , 0.11389949, 0.11021905]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.rand(3,3).flatten().reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:28:28 (running for 00:00:00.14)<br>Memory usage on this node: 2.8/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:28:33 (running for 00:00:05.24)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:28:38 (running for 00:00:10.24)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.204641029238701\n",
      "  episode_reward_mean: -0.1339656263589859\n",
      "  episode_reward_min: -6.808685839176178\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.772907400643954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01680239581065478\n",
      "          policy_loss: -0.04244272310326817\n",
      "          total_loss: 8.415939648946127\n",
      "          vf_explained_var: 0.06189027255581271\n",
      "          vf_loss: 8.455021871033535\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.436363636363637\n",
      "    ram_util_percent: 33.63636363636365\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08142000433804095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07048319245147323\n",
      "    mean_inference_ms: 0.7979270519464389\n",
      "    mean_raw_obs_processing_ms: 0.06803591688652746\n",
      "  time_since_restore: 7.326717853546143\n",
      "  time_this_iter_s: 7.326717853546143\n",
      "  time_total_s: 7.326717853546143\n",
      "  timers:\n",
      "    learn_throughput: 764.036\n",
      "    learn_time_ms: 5235.356\n",
      "    load_throughput: 16644063.492\n",
      "    load_time_ms: 0.24\n",
      "    sample_throughput: 1914.418\n",
      "    sample_time_ms: 2089.408\n",
      "    update_time_ms: 1.231\n",
      "  timestamp: 1643383720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:28:44 (running for 00:00:15.81)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.32672</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-0.133966</td><td style=\"text-align: right;\">             5.20464</td><td style=\"text-align: right;\">            -6.80869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-28-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.427941918373108\n",
      "  episode_reward_mean: -0.10221910402178765\n",
      "  episode_reward_min: -6.808685839176178\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 40\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.775383644719277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016763993811208567\n",
      "          policy_loss: -0.0365477727731109\n",
      "          total_loss: 7.180408461632267\n",
      "          vf_explained_var: 0.278431837661292\n",
      "          vf_loss: 7.213603432973226\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.69\n",
      "    ram_util_percent: 33.650000000000006\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07956767194690376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.069157979872249\n",
      "    mean_inference_ms: 0.7810416489586093\n",
      "    mean_raw_obs_processing_ms: 0.06645286648375544\n",
      "  time_since_restore: 14.478800058364868\n",
      "  time_this_iter_s: 7.152082204818726\n",
      "  time_total_s: 14.478800058364868\n",
      "  timers:\n",
      "    learn_throughput: 762.997\n",
      "    learn_time_ms: 5242.485\n",
      "    load_throughput: 12880780.038\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 865.194\n",
      "    sample_time_ms: 4623.242\n",
      "    update_time_ms: 1.516\n",
      "  timestamp: 1643383727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:28:49 (running for 00:00:20.98)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.4788</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-0.102219</td><td style=\"text-align: right;\">             5.42794</td><td style=\"text-align: right;\">            -6.80869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:28:54 (running for 00:00:25.99)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.4788</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-0.102219</td><td style=\"text-align: right;\">             5.42794</td><td style=\"text-align: right;\">            -6.80869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.858522653579712\n",
      "  episode_reward_mean: 0.004162131746610006\n",
      "  episode_reward_min: -7.026950359344482\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.726002213262742\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017928436028628376\n",
      "          policy_loss: -0.04008667215114079\n",
      "          total_loss: 6.037830718358358\n",
      "          vf_explained_var: 0.39835154741041123\n",
      "          vf_loss: 6.074331706570041\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.48181818181818\n",
      "    ram_util_percent: 33.67272727272728\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07933556035155843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06901691760889105\n",
      "    mean_inference_ms: 0.7796022565412524\n",
      "    mean_raw_obs_processing_ms: 0.06632401787199463\n",
      "  time_since_restore: 21.756299257278442\n",
      "  time_this_iter_s: 7.277499198913574\n",
      "  time_total_s: 21.756299257278442\n",
      "  timers:\n",
      "    learn_throughput: 765.522\n",
      "    learn_time_ms: 5225.195\n",
      "    load_throughput: 14193922.166\n",
      "    load_time_ms: 0.282\n",
      "    sample_throughput: 722.74\n",
      "    sample_time_ms: 5534.492\n",
      "    update_time_ms: 1.428\n",
      "  timestamp: 1643383734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:28:59 (running for 00:00:31.28)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         21.7563</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">0.00416213</td><td style=\"text-align: right;\">             5.85852</td><td style=\"text-align: right;\">            -7.02695</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.858522653579712\n",
      "  episode_reward_mean: -0.015633214637637137\n",
      "  episode_reward_min: -7.026950359344482\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 80\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.627331030240622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019728538769566296\n",
      "          policy_loss: -0.04468121806820554\n",
      "          total_loss: 4.298126762913119\n",
      "          vf_explained_var: 0.5052051310898156\n",
      "          vf_loss: 4.338862266079072\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.98\n",
      "    ram_util_percent: 33.7\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07958827911033828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06934120496883042\n",
      "    mean_inference_ms: 0.7828059202064552\n",
      "    mean_raw_obs_processing_ms: 0.06660109319770219\n",
      "  time_since_restore: 29.34113621711731\n",
      "  time_this_iter_s: 7.584836959838867\n",
      "  time_total_s: 29.34113621711731\n",
      "  timers:\n",
      "    learn_throughput: 759.415\n",
      "    learn_time_ms: 5267.213\n",
      "    load_throughput: 14466234.964\n",
      "    load_time_ms: 0.277\n",
      "    sample_throughput: 666.533\n",
      "    sample_time_ms: 6001.201\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1643383742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:05 (running for 00:00:36.89)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         29.3411</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-0.0156332</td><td style=\"text-align: right;\">             5.85852</td><td style=\"text-align: right;\">            -7.02695</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.645511001348495\n",
      "  episode_reward_mean: -0.01494738608598709\n",
      "  episode_reward_min: -9.161430284380913\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.57543722173219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019327899680863878\n",
      "          policy_loss: -0.03985124720080245\n",
      "          total_loss: 5.874101623668466\n",
      "          vf_explained_var: 0.5507694919904073\n",
      "          vf_loss: 5.910087300628744\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.509090909090908\n",
      "    ram_util_percent: 33.69090909090909\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07995391374432025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06976458319494779\n",
      "    mean_inference_ms: 0.7868543094012611\n",
      "    mean_raw_obs_processing_ms: 0.06691766633934129\n",
      "  time_since_restore: 36.86890959739685\n",
      "  time_this_iter_s: 7.527773380279541\n",
      "  time_total_s: 36.86890959739685\n",
      "  timers:\n",
      "    learn_throughput: 757.867\n",
      "    learn_time_ms: 5277.968\n",
      "    load_throughput: 14475596.204\n",
      "    load_time_ms: 0.276\n",
      "    sample_throughput: 632.38\n",
      "    sample_time_ms: 6325.306\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1643383749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:10 (running for 00:00:42.44)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         36.8689</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-0.0149474</td><td style=\"text-align: right;\">             9.64551</td><td style=\"text-align: right;\">            -9.16143</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:15 (running for 00:00:47.44)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         36.8689</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-0.0149474</td><td style=\"text-align: right;\">             9.64551</td><td style=\"text-align: right;\">            -9.16143</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.645511001348495\n",
      "  episode_reward_mean: 0.019329703897237777\n",
      "  episode_reward_min: -12.1830715239048\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.530341674435522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018206635643580625\n",
      "          policy_loss: -0.03192066641684661\n",
      "          total_loss: 4.704513042972934\n",
      "          vf_explained_var: 0.5882823048740304\n",
      "          vf_loss: 4.7327923790101085\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.85454545454546\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025044797453594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07019840980708925\n",
      "    mean_inference_ms: 0.790611124425506\n",
      "    mean_raw_obs_processing_ms: 0.06718259611319513\n",
      "  time_since_restore: 44.580538272857666\n",
      "  time_this_iter_s: 7.711628675460815\n",
      "  time_total_s: 44.580538272857666\n",
      "  timers:\n",
      "    learn_throughput: 754.957\n",
      "    learn_time_ms: 5298.312\n",
      "    load_throughput: 14590998.116\n",
      "    load_time_ms: 0.274\n",
      "    sample_throughput: 611.016\n",
      "    sample_time_ms: 6546.475\n",
      "    update_time_ms: 1.384\n",
      "  timestamp: 1643383757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:21 (running for 00:00:53.17)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         44.5805</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">0.0193297</td><td style=\"text-align: right;\">             9.64551</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.645511001348495\n",
      "  episode_reward_mean: -0.024845350086688995\n",
      "  episode_reward_min: -12.1830715239048\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.488862453993931\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017184530518383725\n",
      "          policy_loss: -0.04186401437287049\n",
      "          total_loss: 3.752931820961737\n",
      "          vf_explained_var: 0.6469292412522019\n",
      "          vf_loss: 3.791358918528403\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.65\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08175981213922257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0715141003793062\n",
      "    mean_inference_ms: 0.8060536087247209\n",
      "    mean_raw_obs_processing_ms: 0.06842209586990167\n",
      "  time_since_restore: 52.927090883255005\n",
      "  time_this_iter_s: 8.346552610397339\n",
      "  time_total_s: 52.927090883255005\n",
      "  timers:\n",
      "    learn_throughput: 745.146\n",
      "    learn_time_ms: 5368.075\n",
      "    load_throughput: 14211097.773\n",
      "    load_time_ms: 0.281\n",
      "    sample_throughput: 592.466\n",
      "    sample_time_ms: 6751.443\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1643383765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:26 (running for 00:00:58.54)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         52.9271</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.0248454</td><td style=\"text-align: right;\">             9.64551</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:31 (running for 00:01:03.55)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         52.9271</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.0248454</td><td style=\"text-align: right;\">             9.64551</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.645511001348495\n",
      "  episode_reward_mean: -0.04044708847999573\n",
      "  episode_reward_min: -12.1830715239048\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.407544857968567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01869517151160799\n",
      "          policy_loss: -0.06049076790010096\n",
      "          total_loss: 4.068146394657832\n",
      "          vf_explained_var: 0.6502445426679426\n",
      "          vf_loss: 4.124898126689336\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.725\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08335021008589585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07286330673587484\n",
      "    mean_inference_ms: 0.822234004893465\n",
      "    mean_raw_obs_processing_ms: 0.06966971199996057\n",
      "  time_since_restore: 61.17565107345581\n",
      "  time_this_iter_s: 8.248560190200806\n",
      "  time_total_s: 61.17565107345581\n",
      "  timers:\n",
      "    learn_throughput: 738.911\n",
      "    learn_time_ms: 5413.368\n",
      "    load_throughput: 14267856.702\n",
      "    load_time_ms: 0.28\n",
      "    sample_throughput: 575.622\n",
      "    sample_time_ms: 6949.002\n",
      "    update_time_ms: 1.391\n",
      "  timestamp: 1643383774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:37 (running for 00:01:08.81)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         61.1757</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.0404471</td><td style=\"text-align: right;\">             9.64551</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:42 (running for 00:01:13.82)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         61.1757</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.0404471</td><td style=\"text-align: right;\">             9.64551</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.242419123649597\n",
      "  episode_reward_mean: 0.004037189781665802\n",
      "  episode_reward_min: -12.1830715239048\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.422831609172206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018861326599064997\n",
      "          policy_loss: -0.040688292704202156\n",
      "          total_loss: 4.628114406395984\n",
      "          vf_explained_var: 0.6618065834686321\n",
      "          vf_loss: 4.665030417391049\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.633333333333336\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08470515831885177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07398620453129315\n",
      "    mean_inference_ms: 0.8359621424312742\n",
      "    mean_raw_obs_processing_ms: 0.0707039210675739\n",
      "  time_since_restore: 69.5589542388916\n",
      "  time_this_iter_s: 8.383303165435791\n",
      "  time_total_s: 69.5589542388916\n",
      "  timers:\n",
      "    learn_throughput: 729.124\n",
      "    learn_time_ms: 5486.033\n",
      "    load_throughput: 14139427.287\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 565.494\n",
      "    sample_time_ms: 7073.462\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1643383782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:47 (running for 00:01:19.22)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          69.559</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">0.00403719</td><td style=\"text-align: right;\">             10.2424</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.242419123649597\n",
      "  episode_reward_mean: 0.017169919908046723\n",
      "  episode_reward_min: -12.1830715239048\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.328236595276863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020928508421871772\n",
      "          policy_loss: -0.043510415205251306\n",
      "          total_loss: 3.75811550489036\n",
      "          vf_explained_var: 0.7062042695219799\n",
      "          vf_loss: 3.797440226872762\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.916666666666668\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08600851556708389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07500989105643743\n",
      "    mean_inference_ms: 0.8491505680807859\n",
      "    mean_raw_obs_processing_ms: 0.07171215590232735\n",
      "  time_since_restore: 77.81892085075378\n",
      "  time_this_iter_s: 8.259966611862183\n",
      "  time_total_s: 77.81892085075378\n",
      "  timers:\n",
      "    learn_throughput: 724.902\n",
      "    learn_time_ms: 5517.99\n",
      "    load_throughput: 13487592.25\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 553.983\n",
      "    sample_time_ms: 7220.439\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1643383790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:52 (running for 00:01:24.50)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         77.8189</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">0.0171699</td><td style=\"text-align: right;\">             10.2424</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:29:57 (running for 00:01:29.51)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         77.8189</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">0.0171699</td><td style=\"text-align: right;\">             10.2424</td><td style=\"text-align: right;\">            -12.1831</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.242419123649597\n",
      "  episode_reward_mean: -0.003986919969320297\n",
      "  episode_reward_min: -7.643385529518127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.223053274872482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01776788161869951\n",
      "          policy_loss: -0.03380171549055846\n",
      "          total_loss: 2.7934363724083027\n",
      "          vf_explained_var: 0.7457440508950142\n",
      "          vf_loss: 2.821907723206346\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.5\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08716635526456287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07592425758194377\n",
      "    mean_inference_ms: 0.8610502950646227\n",
      "    mean_raw_obs_processing_ms: 0.07261772824054774\n",
      "  time_since_restore: 85.89635729789734\n",
      "  time_this_iter_s: 8.077436447143555\n",
      "  time_total_s: 85.89635729789734\n",
      "  timers:\n",
      "    learn_throughput: 720.032\n",
      "    learn_time_ms: 5555.307\n",
      "    load_throughput: 13301526.996\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 510.128\n",
      "    sample_time_ms: 7841.177\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1643383798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:02 (running for 00:01:34.60)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         85.8964</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.00398692</td><td style=\"text-align: right;\">             10.2424</td><td style=\"text-align: right;\">            -7.64339</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-30-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.242419123649597\n",
      "  episode_reward_mean: 0.017588135600090028\n",
      "  episode_reward_min: -9.0862475335598\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.074080389289445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017948108181172467\n",
      "          policy_loss: -0.054574738984667165\n",
      "          total_loss: 5.3033272723997795\n",
      "          vf_explained_var: 0.7052378272497526\n",
      "          vf_loss: 5.352517583806027\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.490909090909096\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08786538490296208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07650725258011296\n",
      "    mean_inference_ms: 0.8680717134292177\n",
      "    mean_raw_obs_processing_ms: 0.07318605823284445\n",
      "  time_since_restore: 93.59041738510132\n",
      "  time_this_iter_s: 7.6940600872039795\n",
      "  time_total_s: 93.59041738510132\n",
      "  timers:\n",
      "    learn_throughput: 718.031\n",
      "    learn_time_ms: 5570.792\n",
      "    load_throughput: 13784583.025\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 505.237\n",
      "    sample_time_ms: 7917.077\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1643383806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:08 (running for 00:01:40.31)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         93.5904</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">0.0175881</td><td style=\"text-align: right;\">             10.2424</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:13 (running for 00:01:45.32)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         93.5904</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">0.0175881</td><td style=\"text-align: right;\">             10.2424</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-30-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.242419123649597\n",
      "  episode_reward_mean: 0.006119828522205353\n",
      "  episode_reward_min: -9.0862475335598\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 12.014271460297287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01813955418476467\n",
      "          policy_loss: -0.06491488618876344\n",
      "          total_loss: 3.1855176903868236\n",
      "          vf_explained_var: 0.7575308379947499\n",
      "          vf_loss: 3.244990706700151\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.69090909090909\n",
      "    ram_util_percent: 33.70909090909091\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08830026975732581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0769271470142881\n",
      "    mean_inference_ms: 0.8722782325956606\n",
      "    mean_raw_obs_processing_ms: 0.07355650996785269\n",
      "  time_since_restore: 101.24840593338013\n",
      "  time_this_iter_s: 7.657988548278809\n",
      "  time_total_s: 101.24840593338013\n",
      "  timers:\n",
      "    learn_throughput: 716.597\n",
      "    learn_time_ms: 5581.934\n",
      "    load_throughput: 13602412.843\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 502.545\n",
      "    sample_time_ms: 7959.479\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1643383814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:19 (running for 00:01:50.99)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.248</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">0.00611983</td><td style=\"text-align: right;\">             10.2424</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-30-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.026087768375874\n",
      "  episode_reward_mean: -0.00681112140417099\n",
      "  episode_reward_min: -9.0862475335598\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.888056057755666\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01743034233221938\n",
      "          policy_loss: -0.0402987979865703\n",
      "          total_loss: 2.7237793114236606\n",
      "          vf_explained_var: 0.7881667593474029\n",
      "          vf_loss: 2.7588490224653675\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.425\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08881397023229542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07736318971987817\n",
      "    mean_inference_ms: 0.8771747235110768\n",
      "    mean_raw_obs_processing_ms: 0.07396718523060483\n",
      "  time_since_restore: 109.39415884017944\n",
      "  time_this_iter_s: 8.145752906799316\n",
      "  time_total_s: 109.39415884017944\n",
      "  timers:\n",
      "    learn_throughput: 713.744\n",
      "    learn_time_ms: 5604.249\n",
      "    load_throughput: 13569407.959\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 499.71\n",
      "    sample_time_ms: 8004.638\n",
      "    update_time_ms: 1.439\n",
      "  timestamp: 1643383822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:24 (running for 00:01:56.16)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         109.394</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.00681112</td><td style=\"text-align: right;\">             10.0261</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:29 (running for 00:02:01.16)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         109.394</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.00681112</td><td style=\"text-align: right;\">             10.0261</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-30-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.026087768375874\n",
      "  episode_reward_mean: -0.0008428671956062317\n",
      "  episode_reward_min: -9.0862475335598\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.827138246515746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019278373648487678\n",
      "          policy_loss: -0.056098486637316085\n",
      "          total_loss: 3.0297139214572084\n",
      "          vf_explained_var: 0.7967384697288595\n",
      "          vf_loss: 3.0800289063043493\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.141666666666666\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08917388312280383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.077691524221378\n",
      "    mean_inference_ms: 0.8808391448399704\n",
      "    mean_raw_obs_processing_ms: 0.07426646895411765\n",
      "  time_since_restore: 117.65398621559143\n",
      "  time_this_iter_s: 8.259827375411987\n",
      "  time_total_s: 117.65398621559143\n",
      "  timers:\n",
      "    learn_throughput: 706.564\n",
      "    learn_time_ms: 5661.204\n",
      "    load_throughput: 13514754.31\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 497.309\n",
      "    sample_time_ms: 8043.289\n",
      "    update_time_ms: 1.473\n",
      "  timestamp: 1643383830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:34 (running for 00:02:06.44)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         117.654</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-0.000842867</td><td style=\"text-align: right;\">             10.0261</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.403008162975311\n",
      "  episode_reward_mean: 0.02397608369588852\n",
      "  episode_reward_min: -9.0862475335598\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.764107676475279\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01864290871785822\n",
      "          policy_loss: -0.021952989302896044\n",
      "          total_loss: 3.006941024206018\n",
      "          vf_explained_var: 0.7994249072767073\n",
      "          vf_loss: 3.0233011357886816\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.150000000000002\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08944402480907397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07796074260698642\n",
      "    mean_inference_ms: 0.8837406297407527\n",
      "    mean_raw_obs_processing_ms: 0.07448178882076409\n",
      "  time_since_restore: 125.97158145904541\n",
      "  time_this_iter_s: 8.31759524345398\n",
      "  time_total_s: 125.97158145904541\n",
      "  timers:\n",
      "    learn_throughput: 700.776\n",
      "    learn_time_ms: 5707.96\n",
      "    load_throughput: 13461619.193\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 492.949\n",
      "    sample_time_ms: 8114.438\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1643383839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:40 (running for 00:02:11.78)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         125.972</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">0.0239761</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:45 (running for 00:02:16.79)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         125.972</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">0.0239761</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -9.08625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.403008162975311\n",
      "  episode_reward_mean: 0.032963487505912784\n",
      "  episode_reward_min: -7.940320432186127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.74296819727908\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01701387511346835\n",
      "          policy_loss: -0.04788137699867929\n",
      "          total_loss: 1.753679770551702\n",
      "          vf_explained_var: 0.8457145878704645\n",
      "          vf_loss: 1.7964569762829812\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.2\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08974451826611282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07825889773414868\n",
      "    mean_inference_ms: 0.8870639490833354\n",
      "    mean_raw_obs_processing_ms: 0.07472857891623513\n",
      "  time_since_restore: 134.54834032058716\n",
      "  time_this_iter_s: 8.576758861541748\n",
      "  time_total_s: 134.54834032058716\n",
      "  timers:\n",
      "    learn_throughput: 696.365\n",
      "    learn_time_ms: 5744.111\n",
      "    load_throughput: 13684515.498\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 490.912\n",
      "    sample_time_ms: 8148.093\n",
      "    update_time_ms: 1.448\n",
      "  timestamp: 1643383847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:50 (running for 00:02:22.38)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         134.548</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">0.0329635</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -7.94032</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:30:55 (running for 00:02:27.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         134.548</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">0.0329635</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -7.94032</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.403008162975311\n",
      "  episode_reward_mean: -0.06404776394367218\n",
      "  episode_reward_min: -7.940320432186127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.73347110235563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016923773634235098\n",
      "          policy_loss: -0.06652712193768351\n",
      "          total_loss: 2.0421897851651716\n",
      "          vf_explained_var: 0.8480549404698033\n",
      "          vf_loss: 2.103639782628705\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.958333333333336\n",
      "    ram_util_percent: 33.73333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0900438064149992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07855823851284455\n",
      "    mean_inference_ms: 0.8904295276673092\n",
      "    mean_raw_obs_processing_ms: 0.07496467131577358\n",
      "  time_since_restore: 143.28708481788635\n",
      "  time_this_iter_s: 8.738744497299194\n",
      "  time_total_s: 143.28708481788635\n",
      "  timers:\n",
      "    learn_throughput: 689.567\n",
      "    learn_time_ms: 5800.743\n",
      "    load_throughput: 13661115.544\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 489.209\n",
      "    sample_time_ms: 8176.464\n",
      "    update_time_ms: 1.463\n",
      "  timestamp: 1643383856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:01 (running for 00:02:33.15)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         143.287</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-0.0640478</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -7.94032</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-31-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.403008162975311\n",
      "  episode_reward_mean: -0.02830566018819809\n",
      "  episode_reward_min: -7.940320432186127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.618066836941626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018584773795999567\n",
      "          policy_loss: -0.05300107599206028\n",
      "          total_loss: 2.0917980161084926\n",
      "          vf_explained_var: 0.8500385724729107\n",
      "          vf_loss: 2.1392236691008333\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.849999999999998\n",
      "    ram_util_percent: 33.73571428571429\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09043360891413775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07890987376807213\n",
      "    mean_inference_ms: 0.8945948599375204\n",
      "    mean_raw_obs_processing_ms: 0.0752886991525755\n",
      "  time_since_restore: 152.62604594230652\n",
      "  time_this_iter_s: 9.338961124420166\n",
      "  time_total_s: 152.62604594230652\n",
      "  timers:\n",
      "    learn_throughput: 684.724\n",
      "    learn_time_ms: 5841.766\n",
      "    load_throughput: 13612345.639\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 482.635\n",
      "    sample_time_ms: 8287.843\n",
      "    update_time_ms: 1.45\n",
      "  timestamp: 1643383865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:06 (running for 00:02:38.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         152.626</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-0.0283057</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -7.94032</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:11 (running for 00:02:43.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         152.626</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-0.0283057</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -7.94032</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-31-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.403008162975311\n",
      "  episode_reward_mean: -0.06674158096313476\n",
      "  episode_reward_min: -10.520575910806656\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.461957405459497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018410854475012284\n",
      "          policy_loss: -0.0419752469756991\n",
      "          total_loss: 4.201826137336352\n",
      "          vf_explained_var: 0.800153935468325\n",
      "          vf_loss: 4.238278140688455\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.500000000000004\n",
      "    ram_util_percent: 33.708333333333336\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09082470568926145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07925850913827633\n",
      "    mean_inference_ms: 0.8985279929060116\n",
      "    mean_raw_obs_processing_ms: 0.07561930231706974\n",
      "  time_since_restore: 160.90447092056274\n",
      "  time_this_iter_s: 8.278424978256226\n",
      "  time_total_s: 160.90447092056274\n",
      "  timers:\n",
      "    learn_throughput: 683.824\n",
      "    learn_time_ms: 5849.459\n",
      "    load_throughput: 13965883.626\n",
      "    load_time_ms: 0.286\n",
      "    sample_throughput: 480.58\n",
      "    sample_time_ms: 8323.273\n",
      "    update_time_ms: 1.44\n",
      "  timestamp: 1643383874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:17 (running for 00:02:48.82)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         160.904</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-0.0667416</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -10.5206</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:22 (running for 00:02:53.82)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         160.904</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-0.0667416</td><td style=\"text-align: right;\">              10.403</td><td style=\"text-align: right;\">            -10.5206</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-31-24\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.487907528877258\n",
      "  episode_reward_mean: -0.09662881195545196\n",
      "  episode_reward_min: -12.768159031867981\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.473939864866194\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01868339071843763\n",
      "          policy_loss: -0.04008005404023714\n",
      "          total_loss: 3.54682139763909\n",
      "          vf_explained_var: 0.8370783916083715\n",
      "          vf_loss: 3.5812964453492113\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.67142857142857\n",
      "    ram_util_percent: 33.73571428571428\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0915104258874939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07979876510333818\n",
      "    mean_inference_ms: 0.9054935663527257\n",
      "    mean_raw_obs_processing_ms: 0.076197205410287\n",
      "  time_since_restore: 170.85230016708374\n",
      "  time_this_iter_s: 9.947829246520996\n",
      "  time_total_s: 170.85230016708374\n",
      "  timers:\n",
      "    learn_throughput: 672.373\n",
      "    learn_time_ms: 5949.076\n",
      "    load_throughput: 13673362.673\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 475.139\n",
      "    sample_time_ms: 8418.589\n",
      "    update_time_ms: 1.458\n",
      "  timestamp: 1643383884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:28 (running for 00:02:59.79)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         170.852</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-0.0966288</td><td style=\"text-align: right;\">             7.48791</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-31-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.143951058387756\n",
      "  episode_reward_mean: 0.024500915408134462\n",
      "  episode_reward_min: -12.768159031867981\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.43453225679295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018422135566566293\n",
      "          policy_loss: -0.05031618515570318\n",
      "          total_loss: 4.252917681394085\n",
      "          vf_explained_var: 0.7991379867958767\n",
      "          vf_loss: 4.297707220559479\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.384615384615383\n",
      "    ram_util_percent: 33.784615384615385\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09220801803898042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08032868077070565\n",
      "    mean_inference_ms: 0.9124076553925352\n",
      "    mean_raw_obs_processing_ms: 0.07678258308336586\n",
      "  time_since_restore: 179.76003050804138\n",
      "  time_this_iter_s: 8.907730340957642\n",
      "  time_total_s: 179.76003050804138\n",
      "  timers:\n",
      "    learn_throughput: 661.576\n",
      "    learn_time_ms: 6046.172\n",
      "    load_throughput: 12902573.252\n",
      "    load_time_ms: 0.31\n",
      "    sample_throughput: 468.21\n",
      "    sample_time_ms: 8543.181\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1643383893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:34 (running for 00:03:05.72)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">          179.76</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">0.0245009</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:39 (running for 00:03:10.73)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">          179.76</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">0.0245009</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-31-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.143951058387756\n",
      "  episode_reward_mean: 0.07217302829027177\n",
      "  episode_reward_min: -12.768159031867981\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.384239289068407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019163241797196997\n",
      "          policy_loss: -0.05187653779572937\n",
      "          total_loss: 1.9048843874085335\n",
      "          vf_explained_var: 0.8589096207131621\n",
      "          vf_loss: 1.9510119548407934\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.315384615384616\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09295635984301391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08087929203840093\n",
      "    mean_inference_ms: 0.9198441925018181\n",
      "    mean_raw_obs_processing_ms: 0.07739479663341077\n",
      "  time_since_restore: 188.81862902641296\n",
      "  time_this_iter_s: 9.058598518371582\n",
      "  time_total_s: 188.81862902641296\n",
      "  timers:\n",
      "    learn_throughput: 650.267\n",
      "    learn_time_ms: 6151.316\n",
      "    load_throughput: 12612551.496\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 461.049\n",
      "    sample_time_ms: 8675.87\n",
      "    update_time_ms: 1.522\n",
      "  timestamp: 1643383902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:44 (running for 00:03:15.80)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         188.819</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">0.072173</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:49 (running for 00:03:20.81)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         188.819</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">0.072173</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-31-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.143951058387756\n",
      "  episode_reward_mean: 0.02681431859731674\n",
      "  episode_reward_min: -12.768159031867981\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.39544152393136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02039309311527887\n",
      "          policy_loss: -0.05524582078540197\n",
      "          total_loss: 3.3462210644316928\n",
      "          vf_explained_var: 0.8316723798551867\n",
      "          vf_loss: 3.3953489451639114\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.808333333333334\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0936344247457246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08139786647598686\n",
      "    mean_inference_ms: 0.92661959555952\n",
      "    mean_raw_obs_processing_ms: 0.07793637030989206\n",
      "  time_since_restore: 197.74618339538574\n",
      "  time_this_iter_s: 8.927554368972778\n",
      "  time_total_s: 197.74618339538574\n",
      "  timers:\n",
      "    learn_throughput: 645.923\n",
      "    learn_time_ms: 6192.69\n",
      "    load_throughput: 12570027.722\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 453.619\n",
      "    sample_time_ms: 8817.973\n",
      "    update_time_ms: 1.534\n",
      "  timestamp: 1643383911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:31:55 (running for 00:03:26.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         197.746</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">0.0268143</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:00 (running for 00:03:31.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         197.746</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">0.0268143</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.143951058387756\n",
      "  episode_reward_mean: 0.02201646685600281\n",
      "  episode_reward_min: -12.768159031867981\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.245496257658928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016372515075190452\n",
      "          policy_loss: -0.05206059103090596\n",
      "          total_loss: 3.191238707079682\n",
      "          vf_explained_var: 0.8319164582478102\n",
      "          vf_loss: 3.2359316672689173\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.59375\n",
      "    ram_util_percent: 33.7625\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09446202966993028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08202505140827661\n",
      "    mean_inference_ms: 0.9348073631368686\n",
      "    mean_raw_obs_processing_ms: 0.07859367285630263\n",
      "  time_since_restore: 209.02802681922913\n",
      "  time_this_iter_s: 11.281843423843384\n",
      "  time_total_s: 209.02802681922913\n",
      "  timers:\n",
      "    learn_throughput: 621.465\n",
      "    learn_time_ms: 6436.401\n",
      "    load_throughput: 12431250.741\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 448.536\n",
      "    sample_time_ms: 8917.902\n",
      "    update_time_ms: 1.52\n",
      "  timestamp: 1643383922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:05 (running for 00:03:37.06)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         209.028</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">0.0220165</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:10 (running for 00:03:42.07)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         209.028</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">0.0220165</td><td style=\"text-align: right;\">              11.144</td><td style=\"text-align: right;\">            -12.7682</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.412586361169815\n",
      "  episode_reward_mean: 0.03898461267352104\n",
      "  episode_reward_min: -12.121722757816315\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.121098686546407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01580276096096365\n",
      "          policy_loss: -0.05491439823076249\n",
      "          total_loss: 2.790057276333532\n",
      "          vf_explained_var: 0.851150531025343\n",
      "          vf_loss: 2.8378604445406186\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.47692307692308\n",
      "    ram_util_percent: 33.715384615384615\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09513553216235472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08254417080312114\n",
      "    mean_inference_ms: 0.94136230217104\n",
      "    mean_raw_obs_processing_ms: 0.0791174200956076\n",
      "  time_since_restore: 218.031014919281\n",
      "  time_this_iter_s: 9.00298810005188\n",
      "  time_total_s: 218.031014919281\n",
      "  timers:\n",
      "    learn_throughput: 621.678\n",
      "    learn_time_ms: 6434.202\n",
      "    load_throughput: 12363460.575\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 433.259\n",
      "    sample_time_ms: 9232.363\n",
      "    update_time_ms: 1.55\n",
      "  timestamp: 1643383931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:15 (running for 00:03:47.08)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         218.031</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">0.0389846</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -12.1217</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-32-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.412586361169815\n",
      "  episode_reward_mean: -0.020211719274520874\n",
      "  episode_reward_min: -12.121722757816315\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 11.001338085051506\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016262385091277056\n",
      "          policy_loss: -0.037565606292737745\n",
      "          total_loss: 2.397669299379472\n",
      "          vf_explained_var: 0.8499380675054365\n",
      "          vf_loss: 2.4279168416735946\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.458333333333336\n",
      "    ram_util_percent: 33.708333333333336\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0957563965826683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08304151093632882\n",
      "    mean_inference_ms: 0.9475119151946824\n",
      "    mean_raw_obs_processing_ms: 0.07959788521267862\n",
      "  time_since_restore: 226.27618098258972\n",
      "  time_this_iter_s: 8.245166063308716\n",
      "  time_total_s: 226.27618098258972\n",
      "  timers:\n",
      "    learn_throughput: 625.609\n",
      "    learn_time_ms: 6393.767\n",
      "    load_throughput: 12329841.993\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 433.008\n",
      "    sample_time_ms: 9237.703\n",
      "    update_time_ms: 1.56\n",
      "  timestamp: 1643383939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:20 (running for 00:03:52.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         226.276</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-0.0202117</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -12.1217</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:25 (running for 00:03:57.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         226.276</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-0.0202117</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -12.1217</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-32-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.412586361169815\n",
      "  episode_reward_mean: 0.03190071746706963\n",
      "  episode_reward_min: -12.121722757816315\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.86993959283316\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016658698261908952\n",
      "          policy_loss: -0.031654272696143515\n",
      "          total_loss: 1.6258879233672414\n",
      "          vf_explained_var: 0.8888651858734828\n",
      "          vf_loss: 1.6500457839299274\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.107692307692307\n",
      "    ram_util_percent: 33.761538461538464\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09640124204659141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08355709436825735\n",
      "    mean_inference_ms: 0.9538364702526547\n",
      "    mean_raw_obs_processing_ms: 0.08010883400452423\n",
      "  time_since_restore: 235.14813590049744\n",
      "  time_this_iter_s: 8.871954917907715\n",
      "  time_total_s: 235.14813590049744\n",
      "  timers:\n",
      "    learn_throughput: 628.971\n",
      "    learn_time_ms: 6359.592\n",
      "    load_throughput: 12255983.636\n",
      "    load_time_ms: 0.326\n",
      "    sample_throughput: 432.688\n",
      "    sample_time_ms: 9244.545\n",
      "    update_time_ms: 1.547\n",
      "  timestamp: 1643383948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:31 (running for 00:04:03.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         235.148</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">0.0319007</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -12.1217</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:36 (running for 00:04:08.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         235.148</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">0.0319007</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -12.1217</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-32-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.412586361169815\n",
      "  episode_reward_mean: 0.00833134412765503\n",
      "  episode_reward_min: -12.121722757816315\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.73983835404919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016210590994222826\n",
      "          policy_loss: -0.039315528062082106\n",
      "          total_loss: 1.9432247741248019\n",
      "          vf_explained_var: 0.8678875574501612\n",
      "          vf_loss: 1.975245539988241\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.954545454545453\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09689368522681743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08396107210968065\n",
      "    mean_inference_ms: 0.9586609170149694\n",
      "    mean_raw_obs_processing_ms: 0.08050486348790781\n",
      "  time_since_restore: 243.31128907203674\n",
      "  time_this_iter_s: 8.163153171539307\n",
      "  time_total_s: 243.31128907203674\n",
      "  timers:\n",
      "    learn_throughput: 637.311\n",
      "    learn_time_ms: 6276.375\n",
      "    load_throughput: 12309035.95\n",
      "    load_time_ms: 0.325\n",
      "    sample_throughput: 435.927\n",
      "    sample_time_ms: 9175.846\n",
      "    update_time_ms: 1.549\n",
      "  timestamp: 1643383956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:41 (running for 00:04:13.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         243.311</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">0.00833134</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -12.1217</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.412586361169815\n",
      "  episode_reward_mean: 0.09523239523172379\n",
      "  episode_reward_min: -9.147969156503677\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.71413714090983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016863985346593606\n",
      "          policy_loss: -0.047335635361972675\n",
      "          total_loss: 3.2757495308114635\n",
      "          vf_explained_var: 0.8478771035389233\n",
      "          vf_loss: 3.3154963731124836\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.291666666666668\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09719702901826728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0842219429925183\n",
      "    mean_inference_ms: 0.9616551282729985\n",
      "    mean_raw_obs_processing_ms: 0.08073711000921778\n",
      "  time_since_restore: 251.50075936317444\n",
      "  time_this_iter_s: 8.189470291137695\n",
      "  time_total_s: 251.50075936317444\n",
      "  timers:\n",
      "    learn_throughput: 638.282\n",
      "    learn_time_ms: 6266.826\n",
      "    load_throughput: 12558736.432\n",
      "    load_time_ms: 0.319\n",
      "    sample_throughput: 439.704\n",
      "    sample_time_ms: 9097.037\n",
      "    update_time_ms: 1.569\n",
      "  timestamp: 1643383964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:47 (running for 00:04:18.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         251.501</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">0.0952324</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -9.14797</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:52 (running for 00:04:23.69)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         251.501</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">0.0952324</td><td style=\"text-align: right;\">             11.4126</td><td style=\"text-align: right;\">            -9.14797</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-32-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.717913925647736\n",
      "  episode_reward_mean: 0.03561127699911595\n",
      "  episode_reward_min: -8.388105034828186\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.761340413042294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01636703179541937\n",
      "          policy_loss: -0.05098459783100313\n",
      "          total_loss: 2.635431887513848\n",
      "          vf_explained_var: 0.869125828114889\n",
      "          vf_loss: 2.6790513091510344\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.200000000000003\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09728275657771894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08432364629041611\n",
      "    mean_inference_ms: 0.9624166733377874\n",
      "    mean_raw_obs_processing_ms: 0.0807963384304251\n",
      "  time_since_restore: 259.9958071708679\n",
      "  time_this_iter_s: 8.495047807693481\n",
      "  time_total_s: 259.9958071708679\n",
      "  timers:\n",
      "    learn_throughput: 643.983\n",
      "    learn_time_ms: 6211.345\n",
      "    load_throughput: 12758339.163\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 444.561\n",
      "    sample_time_ms: 8997.647\n",
      "    update_time_ms: 1.587\n",
      "  timestamp: 1643383973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:32:57 (running for 00:04:29.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         259.996</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">0.0356113</td><td style=\"text-align: right;\">             10.7179</td><td style=\"text-align: right;\">            -8.38811</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.717913925647736\n",
      "  episode_reward_mean: -0.014923002421855926\n",
      "  episode_reward_min: -8.388105034828186\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.69629830186085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016416487829682042\n",
      "          policy_loss: -0.058551502694445916\n",
      "          total_loss: 2.224986770060114\n",
      "          vf_explained_var: 0.8815811801982182\n",
      "          vf_loss: 2.276150854428609\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.4\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09740054001771302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0844423385074669\n",
      "    mean_inference_ms: 0.9635142117944477\n",
      "    mean_raw_obs_processing_ms: 0.08089070806893899\n",
      "  time_since_restore: 268.5373463630676\n",
      "  time_this_iter_s: 8.541539192199707\n",
      "  time_total_s: 268.5373463630676\n",
      "  timers:\n",
      "    learn_throughput: 649.041\n",
      "    learn_time_ms: 6162.938\n",
      "    load_throughput: 13315250.794\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 446.73\n",
      "    sample_time_ms: 8953.953\n",
      "    update_time_ms: 1.567\n",
      "  timestamp: 1643383982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:03 (running for 00:04:34.77)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         268.537</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">-0.014923</td><td style=\"text-align: right;\">             10.7179</td><td style=\"text-align: right;\">            -8.38811</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:08 (running for 00:04:39.77)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         268.537</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">-0.014923</td><td style=\"text-align: right;\">             10.7179</td><td style=\"text-align: right;\">            -8.38811</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-33-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.717913925647736\n",
      "  episode_reward_mean: -0.038879607170820236\n",
      "  episode_reward_min: -8.388105034828186\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.661184083261798\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0174538804167868\n",
      "          policy_loss: -0.05260134212953109\n",
      "          total_loss: 3.0720144376158713\n",
      "          vf_explained_var: 0.8370020886903168\n",
      "          vf_loss: 3.1167615388990733\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.536363636363635\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09737085586779433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08445021411405097\n",
      "    mean_inference_ms: 0.9631976296982123\n",
      "    mean_raw_obs_processing_ms: 0.08087047773864749\n",
      "  time_since_restore: 276.3601415157318\n",
      "  time_this_iter_s: 7.822795152664185\n",
      "  time_total_s: 276.3601415157318\n",
      "  timers:\n",
      "    learn_throughput: 659.078\n",
      "    learn_time_ms: 6069.08\n",
      "    load_throughput: 13621186.977\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 450.679\n",
      "    sample_time_ms: 8875.505\n",
      "    update_time_ms: 1.493\n",
      "  timestamp: 1643383989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:13 (running for 00:04:45.62)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          276.36</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-0.0388796</td><td style=\"text-align: right;\">             10.7179</td><td style=\"text-align: right;\">            -8.38811</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-33-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.717913925647736\n",
      "  episode_reward_mean: -0.07273925051093101\n",
      "  episode_reward_min: -8.388105034828186\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.518419737456947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016507209992727614\n",
      "          policy_loss: -0.06797322926752429\n",
      "          total_loss: 3.1989257090354477\n",
      "          vf_explained_var: 0.8550315417910135\n",
      "          vf_loss: 3.2594707025956082\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.37272727272727\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09731467641556665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08443648581067018\n",
      "    mean_inference_ms: 0.9626301049162492\n",
      "    mean_raw_obs_processing_ms: 0.08083570043799428\n",
      "  time_since_restore: 284.53636622428894\n",
      "  time_this_iter_s: 8.176224708557129\n",
      "  time_total_s: 284.53636622428894\n",
      "  timers:\n",
      "    learn_throughput: 661.921\n",
      "    learn_time_ms: 6043.016\n",
      "    load_throughput: 13488676.636\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 458.077\n",
      "    sample_time_ms: 8732.162\n",
      "    update_time_ms: 1.454\n",
      "  timestamp: 1643383998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:19 (running for 00:04:50.81)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         284.536</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-0.0727393</td><td style=\"text-align: right;\">             10.7179</td><td style=\"text-align: right;\">            -8.38811</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:24 (running for 00:04:55.82)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         284.536</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-0.0727393</td><td style=\"text-align: right;\">             10.7179</td><td style=\"text-align: right;\">            -8.38811</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-33-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.061462819576263\n",
      "  episode_reward_mean: -0.05135052025318146\n",
      "  episode_reward_min: -9.952581465244293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.497449202178627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01677006325001184\n",
      "          policy_loss: -0.05426341750288522\n",
      "          total_loss: 3.763046031805777\n",
      "          vf_explained_var: 0.8339714842457925\n",
      "          vf_loss: 3.8097629105211586\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.575\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0973078406309273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0844536169065811\n",
      "    mean_inference_ms: 0.9624999270691277\n",
      "    mean_raw_obs_processing_ms: 0.08084353067271245\n",
      "  time_since_restore: 292.84664130210876\n",
      "  time_this_iter_s: 8.310275077819824\n",
      "  time_total_s: 292.84664130210876\n",
      "  timers:\n",
      "    learn_throughput: 691.557\n",
      "    learn_time_ms: 5784.052\n",
      "    load_throughput: 13661115.544\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 461.502\n",
      "    sample_time_ms: 8667.357\n",
      "    update_time_ms: 1.551\n",
      "  timestamp: 1643384006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:29 (running for 00:05:01.15)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         292.847</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-0.0513505</td><td style=\"text-align: right;\">             10.0615</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:34 (running for 00:05:06.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         292.847</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-0.0513505</td><td style=\"text-align: right;\">             10.0615</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-33-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.121542394161224\n",
      "  episode_reward_mean: -0.022890723422169686\n",
      "  episode_reward_min: -9.952581465244293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.41650460458571\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016243253343320774\n",
      "          policy_loss: -0.03730709646898572\n",
      "          total_loss: 1.7903206027783853\n",
      "          vf_explained_var: 0.8884322595852677\n",
      "          vf_loss: 1.8203182317556874\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.759999999999998\n",
      "    ram_util_percent: 33.77333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09762547820184447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0846791700121669\n",
      "    mean_inference_ms: 0.9657360754906118\n",
      "    mean_raw_obs_processing_ms: 0.08109825815625207\n",
      "  time_since_restore: 303.18217754364014\n",
      "  time_this_iter_s: 10.335536241531372\n",
      "  time_total_s: 303.18217754364014\n",
      "  timers:\n",
      "    learn_throughput: 685.269\n",
      "    learn_time_ms: 5837.127\n",
      "    load_throughput: 12803125.763\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 471.189\n",
      "    sample_time_ms: 8489.154\n",
      "    update_time_ms: 1.541\n",
      "  timestamp: 1643384016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:39 (running for 00:05:11.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         303.182</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-0.0228907</td><td style=\"text-align: right;\">             9.12154</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:44 (running for 00:05:16.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         303.182</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-0.0228907</td><td style=\"text-align: right;\">             9.12154</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-33-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.121542394161224\n",
      "  episode_reward_mean: -0.06917245388031006\n",
      "  episode_reward_min: -9.952581465244293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.350087448858446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01603661176257418\n",
      "          policy_loss: -0.0678942377088211\n",
      "          total_loss: 1.7473118688150118\n",
      "          vf_explained_var: 0.8879885011462755\n",
      "          vf_loss: 1.8079896316733413\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.814285714285713\n",
      "    ram_util_percent: 33.81428571428572\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09803553924546757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08496204946251737\n",
      "    mean_inference_ms: 0.9700099879269416\n",
      "    mean_raw_obs_processing_ms: 0.08144029302660849\n",
      "  time_since_restore: 313.23475456237793\n",
      "  time_this_iter_s: 10.052577018737793\n",
      "  time_total_s: 313.23475456237793\n",
      "  timers:\n",
      "    learn_throughput: 672.481\n",
      "    learn_time_ms: 5948.119\n",
      "    load_throughput: 12793362.818\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 464.488\n",
      "    sample_time_ms: 8611.641\n",
      "    update_time_ms: 1.543\n",
      "  timestamp: 1643384026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:49 (running for 00:05:21.58)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         313.235</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-0.0691725</td><td style=\"text-align: right;\">             9.12154</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:33:54 (running for 00:05:26.58)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         313.235</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-0.0691725</td><td style=\"text-align: right;\">             9.12154</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-33-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.121542394161224\n",
      "  episode_reward_mean: 0.012874524891376495\n",
      "  episode_reward_min: -9.952581465244293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.29276933362407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01796637080490577\n",
      "          policy_loss: -0.02975767764352983\n",
      "          total_loss: 2.444970583483096\n",
      "          vf_explained_var: 0.8821186850788773\n",
      "          vf_loss: 2.4666433952508435\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.258333333333336\n",
      "    ram_util_percent: 33.80833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09843138395494279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08524289960193734\n",
      "    mean_inference_ms: 0.9741323786572432\n",
      "    mean_raw_obs_processing_ms: 0.08177039173691572\n",
      "  time_since_restore: 321.3048095703125\n",
      "  time_this_iter_s: 8.07005500793457\n",
      "  time_total_s: 321.3048095703125\n",
      "  timers:\n",
      "    learn_throughput: 675.428\n",
      "    learn_time_ms: 5922.175\n",
      "    load_throughput: 12868923.832\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 461.458\n",
      "    sample_time_ms: 8668.182\n",
      "    update_time_ms: 1.534\n",
      "  timestamp: 1643384034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:00 (running for 00:05:31.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         321.305</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">0.0128745</td><td style=\"text-align: right;\">             9.12154</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.121542394161224\n",
      "  episode_reward_mean: 0.005063367038965225\n",
      "  episode_reward_min: -9.952581465244293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.372500069936116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015794429086666457\n",
      "          policy_loss: -0.06313436622280748\n",
      "          total_loss: 1.9151782517531706\n",
      "          vf_explained_var: 0.8851568095145687\n",
      "          vf_loss: 1.9712051225926286\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.25\n",
      "    ram_util_percent: 33.80833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09884503216515832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08552601364287923\n",
      "    mean_inference_ms: 0.9783905024307445\n",
      "    mean_raw_obs_processing_ms: 0.08209659427868565\n",
      "  time_since_restore: 329.60856652259827\n",
      "  time_this_iter_s: 8.303756952285767\n",
      "  time_total_s: 329.60856652259827\n",
      "  timers:\n",
      "    learn_throughput: 673.762\n",
      "    learn_time_ms: 5936.817\n",
      "    load_throughput: 12940390.282\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 462.824\n",
      "    sample_time_ms: 8642.586\n",
      "    update_time_ms: 1.532\n",
      "  timestamp: 1643384043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:05 (running for 00:05:37.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         329.609</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">0.00506337</td><td style=\"text-align: right;\">             9.12154</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:10 (running for 00:05:42.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         329.609</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">0.00506337</td><td style=\"text-align: right;\">             9.12154</td><td style=\"text-align: right;\">            -9.95258</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.701066076755524\n",
      "  episode_reward_mean: -0.06934875547885895\n",
      "  episode_reward_min: -8.630932450294495\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.455043111821656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01660611031797472\n",
      "          policy_loss: -0.05904336172125993\n",
      "          total_loss: 2.1619225104589774\n",
      "          vf_explained_var: 0.8809781763502347\n",
      "          vf_loss: 2.2134931244516887\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.87272727272727\n",
      "    ram_util_percent: 33.86363636363635\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09921672115938908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08578050010209172\n",
      "    mean_inference_ms: 0.9822802860848554\n",
      "    mean_raw_obs_processing_ms: 0.08238953724144688\n",
      "  time_since_restore: 337.62955689430237\n",
      "  time_this_iter_s: 8.020990371704102\n",
      "  time_total_s: 337.62955689430237\n",
      "  timers:\n",
      "    learn_throughput: 676.085\n",
      "    learn_time_ms: 5916.416\n",
      "    load_throughput: 12795314.216\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 462.062\n",
      "    sample_time_ms: 8656.843\n",
      "    update_time_ms: 1.52\n",
      "  timestamp: 1643384051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:15 (running for 00:05:47.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">          337.63</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-0.0693488</td><td style=\"text-align: right;\">             8.70107</td><td style=\"text-align: right;\">            -8.63093</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.44815906882286\n",
      "  episode_reward_mean: -0.06660608276724815\n",
      "  episode_reward_min: -9.292501389980316\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.454095774824902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01770538776830227\n",
      "          policy_loss: -0.06749005115400719\n",
      "          total_loss: 2.622582377807828\n",
      "          vf_explained_var: 0.8717737592676634\n",
      "          vf_loss: 2.682105017701785\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.7\n",
      "    ram_util_percent: 33.81666666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09924843649537507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08581235075005064\n",
      "    mean_inference_ms: 0.9826422925737842\n",
      "    mean_raw_obs_processing_ms: 0.0824212123555636\n",
      "  time_since_restore: 345.5591564178467\n",
      "  time_this_iter_s: 7.9295995235443115\n",
      "  time_total_s: 345.5591564178467\n",
      "  timers:\n",
      "    learn_throughput: 682.052\n",
      "    learn_time_ms: 5864.658\n",
      "    load_throughput: 12875837.299\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 463.417\n",
      "    sample_time_ms: 8631.534\n",
      "    update_time_ms: 1.496\n",
      "  timestamp: 1643384059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:21 (running for 00:05:53.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         345.559</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-0.0666061</td><td style=\"text-align: right;\">             8.44816</td><td style=\"text-align: right;\">             -9.2925</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:26 (running for 00:05:58.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         345.559</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-0.0666061</td><td style=\"text-align: right;\">             8.44816</td><td style=\"text-align: right;\">             -9.2925</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.44815906882286\n",
      "  episode_reward_mean: 0.07564379304647445\n",
      "  episode_reward_min: -9.292501389980316\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.410451342469903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01585314190448638\n",
      "          policy_loss: -0.04035757517702477\n",
      "          total_loss: 1.4178214805150624\n",
      "          vf_explained_var: 0.9188483879130374\n",
      "          vf_loss: 1.451045151904065\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.454545454545453\n",
      "    ram_util_percent: 33.86363636363636\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09912250315293016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08573522172729575\n",
      "    mean_inference_ms: 0.9813721834058404\n",
      "    mean_raw_obs_processing_ms: 0.0823218816056364\n",
      "  time_since_restore: 353.5121991634369\n",
      "  time_this_iter_s: 7.95304274559021\n",
      "  time_total_s: 353.5121991634369\n",
      "  timers:\n",
      "    learn_throughput: 686.338\n",
      "    learn_time_ms: 5828.036\n",
      "    load_throughput: 12966393.075\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 467.429\n",
      "    sample_time_ms: 8557.443\n",
      "    update_time_ms: 1.519\n",
      "  timestamp: 1643384067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:32 (running for 00:06:03.98)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         353.512</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">0.0756438</td><td style=\"text-align: right;\">             8.44816</td><td style=\"text-align: right;\">             -9.2925</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.611224919557571\n",
      "  episode_reward_mean: -0.04388362914323807\n",
      "  episode_reward_min: -9.292501389980316\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.303392702533353\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017066938364649967\n",
      "          policy_loss: -0.06091691983002488\n",
      "          total_loss: 1.8776813449498306\n",
      "          vf_explained_var: 0.8889444365937222\n",
      "          vf_loss: 1.930918141109969\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.099999999999998\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899673105656596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08565248826790443\n",
      "    mean_inference_ms: 0.9801080444303113\n",
      "    mean_raw_obs_processing_ms: 0.08221926259378537\n",
      "  time_since_restore: 361.30268383026123\n",
      "  time_this_iter_s: 7.790484666824341\n",
      "  time_total_s: 361.30268383026123\n",
      "  timers:\n",
      "    learn_throughput: 685.84\n",
      "    learn_time_ms: 5832.266\n",
      "    load_throughput: 12925436.055\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 469.867\n",
      "    sample_time_ms: 8513.048\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1643384075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:38 (running for 00:06:09.79)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         361.303</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-0.0438836</td><td style=\"text-align: right;\">             7.61122</td><td style=\"text-align: right;\">             -9.2925</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.608595252037048\n",
      "  episode_reward_mean: 0.08375527903437614\n",
      "  episode_reward_min: -9.292501389980316\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.240166338028446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017845174669726665\n",
      "          policy_loss: -0.05940761022039399\n",
      "          total_loss: 2.024545277575011\n",
      "          vf_explained_var: 0.8904427466213062\n",
      "          vf_loss: 2.0759225611725163\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.954545454545453\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09882606226521419\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08554163301506612\n",
      "    mean_inference_ms: 0.9784433419749661\n",
      "    mean_raw_obs_processing_ms: 0.08209597359348976\n",
      "  time_since_restore: 369.0484893321991\n",
      "  time_this_iter_s: 7.745805501937866\n",
      "  time_total_s: 369.0484893321991\n",
      "  timers:\n",
      "    learn_throughput: 689.245\n",
      "    learn_time_ms: 5803.454\n",
      "    load_throughput: 13086751.95\n",
      "    load_time_ms: 0.306\n",
      "    sample_throughput: 470.403\n",
      "    sample_time_ms: 8503.341\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1643384082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:43 (running for 00:06:15.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         369.048</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">0.0837553</td><td style=\"text-align: right;\">              7.6086</td><td style=\"text-align: right;\">             -9.2925</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:48 (running for 00:06:20.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         369.048</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">0.0837553</td><td style=\"text-align: right;\">              7.6086</td><td style=\"text-align: right;\">             -9.2925</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.608595252037048\n",
      "  episode_reward_mean: 0.07701228082180023\n",
      "  episode_reward_min: -9.292501389980316\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.145223363240559\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017823111869168646\n",
      "          policy_loss: -0.05858357619013517\n",
      "          total_loss: 1.4882479919040557\n",
      "          vf_explained_var: 0.9106249496500979\n",
      "          vf_loss: 1.5388111662800594\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.090909090909093\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09863473672234618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08541484241036716\n",
      "    mean_inference_ms: 0.9765895489104167\n",
      "    mean_raw_obs_processing_ms: 0.08195911796881304\n",
      "  time_since_restore: 376.7992720603943\n",
      "  time_this_iter_s: 7.75078272819519\n",
      "  time_total_s: 376.7992720603943\n",
      "  timers:\n",
      "    learn_throughput: 692.628\n",
      "    learn_time_ms: 5775.105\n",
      "    load_throughput: 13017703.29\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 473.538\n",
      "    sample_time_ms: 8447.059\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1643384090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:34:54 (running for 00:06:26.33)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         376.799</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">0.0770123</td><td style=\"text-align: right;\">              7.6086</td><td style=\"text-align: right;\">             -9.2925</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-34-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.621990233659744\n",
      "  episode_reward_mean: 0.045980060398578645\n",
      "  episode_reward_min: -7.908365145325661\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.022775321878413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017758754984571554\n",
      "          policy_loss: -0.06942445051565926\n",
      "          total_loss: 2.5223824251813673\n",
      "          vf_explained_var: 0.8756051683297721\n",
      "          vf_loss: 2.583815450085107\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.374999999999996\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09842924868756224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0852760169458006\n",
      "    mean_inference_ms: 0.9746059097981657\n",
      "    mean_raw_obs_processing_ms: 0.0818110927305126\n",
      "  time_since_restore: 384.7053961753845\n",
      "  time_this_iter_s: 7.906124114990234\n",
      "  time_total_s: 384.7053961753845\n",
      "  timers:\n",
      "    learn_throughput: 701.61\n",
      "    learn_time_ms: 5701.175\n",
      "    load_throughput: 14011371.304\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 484.887\n",
      "    sample_time_ms: 8249.351\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1643384098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:00 (running for 00:06:32.26)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         384.705</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">0.0459801</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -7.90837</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:05 (running for 00:06:37.26)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         384.705</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">0.0459801</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -7.90837</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.621990233659744\n",
      "  episode_reward_mean: -0.05768010199069977\n",
      "  episode_reward_min: -7.908365145325661\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 10.02426097213581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01722885190341223\n",
      "          policy_loss: -0.05628789933198081\n",
      "          total_loss: 2.2311173637347514\n",
      "          vf_explained_var: 0.8839404315717758\n",
      "          vf_loss: 2.2796522750969856\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.2\n",
      "    ram_util_percent: 33.81818181818182\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09824041017121306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08514740134800909\n",
      "    mean_inference_ms: 0.9727313596024796\n",
      "    mean_raw_obs_processing_ms: 0.08165965505680012\n",
      "  time_since_restore: 392.78193616867065\n",
      "  time_this_iter_s: 8.076539993286133\n",
      "  time_total_s: 392.78193616867065\n",
      "  timers:\n",
      "    learn_throughput: 716.948\n",
      "    learn_time_ms: 5579.207\n",
      "    load_throughput: 13823198.484\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 493.811\n",
      "    sample_time_ms: 8100.258\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1643384106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:10 (running for 00:06:42.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         392.782</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">-0.0576801</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -7.90837</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.621990233659744\n",
      "  episode_reward_mean: 0.017008438110351562\n",
      "  episode_reward_min: -7.908365145325661\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.925796334461499\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018100367762587898\n",
      "          policy_loss: -0.06076827319639344\n",
      "          total_loss: 1.733282648363421\n",
      "          vf_explained_var: 0.8999025380739601\n",
      "          vf_loss: 1.7859057516820969\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.616666666666664\n",
      "    ram_util_percent: 33.833333333333336\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0980534744822025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0850208261130587\n",
      "    mean_inference_ms: 0.9708802845629378\n",
      "    mean_raw_obs_processing_ms: 0.08151284538976407\n",
      "  time_since_restore: 400.81394505500793\n",
      "  time_this_iter_s: 8.03200888633728\n",
      "  time_total_s: 400.81394505500793\n",
      "  timers:\n",
      "    learn_throughput: 716.47\n",
      "    learn_time_ms: 5582.927\n",
      "    load_throughput: 13881529.042\n",
      "    load_time_ms: 0.288\n",
      "    sample_throughput: 501.835\n",
      "    sample_time_ms: 7970.755\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1643384114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:15 (running for 00:06:47.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         400.814</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">0.0170084</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -7.90837</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:20 (running for 00:06:52.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         400.814</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">0.0170084</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -7.90837</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.621990233659744\n",
      "  episode_reward_mean: -0.03468667432665825\n",
      "  episode_reward_min: -6.97453847527504\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.961978348352575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01656162346582136\n",
      "          policy_loss: -0.05800372472743914\n",
      "          total_loss: 0.9341118709299393\n",
      "          vf_explained_var: 0.9215979688911028\n",
      "          vf_loss: 0.9846628703737772\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.463636363636365\n",
      "    ram_util_percent: 33.80909090909091\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09788096364849931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08490459241725862\n",
      "    mean_inference_ms: 0.9691933793931551\n",
      "    mean_raw_obs_processing_ms: 0.08137735345056331\n",
      "  time_since_restore: 408.94950103759766\n",
      "  time_this_iter_s: 8.135555982589722\n",
      "  time_total_s: 408.94950103759766\n",
      "  timers:\n",
      "    learn_throughput: 716.017\n",
      "    learn_time_ms: 5586.462\n",
      "    load_throughput: 13407828.658\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 502.984\n",
      "    sample_time_ms: 7952.545\n",
      "    update_time_ms: 1.391\n",
      "  timestamp: 1643384122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:25 (running for 00:06:57.58)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">          408.95</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-0.0346867</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -6.97454</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:30 (running for 00:07:02.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">          408.95</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-0.0346867</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -6.97454</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-35-31\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.621990233659744\n",
      "  episode_reward_mean: -0.0938817611336708\n",
      "  episode_reward_min: -8.321447640657425\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.810792476900163\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017989544350344527\n",
      "          policy_loss: -0.08289422311930246\n",
      "          total_loss: 1.8999044871578614\n",
      "          vf_explained_var: 0.9126928214744855\n",
      "          vf_loss: 1.974703406101914\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.40833333333333\n",
      "    ram_util_percent: 33.800000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09776223047213747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0848365300794779\n",
      "    mean_inference_ms: 0.9680440850450418\n",
      "    mean_raw_obs_processing_ms: 0.08128517554712024\n",
      "  time_since_restore: 417.2380156517029\n",
      "  time_this_iter_s: 8.288514614105225\n",
      "  time_total_s: 417.2380156517029\n",
      "  timers:\n",
      "    learn_throughput: 714.578\n",
      "    learn_time_ms: 5597.709\n",
      "    load_throughput: 13370430.347\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 501.689\n",
      "    sample_time_ms: 7973.062\n",
      "    update_time_ms: 1.415\n",
      "  timestamp: 1643384131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:36 (running for 00:07:07.89)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         417.238</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-0.0938818</td><td style=\"text-align: right;\">             7.62199</td><td style=\"text-align: right;\">            -8.32145</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-35-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.690938621759415\n",
      "  episode_reward_mean: -0.09240396291017533\n",
      "  episode_reward_min: -8.558694452047348\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.828758715557795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018156115735348543\n",
      "          policy_loss: -0.0824752875313323\n",
      "          total_loss: 4.418757864368218\n",
      "          vf_explained_var: 0.8419664787348881\n",
      "          vf_loss: 4.493062904861666\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.775\n",
      "    ram_util_percent: 33.80833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09766813351074152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08478804899973785\n",
      "    mean_inference_ms: 0.967139489581605\n",
      "    mean_raw_obs_processing_ms: 0.08121227329609358\n",
      "  time_since_restore: 425.1630940437317\n",
      "  time_this_iter_s: 7.925078392028809\n",
      "  time_total_s: 425.1630940437317\n",
      "  timers:\n",
      "    learn_throughput: 714.45\n",
      "    learn_time_ms: 5598.71\n",
      "    load_throughput: 13358719.643\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 501.063\n",
      "    sample_time_ms: 7983.024\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1643384139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:42 (running for 00:07:13.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         425.163</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">-0.092404</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-35-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.690938621759415\n",
      "  episode_reward_mean: 0.01119006760418415\n",
      "  episode_reward_min: -8.558694452047348\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.7902424176534\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018356512194355273\n",
      "          policy_loss: -0.047873046870032944\n",
      "          total_loss: 1.9154802232720358\n",
      "          vf_explained_var: 0.8984928115080761\n",
      "          vf_loss: 1.955092834288715\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.700000000000003\n",
      "    ram_util_percent: 33.89090909090908\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09755395533627267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08472181240609719\n",
      "    mean_inference_ms: 0.9660325796922327\n",
      "    mean_raw_obs_processing_ms: 0.08112132323915075\n",
      "  time_since_restore: 432.9669461250305\n",
      "  time_this_iter_s: 7.803852081298828\n",
      "  time_total_s: 432.9669461250305\n",
      "  timers:\n",
      "    learn_throughput: 714.59\n",
      "    learn_time_ms: 5597.613\n",
      "    load_throughput: 13461619.193\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 501.889\n",
      "    sample_time_ms: 7969.892\n",
      "    update_time_ms: 1.345\n",
      "  timestamp: 1643384146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:47 (running for 00:07:19.66)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         432.967</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">0.0111901</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:53 (running for 00:07:24.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         432.967</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">0.0111901</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.690938621759415\n",
      "  episode_reward_mean: 0.0517828893661499\n",
      "  episode_reward_min: -8.558694452047348\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.824321649407828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018997972242434796\n",
      "          policy_loss: -0.059633786397014735\n",
      "          total_loss: 2.499965769434548\n",
      "          vf_explained_var: 0.8815953836646131\n",
      "          vf_loss: 2.551050466170875\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.318181818181817\n",
      "    ram_util_percent: 33.97272727272727\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09745149280701863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0846634417859799\n",
      "    mean_inference_ms: 0.9650320099902594\n",
      "    mean_raw_obs_processing_ms: 0.08103732050239217\n",
      "  time_since_restore: 440.8685097694397\n",
      "  time_this_iter_s: 7.90156364440918\n",
      "  time_total_s: 440.8685097694397\n",
      "  timers:\n",
      "    learn_throughput: 713.117\n",
      "    learn_time_ms: 5609.176\n",
      "    load_throughput: 13469184.329\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 501.984\n",
      "    sample_time_ms: 7968.379\n",
      "    update_time_ms: 1.375\n",
      "  timestamp: 1643384154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:35:58 (running for 00:07:30.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         440.869</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">0.0517829</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.690938621759415\n",
      "  episode_reward_mean: -0.03627298653125763\n",
      "  episode_reward_min: -8.558694452047348\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.77534605662028\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01779124896506476\n",
      "          policy_loss: -0.06617829443145824\n",
      "          total_loss: 1.5483389969614725\n",
      "          vf_explained_var: 0.9115256970287651\n",
      "          vf_loss: 1.606511233442573\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.991666666666667\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0973782070206938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08462776984109538\n",
      "    mean_inference_ms: 0.9643337600328971\n",
      "    mean_raw_obs_processing_ms: 0.0809788889015752\n",
      "  time_since_restore: 449.1782035827637\n",
      "  time_this_iter_s: 8.309693813323975\n",
      "  time_total_s: 449.1782035827637\n",
      "  timers:\n",
      "    learn_throughput: 708.862\n",
      "    learn_time_ms: 5642.845\n",
      "    load_throughput: 13411043.965\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 499.846\n",
      "    sample_time_ms: 8002.469\n",
      "    update_time_ms: 1.386\n",
      "  timestamp: 1643384163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:04 (running for 00:07:35.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         449.178</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">-0.036273</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:09 (running for 00:07:40.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         449.178</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">-0.036273</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-36-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.690938621759415\n",
      "  episode_reward_mean: 0.07553078562021255\n",
      "  episode_reward_min: -8.558694452047348\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.593010449153121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017838893885526\n",
      "          policy_loss: -0.06144377435407331\n",
      "          total_loss: 2.1931578757301455\n",
      "          vf_explained_var: 0.8859314460267302\n",
      "          vf_loss: 2.2465741500739127\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.616666666666667\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0972719407701283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0845571061260507\n",
      "    mean_inference_ms: 0.9632788718596637\n",
      "    mean_raw_obs_processing_ms: 0.08088937506958052\n",
      "  time_since_restore: 457.3484733104706\n",
      "  time_this_iter_s: 8.17026972770691\n",
      "  time_total_s: 457.3484733104706\n",
      "  timers:\n",
      "    learn_throughput: 704.184\n",
      "    learn_time_ms: 5680.336\n",
      "    load_throughput: 13471347.358\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 497.451\n",
      "    sample_time_ms: 8040.992\n",
      "    update_time_ms: 1.465\n",
      "  timestamp: 1643384171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:14 (running for 00:07:46.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         457.348</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">0.0755308</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:19 (running for 00:07:51.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         457.348</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">0.0755308</td><td style=\"text-align: right;\">             8.69094</td><td style=\"text-align: right;\">            -8.55869</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-36-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.008197963237762\n",
      "  episode_reward_mean: 0.11848671272397042\n",
      "  episode_reward_min: -7.131419330835342\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.524878007622176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019638503914921623\n",
      "          policy_loss: -0.055584931239405624\n",
      "          total_loss: 2.393201142481418\n",
      "          vf_explained_var: 0.8971167961756389\n",
      "          vf_loss: 2.439948748773144\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.71666666666667\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09717885010758659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08449934849156068\n",
      "    mean_inference_ms: 0.9623880521145105\n",
      "    mean_raw_obs_processing_ms: 0.08083135486718192\n",
      "  time_since_restore: 465.70193219184875\n",
      "  time_this_iter_s: 8.353458881378174\n",
      "  time_total_s: 465.70193219184875\n",
      "  timers:\n",
      "    learn_throughput: 701.346\n",
      "    learn_time_ms: 5703.319\n",
      "    load_throughput: 13478923.435\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 493.839\n",
      "    sample_time_ms: 8099.803\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1643384179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:24 (running for 00:07:56.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         465.702</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">0.118487</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -7.13142</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-36-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.008197963237762\n",
      "  episode_reward_mean: -0.001016828641295433\n",
      "  episode_reward_min: -7.131419330835342\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.507096665905367\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018050667015123534\n",
      "          policy_loss: -0.06251937347734647\n",
      "          total_loss: 1.5834334327233455\n",
      "          vf_explained_var: 0.9078191330996893\n",
      "          vf_loss: 1.637830003967849\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.476923076923075\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09710339842405677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0844569703366367\n",
      "    mean_inference_ms: 0.9616838273205847\n",
      "    mean_raw_obs_processing_ms: 0.08079057003127041\n",
      "  time_since_restore: 474.93275237083435\n",
      "  time_this_iter_s: 9.230820178985596\n",
      "  time_total_s: 474.93275237083435\n",
      "  timers:\n",
      "    learn_throughput: 686.837\n",
      "    learn_time_ms: 5823.795\n",
      "    load_throughput: 13391775.223\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 492.763\n",
      "    sample_time_ms: 8117.497\n",
      "    update_time_ms: 1.47\n",
      "  timestamp: 1643384189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:30 (running for 00:08:01.74)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         474.933</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-0.00101683</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -7.13142</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:35 (running for 00:08:06.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         474.933</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-0.00101683</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -7.13142</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.008197963237762\n",
      "  episode_reward_mean: -0.09248758397996426\n",
      "  episode_reward_min: -7.131419330835342\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.566638717856458\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018088308549289882\n",
      "          policy_loss: -0.07908953122384045\n",
      "          total_loss: 2.662073724711895\n",
      "          vf_explained_var: 0.8720562233078865\n",
      "          vf_loss: 2.733023530340964\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.984615384615385\n",
      "    ram_util_percent: 34.00769230769231\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09710649230307312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08446541138966801\n",
      "    mean_inference_ms: 0.9617674371735151\n",
      "    mean_raw_obs_processing_ms: 0.08080828001811614\n",
      "  time_since_restore: 484.1885793209076\n",
      "  time_this_iter_s: 9.255826950073242\n",
      "  time_total_s: 484.1885793209076\n",
      "  timers:\n",
      "    learn_throughput: 679.312\n",
      "    learn_time_ms: 5888.308\n",
      "    load_throughput: 13283623.12\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 482.151\n",
      "    sample_time_ms: 8296.158\n",
      "    update_time_ms: 1.497\n",
      "  timestamp: 1643384198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:40 (running for 00:08:12.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         484.189</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-0.0924876</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -7.13142</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:45 (running for 00:08:17.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         484.189</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-0.0924876</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -7.13142</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-36-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.008197963237762\n",
      "  episode_reward_mean: 0.023153260052204132\n",
      "  episode_reward_min: -9.60010814666748\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.539958589820452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01848710654096903\n",
      "          policy_loss: -0.07039804712278389\n",
      "          total_loss: 2.9114754910355494\n",
      "          vf_explained_var: 0.8637261107403745\n",
      "          vf_loss: 2.9735543342207067\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.54166666666666\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09710847817496333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08448474493235608\n",
      "    mean_inference_ms: 0.9618229125800576\n",
      "    mean_raw_obs_processing_ms: 0.08084071522711515\n",
      "  time_since_restore: 492.4975759983063\n",
      "  time_this_iter_s: 8.308996677398682\n",
      "  time_total_s: 492.4975759983063\n",
      "  timers:\n",
      "    learn_throughput: 679.465\n",
      "    learn_time_ms: 5886.988\n",
      "    load_throughput: 12526854.327\n",
      "    load_time_ms: 0.319\n",
      "    sample_throughput: 477.348\n",
      "    sample_time_ms: 8379.637\n",
      "    update_time_ms: 1.48\n",
      "  timestamp: 1643384206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:50 (running for 00:08:22.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         492.498</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">0.0231533</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.008197963237762\n",
      "  episode_reward_mean: -0.04340485453605652\n",
      "  episode_reward_min: -9.60010814666748\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.480143580898162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017573055282229234\n",
      "          policy_loss: -0.08948531885548765\n",
      "          total_loss: 1.5567854273503505\n",
      "          vf_explained_var: 0.9115341537101295\n",
      "          vf_loss: 1.6383628663837269\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.825\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09712631383871434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08451777475647614\n",
      "    mean_inference_ms: 0.9620595531813768\n",
      "    mean_raw_obs_processing_ms: 0.08088699646168776\n",
      "  time_since_restore: 501.0749862194061\n",
      "  time_this_iter_s: 8.577410221099854\n",
      "  time_total_s: 501.0749862194061\n",
      "  timers:\n",
      "    learn_throughput: 674.646\n",
      "    learn_time_ms: 5929.034\n",
      "    load_throughput: 12639156.245\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 478.245\n",
      "    sample_time_ms: 8363.919\n",
      "    update_time_ms: 1.461\n",
      "  timestamp: 1643384215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:36:56 (running for 00:08:27.95)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         501.075</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">-0.0434049</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:01 (running for 00:08:32.96)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         501.075</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">-0.0434049</td><td style=\"text-align: right;\">             10.0082</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-37-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.863148808479309\n",
      "  episode_reward_mean: 0.03618455350399017\n",
      "  episode_reward_min: -9.60010814666748\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.417073587192002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01744627890473055\n",
      "          policy_loss: -0.052915715075708844\n",
      "          total_loss: 1.9842060855209267\n",
      "          vf_explained_var: 0.9045376159170623\n",
      "          vf_loss: 2.0292709754038882\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.007692307692306\n",
      "    ram_util_percent: 34.03076923076923\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09716815684486509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0845693394725512\n",
      "    mean_inference_ms: 0.9625188249829262\n",
      "    mean_raw_obs_processing_ms: 0.08093259893953697\n",
      "  time_since_restore: 509.74149799346924\n",
      "  time_this_iter_s: 8.66651177406311\n",
      "  time_total_s: 509.74149799346924\n",
      "  timers:\n",
      "    learn_throughput: 669.457\n",
      "    learn_time_ms: 5974.993\n",
      "    load_throughput: 12577566.534\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 474.262\n",
      "    sample_time_ms: 8434.164\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1643384223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:06 (running for 00:08:38.64)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         509.741</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">0.0361846</td><td style=\"text-align: right;\">             8.86315</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-37-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.644638061523438\n",
      "  episode_reward_mean: 0.07228001073002815\n",
      "  episode_reward_min: -9.60010814666748\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.316265549198274\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018510140765792424\n",
      "          policy_loss: -0.05542455250146969\n",
      "          total_loss: 4.10079963737598\n",
      "          vf_explained_var: 0.859345071482402\n",
      "          vf_loss: 4.147894592599203\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.98181818181818\n",
      "    ram_util_percent: 34.08181818181819\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09720793471153723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08461899656365794\n",
      "    mean_inference_ms: 0.9629493916742264\n",
      "    mean_raw_obs_processing_ms: 0.08097460674864639\n",
      "  time_since_restore: 517.5909461975098\n",
      "  time_this_iter_s: 7.849448204040527\n",
      "  time_total_s: 517.5909461975098\n",
      "  timers:\n",
      "    learn_throughput: 669.795\n",
      "    learn_time_ms: 5971.976\n",
      "    load_throughput: 12559676.598\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 471.264\n",
      "    sample_time_ms: 8487.804\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1643384231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:12 (running for 00:08:44.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         517.591</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> 0.07228</td><td style=\"text-align: right;\">             10.6446</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:17 (running for 00:08:49.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         517.591</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> 0.07228</td><td style=\"text-align: right;\">             10.6446</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:22 (running for 00:08:54.53)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         517.591</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> 0.07228</td><td style=\"text-align: right;\">             10.6446</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-37-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.644638061523438\n",
      "  episode_reward_mean: 0.04166523970663547\n",
      "  episode_reward_min: -9.60010814666748\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.232675117574713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018260108639077302\n",
      "          policy_loss: -0.0882935922212338\n",
      "          total_loss: 1.118365711849984\n",
      "          vf_explained_var: 0.9235674855529621\n",
      "          vf_loss: 1.1984422508907575\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.305882352941175\n",
      "    ram_util_percent: 34.01176470588236\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09726585093078416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0847024336918536\n",
      "    mean_inference_ms: 0.9635490213272153\n",
      "    mean_raw_obs_processing_ms: 0.08103902138869681\n",
      "  time_since_restore: 529.5239486694336\n",
      "  time_this_iter_s: 11.933002471923828\n",
      "  time_total_s: 529.5239486694336\n",
      "  timers:\n",
      "    learn_throughput: 635.096\n",
      "    learn_time_ms: 6298.265\n",
      "    load_throughput: 12340725.267\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 467.227\n",
      "    sample_time_ms: 8561.141\n",
      "    update_time_ms: 1.534\n",
      "  timestamp: 1643384243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:28 (running for 00:09:00.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         529.524</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">0.0416652</td><td style=\"text-align: right;\">             10.6446</td><td style=\"text-align: right;\">            -9.60011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-37-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.289928317070007\n",
      "  episode_reward_mean: 0.08390907913446427\n",
      "  episode_reward_min: -8.535247415304184\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.1868301053201\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018660874855179043\n",
      "          policy_loss: -0.05436957777147332\n",
      "          total_loss: 2.2421847740533734\n",
      "          vf_explained_var: 0.9102756554721504\n",
      "          vf_loss: 2.2881569829679305\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.7\n",
      "    ram_util_percent: 34.04285714285715\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09749046625892649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08491443492145206\n",
      "    mean_inference_ms: 0.9657789268235516\n",
      "    mean_raw_obs_processing_ms: 0.08120888024911264\n",
      "  time_since_restore: 539.1952073574066\n",
      "  time_this_iter_s: 9.671258687973022\n",
      "  time_total_s: 539.1952073574066\n",
      "  timers:\n",
      "    learn_throughput: 634.65\n",
      "    learn_time_ms: 6302.683\n",
      "    load_throughput: 12182120.244\n",
      "    load_time_ms: 0.328\n",
      "    sample_throughput: 443.372\n",
      "    sample_time_ms: 9021.763\n",
      "    update_time_ms: 1.545\n",
      "  timestamp: 1643384253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:34 (running for 00:09:06.18)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         539.195</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">0.0839091</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -8.53525</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:39 (running for 00:09:11.18)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         539.195</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">0.0839091</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -8.53525</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-37-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.289928317070007\n",
      "  episode_reward_mean: 0.0787227514386177\n",
      "  episode_reward_min: -9.272038161754608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.115332243519445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019309661418447078\n",
      "          policy_loss: -0.05593343532534056\n",
      "          total_loss: 1.8536484004897116\n",
      "          vf_explained_var: 0.909294073171513\n",
      "          vf_loss: 1.900892482553759\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.327272727272728\n",
      "    ram_util_percent: 34.06363636363637\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09770396841080965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08511901266830682\n",
      "    mean_inference_ms: 0.9679105087541422\n",
      "    mean_raw_obs_processing_ms: 0.08137409374884966\n",
      "  time_since_restore: 547.2946131229401\n",
      "  time_this_iter_s: 8.099405765533447\n",
      "  time_total_s: 547.2946131229401\n",
      "  timers:\n",
      "    learn_throughput: 636.024\n",
      "    learn_time_ms: 6289.071\n",
      "    load_throughput: 12222946.233\n",
      "    load_time_ms: 0.327\n",
      "    sample_throughput: 442.841\n",
      "    sample_time_ms: 9032.586\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1643384261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:44 (running for 00:09:16.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         547.295</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">0.0787228</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -9.27204</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:49 (running for 00:09:21.31)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         547.295</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">0.0787228</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -9.27204</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-37-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.289928317070007\n",
      "  episode_reward_mean: -0.05596471548080444\n",
      "  episode_reward_min: -9.272038161754608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.121543204399847\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01919646099584867\n",
      "          policy_loss: -0.0725223859812143\n",
      "          total_loss: 1.4513602569013313\n",
      "          vf_explained_var: 0.9161677470771216\n",
      "          vf_loss: 1.515244235947568\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.541666666666668\n",
      "    ram_util_percent: 34.03333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09788651353469813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08529568033313124\n",
      "    mean_inference_ms: 0.9697271215414311\n",
      "    mean_raw_obs_processing_ms: 0.08151351573183307\n",
      "  time_since_restore: 555.493173122406\n",
      "  time_this_iter_s: 8.198559999465942\n",
      "  time_total_s: 555.493173122406\n",
      "  timers:\n",
      "    learn_throughput: 636.945\n",
      "    learn_time_ms: 6279.976\n",
      "    load_throughput: 12200724.311\n",
      "    load_time_ms: 0.328\n",
      "    sample_throughput: 443.83\n",
      "    sample_time_ms: 9012.466\n",
      "    update_time_ms: 1.451\n",
      "  timestamp: 1643384269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:37:54 (running for 00:09:26.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         555.493</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">-0.0559647</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -9.27204</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-37-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.289928317070007\n",
      "  episode_reward_mean: -0.004190225005149841\n",
      "  episode_reward_min: -9.272038161754608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 9.063607040528328\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01874071812028425\n",
      "          policy_loss: -0.05607741062319086\n",
      "          total_loss: 1.5982894103438343\n",
      "          vf_explained_var: 0.9155763603025867\n",
      "          vf_loss: 1.6459335009897909\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.883333333333336\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09806703736208053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08547106930153138\n",
      "    mean_inference_ms: 0.971529350876004\n",
      "    mean_raw_obs_processing_ms: 0.0816542594631366\n",
      "  time_since_restore: 563.869987487793\n",
      "  time_this_iter_s: 8.376814365386963\n",
      "  time_total_s: 563.869987487793\n",
      "  timers:\n",
      "    learn_throughput: 645.458\n",
      "    learn_time_ms: 6197.154\n",
      "    load_throughput: 12409183.432\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 444.398\n",
      "    sample_time_ms: 9000.941\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1643384278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:00 (running for 00:09:31.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">          563.87</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-0.00419023</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -9.27204</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:05 (running for 00:09:36.93)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">          563.87</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-0.00419023</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -9.27204</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-38-07\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.289928317070007\n",
      "  episode_reward_mean: -0.004871810227632523\n",
      "  episode_reward_min: -9.63742470741272\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.991185942003804\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018863229112931704\n",
      "          policy_loss: -0.07391500045615498\n",
      "          total_loss: 1.8755165648155956\n",
      "          vf_explained_var: 0.9078023401639794\n",
      "          vf_loss: 1.9409431150363337\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.700000000000003\n",
      "    ram_util_percent: 34.0846153846154\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09817320605345714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08557759216688637\n",
      "    mean_inference_ms: 0.9725933301132497\n",
      "    mean_raw_obs_processing_ms: 0.08173177915446785\n",
      "  time_since_restore: 573.1818709373474\n",
      "  time_this_iter_s: 9.311883449554443\n",
      "  time_total_s: 573.1818709373474\n",
      "  timers:\n",
      "    learn_throughput: 640.972\n",
      "    learn_time_ms: 6240.526\n",
      "    load_throughput: 11992291.637\n",
      "    load_time_ms: 0.334\n",
      "    sample_throughput: 450.461\n",
      "    sample_time_ms: 8879.786\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1643384287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:10 (running for 00:09:42.26)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         573.182</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-0.00487181</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -9.63742</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:15 (running for 00:09:47.27)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         573.182</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-0.00487181</td><td style=\"text-align: right;\">             11.2899</td><td style=\"text-align: right;\">            -9.63742</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-38-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.109760478138924\n",
      "  episode_reward_mean: -0.1113457015901804\n",
      "  episode_reward_min: -9.700865864753723\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.87790710797874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017786959169766928\n",
      "          policy_loss: -0.08798318713382687\n",
      "          total_loss: 3.0088104344143343\n",
      "          vf_explained_var: 0.8749250583751227\n",
      "          vf_loss: 3.0887895013055493\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.630769230769232\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09815136038120284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08556870113019598\n",
      "    mean_inference_ms: 0.9724380686280847\n",
      "    mean_raw_obs_processing_ms: 0.08171200264381862\n",
      "  time_since_restore: 582.0179374217987\n",
      "  time_this_iter_s: 8.836066484451294\n",
      "  time_total_s: 582.0179374217987\n",
      "  timers:\n",
      "    learn_throughput: 639.198\n",
      "    learn_time_ms: 6257.84\n",
      "    load_throughput: 13141079.345\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 446.432\n",
      "    sample_time_ms: 8959.93\n",
      "    update_time_ms: 1.48\n",
      "  timestamp: 1643384296\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:21 (running for 00:09:53.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         582.018</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">-0.111346</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -9.70087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.109760478138924\n",
      "  episode_reward_mean: 0.041460651457309726\n",
      "  episode_reward_min: -9.700865864753723\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.822024025455597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020471879385126533\n",
      "          policy_loss: -0.048354695033362156\n",
      "          total_loss: 3.2165380311589087\n",
      "          vf_explained_var: 0.863965008848457\n",
      "          vf_loss: 3.2556803828766268\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.999999999999996\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09815349667654008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08557513261189623\n",
      "    mean_inference_ms: 0.9724996850083008\n",
      "    mean_raw_obs_processing_ms: 0.08170643555882805\n",
      "  time_since_restore: 590.5503151416779\n",
      "  time_this_iter_s: 8.53237771987915\n",
      "  time_total_s: 590.5503151416779\n",
      "  timers:\n",
      "    learn_throughput: 641.184\n",
      "    learn_time_ms: 6238.459\n",
      "    load_throughput: 13210406.299\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 444.822\n",
      "    sample_time_ms: 8992.353\n",
      "    update_time_ms: 1.452\n",
      "  timestamp: 1643384304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:27 (running for 00:09:58.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">          590.55</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">0.0414607</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -9.70087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:32 (running for 00:10:03.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">          590.55</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">0.0414607</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -9.70087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.109760478138924\n",
      "  episode_reward_mean: 0.03961603805422783\n",
      "  episode_reward_min: -9.700865864753723\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.815630660518524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016648084490155073\n",
      "          policy_loss: -0.0693997244205406\n",
      "          total_loss: 2.035310410027222\n",
      "          vf_explained_var: 0.9076389640249232\n",
      "          vf_loss: 2.0934726637217307\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.683333333333334\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09817718613959846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08559524170731843\n",
      "    mean_inference_ms: 0.9729162989400285\n",
      "    mean_raw_obs_processing_ms: 0.08171763716191646\n",
      "  time_since_restore: 599.204992055893\n",
      "  time_this_iter_s: 8.654676914215088\n",
      "  time_total_s: 599.204992055893\n",
      "  timers:\n",
      "    learn_throughput: 641.862\n",
      "    learn_time_ms: 6231.869\n",
      "    load_throughput: 13148288.401\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 445.542\n",
      "    sample_time_ms: 8977.824\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1643384313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:37 (running for 00:10:09.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         599.205</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">0.039616</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -9.70087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-38-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.109760478138924\n",
      "  episode_reward_mean: -0.010988051444292069\n",
      "  episode_reward_min: -9.700865864753723\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.758519689498408\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015922600119634627\n",
      "          policy_loss: -0.060030732091556314\n",
      "          total_loss: 1.8204276151334247\n",
      "          vf_explained_var: 0.9100072706258425\n",
      "          vf_loss: 1.8697105932620264\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.099999999999998\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09821173020499918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08562128467074277\n",
      "    mean_inference_ms: 0.9734073655731481\n",
      "    mean_raw_obs_processing_ms: 0.08173451098345078\n",
      "  time_since_restore: 607.632853269577\n",
      "  time_this_iter_s: 8.427861213684082\n",
      "  time_total_s: 607.632853269577\n",
      "  timers:\n",
      "    learn_throughput: 636.572\n",
      "    learn_time_ms: 6283.659\n",
      "    load_throughput: 13161697.654\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 445.543\n",
      "    sample_time_ms: 8977.802\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1643384322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:43 (running for 00:10:14.81)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         607.633</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">-0.0109881</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -9.70087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:48 (running for 00:10:19.82)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         607.633</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">-0.0109881</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -9.70087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-38-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.109760478138924\n",
      "  episode_reward_mean: 0.05054041504859924\n",
      "  episode_reward_min: -10.507900297641754\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.76466216118105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016560796010796356\n",
      "          policy_loss: -0.057579530907734745\n",
      "          total_loss: 2.619716302688003\n",
      "          vf_explained_var: 0.8815663048016128\n",
      "          vf_loss: 2.666117293587936\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.241666666666667\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09826264629928534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08566086742960494\n",
      "    mean_inference_ms: 0.9740346633571605\n",
      "    mean_raw_obs_processing_ms: 0.08176356243162393\n",
      "  time_since_restore: 616.2710194587708\n",
      "  time_this_iter_s: 8.638166189193726\n",
      "  time_total_s: 616.2710194587708\n",
      "  timers:\n",
      "    learn_throughput: 666.54\n",
      "    learn_time_ms: 6001.141\n",
      "    load_throughput: 13231242.902\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 445.279\n",
      "    sample_time_ms: 8983.122\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1643384330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:53 (running for 00:10:25.46)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         616.271</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">0.0505404</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:38:58 (running for 00:10:30.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         616.271</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">0.0505404</td><td style=\"text-align: right;\">             14.1098</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-38-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.328743174672127\n",
      "  episode_reward_mean: 0.08619278199970722\n",
      "  episode_reward_min: -10.507900297641754\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.80497657304169\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015872680797067846\n",
      "          policy_loss: -0.0422908318469361\n",
      "          total_loss: 2.134394140757861\n",
      "          vf_explained_var: 0.9127019697620022\n",
      "          vf_loss: 2.1659709048367315\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.224999999999998\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09826682890999533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08566774547518172\n",
      "    mean_inference_ms: 0.9741845455915851\n",
      "    mean_raw_obs_processing_ms: 0.08176068754785785\n",
      "  time_since_restore: 624.4984121322632\n",
      "  time_this_iter_s: 8.227392673492432\n",
      "  time_total_s: 624.4984121322632\n",
      "  timers:\n",
      "    learn_throughput: 667.111\n",
      "    learn_time_ms: 5996.004\n",
      "    load_throughput: 13427143.657\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 467.358\n",
      "    sample_time_ms: 8558.744\n",
      "    update_time_ms: 1.458\n",
      "  timestamp: 1643384339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:04 (running for 00:10:35.72)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         624.498</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">0.0861928</td><td style=\"text-align: right;\">             11.3287</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-39-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.602645050734282\n",
      "  episode_reward_mean: -0.07116256058216094\n",
      "  episode_reward_min: -10.507900297641754\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.78812366198468\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01782952667558548\n",
      "          policy_loss: -0.059286876641694575\n",
      "          total_loss: 1.3021540331203612\n",
      "          vf_explained_var: 0.923457060898504\n",
      "          vf_loss: 1.3494059795013038\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.99230769230769\n",
      "    ram_util_percent: 34.13846153846155\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09828188946380333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08568282082557711\n",
      "    mean_inference_ms: 0.9744157639399907\n",
      "    mean_raw_obs_processing_ms: 0.08176800420058805\n",
      "  time_since_restore: 634.1775979995728\n",
      "  time_this_iter_s: 9.67918586730957\n",
      "  time_total_s: 634.1775979995728\n",
      "  timers:\n",
      "    learn_throughput: 652.936\n",
      "    learn_time_ms: 6126.176\n",
      "    load_throughput: 13433594.363\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 466.123\n",
      "    sample_time_ms: 8581.417\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1643384348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:09 (running for 00:10:41.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         634.178</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-0.0711626</td><td style=\"text-align: right;\">             8.60265</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:14 (running for 00:10:46.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         634.178</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-0.0711626</td><td style=\"text-align: right;\">             8.60265</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-39-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.602645050734282\n",
      "  episode_reward_mean: -0.026527581214904786\n",
      "  episode_reward_min: -10.507900297641754\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.794290594900808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015876709318974933\n",
      "          policy_loss: -0.07254153591310306\n",
      "          total_loss: 1.7373014246584266\n",
      "          vf_explained_var: 0.912072979442535\n",
      "          vf_loss: 1.7991261690053888\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.557142857142857\n",
      "    ram_util_percent: 34.10714285714287\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09830406056155269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08570410071606044\n",
      "    mean_inference_ms: 0.9745521225820596\n",
      "    mean_raw_obs_processing_ms: 0.0817825978211916\n",
      "  time_since_restore: 643.2752594947815\n",
      "  time_this_iter_s: 9.09766149520874\n",
      "  time_total_s: 643.2752594947815\n",
      "  timers:\n",
      "    learn_throughput: 646.291\n",
      "    learn_time_ms: 6189.159\n",
      "    load_throughput: 12428488.036\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 457.731\n",
      "    sample_time_ms: 8738.749\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1643384357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:19 (running for 00:10:51.54)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         643.275</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">-0.0265276</td><td style=\"text-align: right;\">             8.60265</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:24 (running for 00:10:56.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         643.275</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">-0.0265276</td><td style=\"text-align: right;\">             8.60265</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-39-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.958253309130669\n",
      "  episode_reward_mean: -0.0329676365852356\n",
      "  episode_reward_min: -10.507900297641754\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.71394115263416\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015915553773201516\n",
      "          policy_loss: -0.08418508527640213\n",
      "          total_loss: 2.193797458687757\n",
      "          vf_explained_var: 0.9106837807804026\n",
      "          vf_loss: 2.2672395421131966\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.291666666666668\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.098334805797992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08574057281122763\n",
      "    mean_inference_ms: 0.9748171265911648\n",
      "    mean_raw_obs_processing_ms: 0.08180510327168529\n",
      "  time_since_restore: 651.8029067516327\n",
      "  time_this_iter_s: 8.527647256851196\n",
      "  time_total_s: 651.8029067516327\n",
      "  timers:\n",
      "    learn_throughput: 646.715\n",
      "    learn_time_ms: 6185.108\n",
      "    load_throughput: 12516574.157\n",
      "    load_time_ms: 0.32\n",
      "    sample_throughput: 453.47\n",
      "    sample_time_ms: 8820.872\n",
      "    update_time_ms: 1.509\n",
      "  timestamp: 1643384366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:30 (running for 00:11:02.09)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         651.803</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">-0.0329676</td><td style=\"text-align: right;\">             7.95825</td><td style=\"text-align: right;\">            -10.5079</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-39-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.450956881046295\n",
      "  episode_reward_mean: -0.030525269880890847\n",
      "  episode_reward_min: -7.302322119474411\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.634236142968618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01566955465017604\n",
      "          policy_loss: -0.0880840487648002\n",
      "          total_loss: 1.248652432011741\n",
      "          vf_explained_var: 0.9176515260691284\n",
      "          vf_loss: 1.3261595276414706\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.59090909090909\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09834600107432488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08576056969728155\n",
      "    mean_inference_ms: 0.9748847543293313\n",
      "    mean_raw_obs_processing_ms: 0.08181201538572008\n",
      "  time_since_restore: 659.9309132099152\n",
      "  time_this_iter_s: 8.12800645828247\n",
      "  time_total_s: 659.9309132099152\n",
      "  timers:\n",
      "    learn_throughput: 658.693\n",
      "    learn_time_ms: 6072.633\n",
      "    load_throughput: 12690783.661\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 453.961\n",
      "    sample_time_ms: 8811.339\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1643384374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:35 (running for 00:11:07.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         659.931</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">-0.0305253</td><td style=\"text-align: right;\">             7.45096</td><td style=\"text-align: right;\">            -7.30232</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:40 (running for 00:11:12.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         659.931</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">-0.0305253</td><td style=\"text-align: right;\">             7.45096</td><td style=\"text-align: right;\">            -7.30232</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-39-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.450956881046295\n",
      "  episode_reward_mean: -0.05930875301361084\n",
      "  episode_reward_min: -9.229682505130768\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.430648019236903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016745499198504238\n",
      "          policy_loss: -0.06673381107919399\n",
      "          total_loss: 2.560904563133735\n",
      "          vf_explained_var: 0.8816438271153357\n",
      "          vf_loss: 2.6163351595241537\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.774999999999995\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09836099535168649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0857832673608755\n",
      "    mean_inference_ms: 0.9749743622641179\n",
      "    mean_raw_obs_processing_ms: 0.08182005179909471\n",
      "  time_since_restore: 668.2279028892517\n",
      "  time_this_iter_s: 8.296989679336548\n",
      "  time_total_s: 668.2279028892517\n",
      "  timers:\n",
      "    learn_throughput: 659.999\n",
      "    learn_time_ms: 6060.617\n",
      "    load_throughput: 12649638.845\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 462.133\n",
      "    sample_time_ms: 8655.518\n",
      "    update_time_ms: 1.442\n",
      "  timestamp: 1643384382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:45 (running for 00:11:17.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         668.228</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">-0.0593088</td><td style=\"text-align: right;\">             7.45096</td><td style=\"text-align: right;\">            -9.22968</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:50 (running for 00:11:22.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         668.228</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">-0.0593088</td><td style=\"text-align: right;\">             7.45096</td><td style=\"text-align: right;\">            -9.22968</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-39-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.450956881046295\n",
      "  episode_reward_mean: -0.12993623822927475\n",
      "  episode_reward_min: -10.652702689170837\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.405011307295933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015247599575502938\n",
      "          policy_loss: -0.08881724069235467\n",
      "          total_loss: 2.1800518092103505\n",
      "          vf_explained_var: 0.8962377522581367\n",
      "          vf_loss: 2.2585769180168387\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.915384615384614\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09838326285054169\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08580734040524415\n",
      "    mean_inference_ms: 0.9751099081547213\n",
      "    mean_raw_obs_processing_ms: 0.08182929774478938\n",
      "  time_since_restore: 676.9438781738281\n",
      "  time_this_iter_s: 8.715975284576416\n",
      "  time_total_s: 676.9438781738281\n",
      "  timers:\n",
      "    learn_throughput: 659.365\n",
      "    learn_time_ms: 6066.439\n",
      "    load_throughput: 12648685.163\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 462.135\n",
      "    sample_time_ms: 8655.486\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1643384391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:39:56 (running for 00:11:28.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         676.944</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">-0.129936</td><td style=\"text-align: right;\">             7.45096</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-40-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.973640471696854\n",
      "  episode_reward_mean: 0.011550167798995972\n",
      "  episode_reward_min: -10.652702689170837\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.290285906227686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016589342725630254\n",
      "          policy_loss: -0.07072573523738131\n",
      "          total_loss: 1.8655497039683284\n",
      "          vf_explained_var: 0.9073987389764478\n",
      "          vf_loss: 1.925077628480491\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.008333333333336\n",
      "    ram_util_percent: 34.166666666666664\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09839278673536589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08582187031506588\n",
      "    mean_inference_ms: 0.975112795025067\n",
      "    mean_raw_obs_processing_ms: 0.08182666337225493\n",
      "  time_since_restore: 685.8691334724426\n",
      "  time_this_iter_s: 8.925255298614502\n",
      "  time_total_s: 685.8691334724426\n",
      "  timers:\n",
      "    learn_throughput: 654.508\n",
      "    learn_time_ms: 6111.465\n",
      "    load_throughput: 12643918.909\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 462.749\n",
      "    sample_time_ms: 8643.987\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1643384400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:02 (running for 00:11:34.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         685.869</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">0.0115502</td><td style=\"text-align: right;\">             6.97364</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:07 (running for 00:11:39.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         685.869</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">0.0115502</td><td style=\"text-align: right;\">             6.97364</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-40-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.868246018886566\n",
      "  episode_reward_mean: -0.023096393197774887\n",
      "  episode_reward_min: -10.652702689170837\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.193837542174965\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016814911080385648\n",
      "          policy_loss: -0.06874043767275388\n",
      "          total_loss: 2.4347791492748727\n",
      "          vf_explained_var: 0.898274734007415\n",
      "          vf_loss: 2.492169506879904\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.983333333333334\n",
      "    ram_util_percent: 34.175\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0984074565555857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08583201040016092\n",
      "    mean_inference_ms: 0.9751160197236345\n",
      "    mean_raw_obs_processing_ms: 0.08182876395991973\n",
      "  time_since_restore: 694.2994894981384\n",
      "  time_this_iter_s: 8.4303560256958\n",
      "  time_total_s: 694.2994894981384\n",
      "  timers:\n",
      "    learn_throughput: 655.902\n",
      "    learn_time_ms: 6098.476\n",
      "    load_throughput: 12560616.905\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 459.643\n",
      "    sample_time_ms: 8702.408\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1643384409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:13 (running for 00:11:44.71)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         694.299</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">-0.0230964</td><td style=\"text-align: right;\">             8.86825</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-40-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.021727949380875\n",
      "  episode_reward_mean: -0.07160794518887997\n",
      "  episode_reward_min: -10.652702689170837\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.135219286846858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016796715805051928\n",
      "          policy_loss: -0.07722371660012711\n",
      "          total_loss: 2.976447506780706\n",
      "          vf_explained_var: 0.9003121429233141\n",
      "          vf_loss: 3.0423334452734196\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.616666666666667\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09841641483590066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08583891077348277\n",
      "    mean_inference_ms: 0.9750755849982948\n",
      "    mean_raw_obs_processing_ms: 0.08182704992607218\n",
      "  time_since_restore: 702.3179004192352\n",
      "  time_this_iter_s: 8.018410921096802\n",
      "  time_total_s: 702.3179004192352\n",
      "  timers:\n",
      "    learn_throughput: 660.046\n",
      "    learn_time_ms: 6060.184\n",
      "    load_throughput: 12592671.32\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 461.585\n",
      "    sample_time_ms: 8665.785\n",
      "    update_time_ms: 1.462\n",
      "  timestamp: 1643384417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:18 (running for 00:11:49.74)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         702.318</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">-0.0716079</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:23 (running for 00:11:54.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         702.318</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">-0.0716079</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.021727949380875\n",
      "  episode_reward_mean: 0.1061735537648201\n",
      "  episode_reward_min: -10.652702689170837\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 8.05512580051217\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017860052557471504\n",
      "          policy_loss: -0.05566838763814459\n",
      "          total_loss: 2.2161092899178945\n",
      "          vf_explained_var: 0.906590360851698\n",
      "          vf_loss: 2.2597221430591357\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.09166666666667\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09842328914824723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08584386378862358\n",
      "    mean_inference_ms: 0.9750237301570899\n",
      "    mean_raw_obs_processing_ms: 0.08182193303046724\n",
      "  time_since_restore: 710.7529754638672\n",
      "  time_this_iter_s: 8.435075044631958\n",
      "  time_total_s: 710.7529754638672\n",
      "  timers:\n",
      "    learn_throughput: 657.671\n",
      "    learn_time_ms: 6082.072\n",
      "    load_throughput: 12487693.338\n",
      "    load_time_ms: 0.32\n",
      "    sample_throughput: 463.68\n",
      "    sample_time_ms: 8626.64\n",
      "    update_time_ms: 1.421\n",
      "  timestamp: 1643384425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:28 (running for 00:12:00.20)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         710.753</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">0.106174</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:33 (running for 00:12:05.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         710.753</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">0.106174</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -10.6527</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-40-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.021727949380875\n",
      "  episode_reward_mean: 0.09627711713314056\n",
      "  episode_reward_min: -10.62423712015152\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.855611252528365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016185909078218253\n",
      "          policy_loss: -0.05932076845057709\n",
      "          total_loss: 1.5723086091900065\n",
      "          vf_explained_var: 0.9134065611388094\n",
      "          vf_loss: 1.6207038885483178\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.353846153846156\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09842160605246086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08584906373864915\n",
      "    mean_inference_ms: 0.9748881330093269\n",
      "    mean_raw_obs_processing_ms: 0.08181028604962476\n",
      "  time_since_restore: 719.7449855804443\n",
      "  time_this_iter_s: 8.992010116577148\n",
      "  time_total_s: 719.7449855804443\n",
      "  timers:\n",
      "    learn_throughput: 665.05\n",
      "    learn_time_ms: 6014.584\n",
      "    load_throughput: 12472839.194\n",
      "    load_time_ms: 0.321\n",
      "    sample_throughput: 462.576\n",
      "    sample_time_ms: 8647.22\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1643384434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:38 (running for 00:12:10.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         719.745</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">0.0962771</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -10.6242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.021727949380875\n",
      "  episode_reward_mean: 0.01384128898382187\n",
      "  episode_reward_min: -10.62423712015152\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.828629307080341\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01783598565514681\n",
      "          policy_loss: -0.07897910008267049\n",
      "          total_loss: 1.4589721291916826\n",
      "          vf_explained_var: 0.9241350789864858\n",
      "          vf_loss: 1.5259119435183464\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.48333333333333\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0984314605036043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08586570356137184\n",
      "    mean_inference_ms: 0.974896431578137\n",
      "    mean_raw_obs_processing_ms: 0.081808712893065\n",
      "  time_since_restore: 728.4463167190552\n",
      "  time_this_iter_s: 8.70133113861084\n",
      "  time_total_s: 728.4463167190552\n",
      "  timers:\n",
      "    learn_throughput: 669.529\n",
      "    learn_time_ms: 5974.348\n",
      "    load_throughput: 13422846.628\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 466.179\n",
      "    sample_time_ms: 8580.397\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1643384443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:44 (running for 00:12:15.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         728.446</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">0.0138413</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -10.6242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:49 (running for 00:12:20.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         728.446</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">0.0138413</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -10.6242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-40-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.021727949380875\n",
      "  episode_reward_mean: 0.14228468030691147\n",
      "  episode_reward_min: -7.800182431936264\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.7553597327201595\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017143970191487164\n",
      "          policy_loss: -0.05272871959724173\n",
      "          total_loss: 2.8173618871097763\n",
      "          vf_explained_var: 0.88243552991139\n",
      "          vf_loss: 2.8585184354935924\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.200000000000003\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09845885454926051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0858901327680914\n",
      "    mean_inference_ms: 0.974943660273214\n",
      "    mean_raw_obs_processing_ms: 0.08180691000173945\n",
      "  time_since_restore: 737.16037940979\n",
      "  time_this_iter_s: 8.714062690734863\n",
      "  time_total_s: 737.16037940979\n",
      "  timers:\n",
      "    learn_throughput: 668.044\n",
      "    learn_time_ms: 5987.626\n",
      "    load_throughput: 13132850.098\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 468.086\n",
      "    sample_time_ms: 8545.445\n",
      "    update_time_ms: 1.389\n",
      "  timestamp: 1643384451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:40:55 (running for 00:12:26.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">          737.16</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">0.142285</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -7.80018</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:00 (running for 00:12:31.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">          737.16</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">0.142285</td><td style=\"text-align: right;\">             10.0217</td><td style=\"text-align: right;\">            -7.80018</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-41-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.97196251153946\n",
      "  episode_reward_mean: 0.09596649259328842\n",
      "  episode_reward_min: -10.378554821014404\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.6126520638824795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017547607590333313\n",
      "          policy_loss: -0.08182310609106895\n",
      "          total_loss: 2.6769079592269196\n",
      "          vf_explained_var: 0.8817963302135468\n",
      "          vf_loss: 2.746886425184947\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.0\n",
      "    ram_util_percent: 34.207142857142856\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0985381673479953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0859532100031392\n",
      "    mean_inference_ms: 0.9755160529250579\n",
      "    mean_raw_obs_processing_ms: 0.08184628349674768\n",
      "  time_since_restore: 747.2964873313904\n",
      "  time_this_iter_s: 10.136107921600342\n",
      "  time_total_s: 747.2964873313904\n",
      "  timers:\n",
      "    learn_throughput: 652.017\n",
      "    learn_time_ms: 6134.807\n",
      "    load_throughput: 12928424.135\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 464.441\n",
      "    sample_time_ms: 8612.502\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1643384462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:05 (running for 00:12:36.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         747.296</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">0.0959665</td><td style=\"text-align: right;\">             9.97196</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:10 (running for 00:12:41.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         747.296</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">0.0959665</td><td style=\"text-align: right;\">             9.97196</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.97196251153946\n",
      "  episode_reward_mean: -0.06119426190853119\n",
      "  episode_reward_min: -10.378554821014404\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.582902398673437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016548739707752388\n",
      "          policy_loss: -0.05640206177348411\n",
      "          total_loss: 1.1549428715841263\n",
      "          vf_explained_var: 0.9334518380062554\n",
      "          vf_loss: 1.2001745382143605\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.492307692307694\n",
      "    ram_util_percent: 34.284615384615385\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09863680167960019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08603655529109883\n",
      "    mean_inference_ms: 0.9762917111042358\n",
      "    mean_raw_obs_processing_ms: 0.0819186201065336\n",
      "  time_since_restore: 756.0256843566895\n",
      "  time_this_iter_s: 8.729197025299072\n",
      "  time_total_s: 756.0256843566895\n",
      "  timers:\n",
      "    learn_throughput: 649.946\n",
      "    learn_time_ms: 6154.356\n",
      "    load_throughput: 12222946.233\n",
      "    load_time_ms: 0.327\n",
      "    sample_throughput: 455.378\n",
      "    sample_time_ms: 8783.917\n",
      "    update_time_ms: 1.391\n",
      "  timestamp: 1643384470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:15 (running for 00:12:47.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         756.026</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">-0.0611943</td><td style=\"text-align: right;\">             9.97196</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-41-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.97196251153946\n",
      "  episode_reward_mean: 0.004566696584224701\n",
      "  episode_reward_min: -10.378554821014404\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.524695248757639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01836608614298009\n",
      "          policy_loss: -0.08045698969014069\n",
      "          total_loss: 1.2056813157273716\n",
      "          vf_explained_var: 0.9268523313665903\n",
      "          vf_loss: 1.2737411947500321\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.125\n",
      "    ram_util_percent: 34.28333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09874235414599167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08613026552308074\n",
      "    mean_inference_ms: 0.9771982984443802\n",
      "    mean_raw_obs_processing_ms: 0.08200532516991366\n",
      "  time_since_restore: 764.7598760128021\n",
      "  time_this_iter_s: 8.734191656112671\n",
      "  time_total_s: 764.7598760128021\n",
      "  timers:\n",
      "    learn_throughput: 650.854\n",
      "    learn_time_ms: 6145.769\n",
      "    load_throughput: 11913944.042\n",
      "    load_time_ms: 0.336\n",
      "    sample_throughput: 453.807\n",
      "    sample_time_ms: 8814.317\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1643384479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:21 (running for 00:12:53.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">          764.76</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">0.0045667</td><td style=\"text-align: right;\">             9.97196</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:26 (running for 00:12:58.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">          764.76</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">0.0045667</td><td style=\"text-align: right;\">             9.97196</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-41-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.97196251153946\n",
      "  episode_reward_mean: 0.008240809440612793\n",
      "  episode_reward_min: -10.378554821014404\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.442667347385037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01703888864559402\n",
      "          policy_loss: -0.06018389337295566\n",
      "          total_loss: 1.518079202628935\n",
      "          vf_explained_var: 0.9242367693813899\n",
      "          vf_loss: 1.5667618407196897\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.675\n",
      "    ram_util_percent: 34.291666666666664\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09883286561609911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0862122778551673\n",
      "    mean_inference_ms: 0.9779779529438382\n",
      "    mean_raw_obs_processing_ms: 0.0820800515207802\n",
      "  time_since_restore: 773.1435704231262\n",
      "  time_this_iter_s: 8.383694410324097\n",
      "  time_total_s: 773.1435704231262\n",
      "  timers:\n",
      "    learn_throughput: 656.884\n",
      "    learn_time_ms: 6089.354\n",
      "    load_throughput: 12007741.197\n",
      "    load_time_ms: 0.333\n",
      "    sample_throughput: 454.156\n",
      "    sample_time_ms: 8807.544\n",
      "    update_time_ms: 1.424\n",
      "  timestamp: 1643384488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:32 (running for 00:13:03.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         773.144</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">0.00824081</td><td style=\"text-align: right;\">             9.97196</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-41-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.077113211154938\n",
      "  episode_reward_mean: -0.0981811174750328\n",
      "  episode_reward_min: -10.378554821014404\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.505701712639101\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018212742169881762\n",
      "          policy_loss: -0.07137050853481376\n",
      "          total_loss: 1.9762331742653123\n",
      "          vf_explained_var: 0.8958672005002216\n",
      "          vf_loss: 2.0353100859990683\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.107692307692307\n",
      "    ram_util_percent: 34.292307692307695\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0988977364238552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08628147331414482\n",
      "    mean_inference_ms: 0.9786856490455105\n",
      "    mean_raw_obs_processing_ms: 0.08215649787829724\n",
      "  time_since_restore: 781.8612155914307\n",
      "  time_this_iter_s: 8.717645168304443\n",
      "  time_total_s: 781.8612155914307\n",
      "  timers:\n",
      "    learn_throughput: 653.458\n",
      "    learn_time_ms: 6121.281\n",
      "    load_throughput: 12082108.599\n",
      "    load_time_ms: 0.331\n",
      "    sample_throughput: 457.282\n",
      "    sample_time_ms: 8747.329\n",
      "    update_time_ms: 1.465\n",
      "  timestamp: 1643384496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:37 (running for 00:13:09.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         781.861</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">-0.0981811</td><td style=\"text-align: right;\">             8.07711</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:42 (running for 00:13:14.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         781.861</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">-0.0981811</td><td style=\"text-align: right;\">             8.07711</td><td style=\"text-align: right;\">            -10.3786</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-41-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.8758865892887115\n",
      "  episode_reward_mean: -0.04110900342464447\n",
      "  episode_reward_min: -6.401967257261276\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.546214974823818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017673047464941315\n",
      "          policy_loss: -0.08818772745228583\n",
      "          total_loss: 1.6047165612509895\n",
      "          vf_explained_var: 0.9115231910059529\n",
      "          vf_loss: 1.6809749782245647\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.158333333333335\n",
      "    ram_util_percent: 34.26666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09891983632658724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08631918679165651\n",
      "    mean_inference_ms: 0.9789668220497322\n",
      "    mean_raw_obs_processing_ms: 0.08220021019875884\n",
      "  time_since_restore: 790.3379909992218\n",
      "  time_this_iter_s: 8.476775407791138\n",
      "  time_total_s: 790.3379909992218\n",
      "  timers:\n",
      "    learn_throughput: 649.736\n",
      "    learn_time_ms: 6156.349\n",
      "    load_throughput: 12194516.645\n",
      "    load_time_ms: 0.328\n",
      "    sample_throughput: 455.035\n",
      "    sample_time_ms: 8790.527\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1643384505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:48 (running for 00:13:20.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         790.338</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">-0.041109</td><td style=\"text-align: right;\">             6.87589</td><td style=\"text-align: right;\">            -6.40197</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.474287033081055\n",
      "  episode_reward_mean: -0.056199684143066406\n",
      "  episode_reward_min: -6.534778535366058\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.507759620810067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01757503699989183\n",
      "          policy_loss: -0.08975166885647923\n",
      "          total_loss: 3.058684509650113\n",
      "          vf_explained_var: 0.8710178868104053\n",
      "          vf_loss: 3.1365730249112653\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.11818181818182\n",
      "    ram_util_percent: 34.20909090909091\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09892458287554005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08633654652588234\n",
      "    mean_inference_ms: 0.9790981264720865\n",
      "    mean_raw_obs_processing_ms: 0.0822129709708765\n",
      "  time_since_restore: 798.2682416439056\n",
      "  time_this_iter_s: 7.930250644683838\n",
      "  time_total_s: 798.2682416439056\n",
      "  timers:\n",
      "    learn_throughput: 655.689\n",
      "    learn_time_ms: 6100.457\n",
      "    load_throughput: 12308132.932\n",
      "    load_time_ms: 0.325\n",
      "    sample_throughput: 452.958\n",
      "    sample_time_ms: 8830.832\n",
      "    update_time_ms: 1.488\n",
      "  timestamp: 1643384513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:54 (running for 00:13:25.95)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         798.268</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">-0.0561997</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -6.53478</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:41:59 (running for 00:13:30.95)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         798.268</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">-0.0561997</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -6.53478</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-42-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.474287033081055\n",
      "  episode_reward_mean: 0.0634514506906271\n",
      "  episode_reward_min: -6.534778535366058\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.570439386367798\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018094848240153304\n",
      "          policy_loss: -0.07143999273257871\n",
      "          total_loss: 1.6025579743898444\n",
      "          vf_explained_var: 0.9183338304360708\n",
      "          vf_loss: 1.6617839428686327\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.474999999999998\n",
      "    ram_util_percent: 34.25833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09889321216890234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08631811744041393\n",
      "    mean_inference_ms: 0.9788604758436099\n",
      "    mean_raw_obs_processing_ms: 0.08219340960825872\n",
      "  time_since_restore: 806.4421179294586\n",
      "  time_this_iter_s: 8.173876285552979\n",
      "  time_total_s: 806.4421179294586\n",
      "  timers:\n",
      "    learn_throughput: 661.541\n",
      "    learn_time_ms: 6046.488\n",
      "    load_throughput: 12319882.508\n",
      "    load_time_ms: 0.325\n",
      "    sample_throughput: 457.297\n",
      "    sample_time_ms: 8747.046\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1643384521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:04 (running for 00:13:36.14)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         806.442</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">0.0634515</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -6.53478</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.474287033081055\n",
      "  episode_reward_mean: 0.0015755078196525573\n",
      "  episode_reward_min: -6.709223747253418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.55000974593624\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017893757787537285\n",
      "          policy_loss: -0.058426603168169014\n",
      "          total_loss: 2.0437800025280755\n",
      "          vf_explained_var: 0.8981872625889317\n",
      "          vf_loss: 2.0901283220578266\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.13636363636364\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09884497834290601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08628362466572284\n",
      "    mean_inference_ms: 0.9784146219418244\n",
      "    mean_raw_obs_processing_ms: 0.08216038249765488\n",
      "  time_since_restore: 814.2319254875183\n",
      "  time_this_iter_s: 7.789807558059692\n",
      "  time_total_s: 814.2319254875183\n",
      "  timers:\n",
      "    learn_throughput: 667.361\n",
      "    learn_time_ms: 5993.757\n",
      "    load_throughput: 12408265.661\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 462.178\n",
      "    sample_time_ms: 8654.67\n",
      "    update_time_ms: 1.493\n",
      "  timestamp: 1643384529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:10 (running for 00:13:41.95)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         814.232</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">0.00157551</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -6.70922</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:15 (running for 00:13:46.96)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         814.232</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">0.00157551</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -6.70922</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-42-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.474287033081055\n",
      "  episode_reward_mean: -0.009646168351173401\n",
      "  episode_reward_min: -8.525794386863708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.616054367250012\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01736108895725089\n",
      "          policy_loss: -0.07174095074816417\n",
      "          total_loss: 2.096911507562524\n",
      "          vf_explained_var: 0.9077385330072013\n",
      "          vf_loss: 2.156933719897142\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.023076923076925\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09878290376158119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08623622613115849\n",
      "    mean_inference_ms: 0.9777997309116967\n",
      "    mean_raw_obs_processing_ms: 0.08210828984581874\n",
      "  time_since_restore: 822.9987945556641\n",
      "  time_this_iter_s: 8.766869068145752\n",
      "  time_total_s: 822.9987945556641\n",
      "  timers:\n",
      "    learn_throughput: 663.469\n",
      "    learn_time_ms: 6028.92\n",
      "    load_throughput: 12657273.482\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 466.668\n",
      "    sample_time_ms: 8571.412\n",
      "    update_time_ms: 1.604\n",
      "  timestamp: 1643384538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:21 (running for 00:13:52.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         822.999</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">-0.00964617</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -8.52579</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:26 (running for 00:13:57.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         822.999</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">-0.00964617</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -8.52579</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.474287033081055\n",
      "  episode_reward_mean: 0.0660619755089283\n",
      "  episode_reward_min: -8.525794386863708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.575185267643262\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01793531130303679\n",
      "          policy_loss: -0.05665993388862379\n",
      "          total_loss: 1.440798586792743\n",
      "          vf_explained_var: 0.9218804121658366\n",
      "          vf_loss: 1.4853521760753405\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.16923076923077\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09878772257270291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0862362226682048\n",
      "    mean_inference_ms: 0.9779353102162125\n",
      "    mean_raw_obs_processing_ms: 0.08210970333137617\n",
      "  time_since_restore: 832.6669015884399\n",
      "  time_this_iter_s: 9.668107032775879\n",
      "  time_total_s: 832.6669015884399\n",
      "  timers:\n",
      "    learn_throughput: 673.439\n",
      "    learn_time_ms: 5939.658\n",
      "    load_throughput: 13004585.691\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 462.435\n",
      "    sample_time_ms: 8649.858\n",
      "    update_time_ms: 1.607\n",
      "  timestamp: 1643384547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:31 (running for 00:14:03.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         832.667</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">0.066062</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -8.52579</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:36 (running for 00:14:08.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         832.667</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">0.066062</td><td style=\"text-align: right;\">             10.4743</td><td style=\"text-align: right;\">            -8.52579</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-42-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.6154933869838715\n",
      "  episode_reward_mean: 0.06355844490230084\n",
      "  episode_reward_min: -8.525794386863708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.497413795225082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018810923784833542\n",
      "          policy_loss: -0.06398690031010217\n",
      "          total_loss: 1.4210848561948746\n",
      "          vf_explained_var: 0.9228600111699874\n",
      "          vf_loss: 1.4723743833681588\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.17142857142857\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09880902555943237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08624908349058241\n",
      "    mean_inference_ms: 0.9782063742857241\n",
      "    mean_raw_obs_processing_ms: 0.08212319615182087\n",
      "  time_since_restore: 842.1509358882904\n",
      "  time_this_iter_s: 9.484034299850464\n",
      "  time_total_s: 842.1509358882904\n",
      "  timers:\n",
      "    learn_throughput: 664.745\n",
      "    learn_time_ms: 6017.342\n",
      "    load_throughput: 13759711.31\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 467.398\n",
      "    sample_time_ms: 8558.009\n",
      "    update_time_ms: 1.609\n",
      "  timestamp: 1643384557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:42 (running for 00:14:13.96)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         842.151</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">0.0635584</td><td style=\"text-align: right;\">             7.61549</td><td style=\"text-align: right;\">            -8.52579</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-42-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.6154933869838715\n",
      "  episode_reward_mean: 0.009492268934845925\n",
      "  episode_reward_min: -9.323058903217316\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.532817431931854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01710600439345751\n",
      "          policy_loss: -0.06156897048956604\n",
      "          total_loss: 2.6891032682286116\n",
      "          vf_explained_var: 0.8793662128269032\n",
      "          vf_loss: 2.739125709780442\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.933333333333334\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09884803681652848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0862909925335156\n",
      "    mean_inference_ms: 0.9786625097653625\n",
      "    mean_raw_obs_processing_ms: 0.08215230403432201\n",
      "  time_since_restore: 850.650631904602\n",
      "  time_this_iter_s: 8.499696016311646\n",
      "  time_total_s: 850.650631904602\n",
      "  timers:\n",
      "    learn_throughput: 665.233\n",
      "    learn_time_ms: 6012.931\n",
      "    load_throughput: 14112732.167\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 464.179\n",
      "    sample_time_ms: 8617.361\n",
      "    update_time_ms: 1.599\n",
      "  timestamp: 1643384565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:47 (running for 00:14:19.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         850.651</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">0.00949227</td><td style=\"text-align: right;\">             7.61549</td><td style=\"text-align: right;\">            -9.32306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:52 (running for 00:14:24.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         850.651</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">0.00949227</td><td style=\"text-align: right;\">             7.61549</td><td style=\"text-align: right;\">            -9.32306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-42-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.362266898155212\n",
      "  episode_reward_mean: -0.12487839818000794\n",
      "  episode_reward_min: -10.315696313977242\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.429720768877255\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017975132230147618\n",
      "          policy_loss: -0.08355439820687377\n",
      "          total_loss: 2.192575047614794\n",
      "          vf_explained_var: 0.9072421465509681\n",
      "          vf_loss: 2.263996218096825\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.007692307692306\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09891667124145137\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08635305770019494\n",
      "    mean_inference_ms: 0.9793889883663515\n",
      "    mean_raw_obs_processing_ms: 0.08220706317170819\n",
      "  time_since_restore: 859.52499127388\n",
      "  time_this_iter_s: 8.874359369277954\n",
      "  time_total_s: 859.52499127388\n",
      "  timers:\n",
      "    learn_throughput: 661.013\n",
      "    learn_time_ms: 6051.318\n",
      "    load_throughput: 14115106.848\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 463.83\n",
      "    sample_time_ms: 8623.853\n",
      "    update_time_ms: 1.557\n",
      "  timestamp: 1643384574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:42:58 (running for 00:14:30.38)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         859.525</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">-0.124878</td><td style=\"text-align: right;\">             7.36227</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:03 (running for 00:14:35.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         859.525</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">-0.124878</td><td style=\"text-align: right;\">             7.36227</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.337611258029938\n",
      "  episode_reward_mean: 0.08856311798095703\n",
      "  episode_reward_min: -10.315696313977242\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.442137065497778\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016964218128021943\n",
      "          policy_loss: -0.05719316475974616\n",
      "          total_loss: 1.9762755816882496\n",
      "          vf_explained_var: 0.9222096505344555\n",
      "          vf_loss: 2.022017905388468\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.521428571428572\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09901081346626964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08643532104524136\n",
      "    mean_inference_ms: 0.9803625400074445\n",
      "    mean_raw_obs_processing_ms: 0.08228133612813361\n",
      "  time_since_restore: 869.1461777687073\n",
      "  time_this_iter_s: 9.62118649482727\n",
      "  time_total_s: 869.1461777687073\n",
      "  timers:\n",
      "    learn_throughput: 652.564\n",
      "    learn_time_ms: 6129.664\n",
      "    load_throughput: 14024254.786\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 461.128\n",
      "    sample_time_ms: 8674.388\n",
      "    update_time_ms: 1.532\n",
      "  timestamp: 1643384584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:09 (running for 00:14:41.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         869.146</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">0.0885631</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.337611258029938\n",
      "  episode_reward_mean: -0.10122798144817352\n",
      "  episode_reward_min: -10.315696313977242\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.393941810566892\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018180633012846607\n",
      "          policy_loss: -0.08641556049546888\n",
      "          total_loss: 1.6774581813548881\n",
      "          vf_explained_var: 0.9125976670172906\n",
      "          vf_loss: 1.75160181450267\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.800000000000004\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09905224054199799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08648120760108523\n",
      "    mean_inference_ms: 0.9807206502443551\n",
      "    mean_raw_obs_processing_ms: 0.08231287299318871\n",
      "  time_since_restore: 878.470954656601\n",
      "  time_this_iter_s: 9.324776887893677\n",
      "  time_total_s: 878.470954656601\n",
      "  timers:\n",
      "    learn_throughput: 645.654\n",
      "    learn_time_ms: 6195.272\n",
      "    load_throughput: 13887274.232\n",
      "    load_time_ms: 0.288\n",
      "    sample_throughput: 456.027\n",
      "    sample_time_ms: 8771.404\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1643384593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:14 (running for 00:14:46.37)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         878.471</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">-0.101228</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:19 (running for 00:14:51.38)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         878.471</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">-0.101228</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-43-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.337611258029938\n",
      "  episode_reward_mean: -0.08017338685691357\n",
      "  episode_reward_min: -10.315696313977242\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.396976778071414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01729899212296821\n",
      "          policy_loss: -0.0951253030606876\n",
      "          total_loss: 2.3211351682021415\n",
      "          vf_explained_var: 0.8907158442081944\n",
      "          vf_loss: 2.404583649641724\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.3\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09908327460038294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08652072619302179\n",
      "    mean_inference_ms: 0.9809617015683554\n",
      "    mean_raw_obs_processing_ms: 0.08233709601965877\n",
      "  time_since_restore: 887.0354645252228\n",
      "  time_this_iter_s: 8.564509868621826\n",
      "  time_total_s: 887.0354645252228\n",
      "  timers:\n",
      "    learn_throughput: 639.613\n",
      "    learn_time_ms: 6253.781\n",
      "    load_throughput: 13863176.334\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 452.358\n",
      "    sample_time_ms: 8842.556\n",
      "    update_time_ms: 1.523\n",
      "  timestamp: 1643384602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:25 (running for 00:14:56.96)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         887.035</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">-0.0801734</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:30 (running for 00:15:01.97)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         887.035</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">-0.0801734</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.337611258029938\n",
      "  episode_reward_mean: -0.08984386622905731\n",
      "  episode_reward_min: -10.315696313977242\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.340770600431709\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019291097390599077\n",
      "          policy_loss: -0.08481787393490474\n",
      "          total_loss: 2.329619736523838\n",
      "          vf_explained_var: 0.8824743913066002\n",
      "          vf_loss: 2.4014161180584663\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.366666666666667\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09910896115904685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08654455605785025\n",
      "    mean_inference_ms: 0.9811639851204147\n",
      "    mean_raw_obs_processing_ms: 0.08236014600310321\n",
      "  time_since_restore: 895.4309248924255\n",
      "  time_this_iter_s: 8.395460367202759\n",
      "  time_total_s: 895.4309248924255\n",
      "  timers:\n",
      "    learn_throughput: 639.434\n",
      "    learn_time_ms: 6255.529\n",
      "    load_throughput: 13834597.18\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 448.329\n",
      "    sample_time_ms: 8922.022\n",
      "    update_time_ms: 1.531\n",
      "  timestamp: 1643384610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:35 (running for 00:15:07.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         895.431</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">-0.0898439</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -10.3157</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.337611258029938\n",
      "  episode_reward_mean: 0.14867464154958726\n",
      "  episode_reward_min: -9.356827273964882\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.285997704536684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017712693509165097\n",
      "          policy_loss: -0.0631912073220617\n",
      "          total_loss: 1.331700225896953\n",
      "          vf_explained_var: 0.9364297839903062\n",
      "          vf_loss: 1.3829353658742802\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.341666666666665\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09911058895121258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.086553976959684\n",
      "    mean_inference_ms: 0.9811536950350715\n",
      "    mean_raw_obs_processing_ms: 0.08236149464935615\n",
      "  time_since_restore: 903.4894561767578\n",
      "  time_this_iter_s: 8.058531284332275\n",
      "  time_total_s: 903.4894561767578\n",
      "  timers:\n",
      "    learn_throughput: 637.419\n",
      "    learn_time_ms: 6275.306\n",
      "    load_throughput: 13777790.917\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 447.873\n",
      "    sample_time_ms: 8931.096\n",
      "    update_time_ms: 1.534\n",
      "  timestamp: 1643384618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:40 (running for 00:15:12.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         903.489</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">0.148675</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -9.35683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:45 (running for 00:15:17.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         903.489</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">0.148675</td><td style=\"text-align: right;\">             9.33761</td><td style=\"text-align: right;\">            -9.35683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.818871319293976\n",
      "  episode_reward_mean: -0.0691682082414627\n",
      "  episode_reward_min: -7.554900527000427\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.303803680276358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01862047501808122\n",
      "          policy_loss: -0.07655246941803363\n",
      "          total_loss: 1.72790554964975\n",
      "          vf_explained_var: 0.9271149275123433\n",
      "          vf_loss: 1.7918891997427069\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.083333333333332\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0990979156705733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08655162321736419\n",
      "    mean_inference_ms: 0.981022349586431\n",
      "    mean_raw_obs_processing_ms: 0.08235302731215555\n",
      "  time_since_restore: 912.3774964809418\n",
      "  time_this_iter_s: 8.88804030418396\n",
      "  time_total_s: 912.3774964809418\n",
      "  timers:\n",
      "    learn_throughput: 637.788\n",
      "    learn_time_ms: 6271.674\n",
      "    load_throughput: 13665566.506\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 446.065\n",
      "    sample_time_ms: 8967.298\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1643384627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:51 (running for 00:15:23.38)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         912.377</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">-0.0691682</td><td style=\"text-align: right;\">             7.81887</td><td style=\"text-align: right;\">             -7.5549</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.818871319293976\n",
      "  episode_reward_mean: 0.02499546840786934\n",
      "  episode_reward_min: -7.554900527000427\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.160335935572142\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01879431970886324\n",
      "          policy_loss: -0.09953922725893477\n",
      "          total_loss: 1.424602840846825\n",
      "          vf_explained_var: 0.9227898161898377\n",
      "          vf_loss: 1.511455898772004\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.176923076923078\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09908362109989209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08654483237765515\n",
      "    mean_inference_ms: 0.9808504466672433\n",
      "    mean_raw_obs_processing_ms: 0.082339122353909\n",
      "  time_since_restore: 920.9879415035248\n",
      "  time_this_iter_s: 8.610445022583008\n",
      "  time_total_s: 920.9879415035248\n",
      "  timers:\n",
      "    learn_throughput: 640.847\n",
      "    learn_time_ms: 6241.735\n",
      "    load_throughput: 13843729.681\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 450.101\n",
      "    sample_time_ms: 8886.893\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1643384636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:43:57 (running for 00:15:29.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         920.988</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">0.0249955</td><td style=\"text-align: right;\">             7.81887</td><td style=\"text-align: right;\">             -7.5549</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:02 (running for 00:15:34.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         920.988</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">0.0249955</td><td style=\"text-align: right;\">             7.81887</td><td style=\"text-align: right;\">             -7.5549</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-44-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.818871319293976\n",
      "  episode_reward_mean: 0.027410298585891724\n",
      "  episode_reward_min: -7.175157129764557\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.134348369926535\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01829041442403579\n",
      "          policy_loss: -0.09700443804494396\n",
      "          total_loss: 1.2741789387010778\n",
      "          vf_explained_var: 0.9323687588014911\n",
      "          vf_loss: 1.3588373432877243\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.200000000000003\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09905663590720382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08652764593009653\n",
      "    mean_inference_ms: 0.9805661644737925\n",
      "    mean_raw_obs_processing_ms: 0.0823174456674283\n",
      "  time_since_restore: 930.5415573120117\n",
      "  time_this_iter_s: 9.553615808486938\n",
      "  time_total_s: 930.5415573120117\n",
      "  timers:\n",
      "    learn_throughput: 637.01\n",
      "    learn_time_ms: 6279.332\n",
      "    load_throughput: 13925312.085\n",
      "    load_time_ms: 0.287\n",
      "    sample_throughput: 453.197\n",
      "    sample_time_ms: 8826.189\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1643384645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:07 (running for 00:15:39.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         930.542</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">0.0274103</td><td style=\"text-align: right;\">             7.81887</td><td style=\"text-align: right;\">            -7.17516</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:12 (running for 00:15:44.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         930.542</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">0.0274103</td><td style=\"text-align: right;\">             7.81887</td><td style=\"text-align: right;\">            -7.17516</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.427403539419174\n",
      "  episode_reward_mean: 0.07542749166488648\n",
      "  episode_reward_min: -6.804253101348877\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.155345813176965\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018534876485993983\n",
      "          policy_loss: -0.07763491988742864\n",
      "          total_loss: 2.270327520230785\n",
      "          vf_explained_var: 0.9067302985216982\n",
      "          vf_loss: 2.3354513952328313\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.766666666666662\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09903483041092968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08651021537810444\n",
      "    mean_inference_ms: 0.9802960918440293\n",
      "    mean_raw_obs_processing_ms: 0.08229380237252817\n",
      "  time_since_restore: 938.9407534599304\n",
      "  time_this_iter_s: 8.399196147918701\n",
      "  time_total_s: 938.9407534599304\n",
      "  timers:\n",
      "    learn_throughput: 637.664\n",
      "    learn_time_ms: 6272.892\n",
      "    load_throughput: 14020738.76\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 451.499\n",
      "    sample_time_ms: 8859.373\n",
      "    update_time_ms: 1.381\n",
      "  timestamp: 1643384654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:18 (running for 00:15:50.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         938.941</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">0.0754275</td><td style=\"text-align: right;\">              8.4274</td><td style=\"text-align: right;\">            -6.80425</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-44-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.60255292057991\n",
      "  episode_reward_mean: -0.026934076845645905\n",
      "  episode_reward_min: -6.804253101348877\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.011237653609245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019247185301520447\n",
      "          policy_loss: -0.07665100768759286\n",
      "          total_loss: 2.5160439847788263\n",
      "          vf_explained_var: 0.894406448641131\n",
      "          vf_loss: 2.5797031279693368\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.89090909090909\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899903916729869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08648073848274063\n",
      "    mean_inference_ms: 0.9798930906413027\n",
      "    mean_raw_obs_processing_ms: 0.08225960583337395\n",
      "  time_since_restore: 946.595253944397\n",
      "  time_this_iter_s: 7.654500484466553\n",
      "  time_total_s: 946.595253944397\n",
      "  timers:\n",
      "    learn_throughput: 645.228\n",
      "    learn_time_ms: 6199.362\n",
      "    load_throughput: 14119858.61\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 454.32\n",
      "    sample_time_ms: 8804.374\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1643384661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:24 (running for 00:15:55.69)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         946.595</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">-0.0269341</td><td style=\"text-align: right;\">             8.60255</td><td style=\"text-align: right;\">            -6.80425</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:29 (running for 00:16:00.69)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         946.595</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">-0.0269341</td><td style=\"text-align: right;\">             8.60255</td><td style=\"text-align: right;\">            -6.80425</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-44-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.60255292057991\n",
      "  episode_reward_mean: 0.02622387558221817\n",
      "  episode_reward_min: -6.804253101348877\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 7.005230229388001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018777214785098053\n",
      "          policy_loss: -0.07953435179767429\n",
      "          total_loss: 1.4940555567577762\n",
      "          vf_explained_var: 0.9335145643962327\n",
      "          vf_loss: 1.5609152925911771\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.999999999999996\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09895228560563235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08644435079581961\n",
      "    mean_inference_ms: 0.9794077741109821\n",
      "    mean_raw_obs_processing_ms: 0.08222221727202862\n",
      "  time_since_restore: 955.3063428401947\n",
      "  time_this_iter_s: 8.71108889579773\n",
      "  time_total_s: 955.3063428401947\n",
      "  timers:\n",
      "    learn_throughput: 651.761\n",
      "    learn_time_ms: 6137.218\n",
      "    load_throughput: 14147243.444\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 459.671\n",
      "    sample_time_ms: 8701.877\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1643384670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:34 (running for 00:16:06.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         955.306</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">0.0262239</td><td style=\"text-align: right;\">             8.60255</td><td style=\"text-align: right;\">            -6.80425</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-44-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.53099149465561\n",
      "  episode_reward_mean: 0.03348866418004036\n",
      "  episode_reward_min: -7.337906986474991\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.966108936391851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019218606719799494\n",
      "          policy_loss: -0.08072693991995547\n",
      "          total_loss: 2.105970067952469\n",
      "          vf_explained_var: 0.9180151742632671\n",
      "          vf_loss: 2.1737244504594035\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.61666666666667\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09889718884588855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08640203949736876\n",
      "    mean_inference_ms: 0.9788622717353417\n",
      "    mean_raw_obs_processing_ms: 0.08218283395423034\n",
      "  time_since_restore: 964.0241606235504\n",
      "  time_this_iter_s: 8.717817783355713\n",
      "  time_total_s: 964.0241606235504\n",
      "  timers:\n",
      "    learn_throughput: 656.866\n",
      "    learn_time_ms: 6089.524\n",
      "    load_throughput: 14103241.426\n",
      "    load_time_ms: 0.284\n",
      "    sample_throughput: 463.661\n",
      "    sample_time_ms: 8626.984\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1643384679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:40 (running for 00:16:12.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         964.024</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">0.0334887</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -7.33791</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:45 (running for 00:16:17.17)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         964.024</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">0.0334887</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -7.33791</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-44-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.53099149465561\n",
      "  episode_reward_mean: 0.057481853067874907\n",
      "  episode_reward_min: -7.337906986474991\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.999293973881711\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018499289368754054\n",
      "          policy_loss: -0.07800248422189265\n",
      "          total_loss: 1.0080112987533412\n",
      "          vf_explained_var: 0.9474629578410938\n",
      "          vf_loss: 1.0735267652779497\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.908333333333335\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09884777816487464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0863636140250933\n",
      "    mean_inference_ms: 0.97836129259744\n",
      "    mean_raw_obs_processing_ms: 0.08214637576994319\n",
      "  time_since_restore: 972.2375257015228\n",
      "  time_this_iter_s: 8.213365077972412\n",
      "  time_total_s: 972.2375257015228\n",
      "  timers:\n",
      "    learn_throughput: 659.509\n",
      "    learn_time_ms: 6065.117\n",
      "    load_throughput: 14032465.708\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 466.848\n",
      "    sample_time_ms: 8568.103\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1643384687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:50 (running for 00:16:22.40)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         972.238</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">0.0574819</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -7.33791</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:44:55 (running for 00:16:27.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         972.238</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">0.0574819</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -7.33791</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.53099149465561\n",
      "  episode_reward_mean: 0.03352672815322876\n",
      "  episode_reward_min: -7.337906986474991\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.876414692786432\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019160795710500576\n",
      "          policy_loss: -0.05223084882863106\n",
      "          total_loss: 1.5370462702376948\n",
      "          vf_explained_var: 0.9290348161292332\n",
      "          vf_loss: 1.5763435833877133\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.576923076923077\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09878413420208909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08631558867742088\n",
      "    mean_inference_ms: 0.9777254818953254\n",
      "    mean_raw_obs_processing_ms: 0.08209946162023046\n",
      "  time_since_restore: 981.1399435997009\n",
      "  time_this_iter_s: 8.9024178981781\n",
      "  time_total_s: 981.1399435997009\n",
      "  timers:\n",
      "    learn_throughput: 652.023\n",
      "    learn_time_ms: 6134.758\n",
      "    load_throughput: 14110358.284\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 469.238\n",
      "    sample_time_ms: 8524.454\n",
      "    update_time_ms: 1.336\n",
      "  timestamp: 1643384696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:01 (running for 00:16:33.33)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">          981.14</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">0.0335267</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -7.33791</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-45-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.53099149465561\n",
      "  episode_reward_mean: -0.04038522481918335\n",
      "  episode_reward_min: -10.388380616903305\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.860939049977128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018882297443258297\n",
      "          policy_loss: -0.09199905131733226\n",
      "          total_loss: 2.4783800582731925\n",
      "          vf_explained_var: 0.8908962825934092\n",
      "          vf_loss: 2.5576335602069413\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.184615384615384\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09877518177341997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08631591305232725\n",
      "    mean_inference_ms: 0.9776353091748594\n",
      "    mean_raw_obs_processing_ms: 0.08210162911550839\n",
      "  time_since_restore: 990.1142401695251\n",
      "  time_this_iter_s: 8.974296569824219\n",
      "  time_total_s: 990.1142401695251\n",
      "  timers:\n",
      "    learn_throughput: 648.586\n",
      "    learn_time_ms: 6167.259\n",
      "    load_throughput: 13888423.841\n",
      "    load_time_ms: 0.288\n",
      "    sample_throughput: 462.216\n",
      "    sample_time_ms: 8653.959\n",
      "    update_time_ms: 1.312\n",
      "  timestamp: 1643384705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:07 (running for 00:16:39.33)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         990.114</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">-0.0403852</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -10.3884</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:12 (running for 00:16:44.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         990.114</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">-0.0403852</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -10.3884</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-45-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.53099149465561\n",
      "  episode_reward_mean: -0.01446007877588272\n",
      "  episode_reward_min: -10.388380616903305\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.745440261081983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018563972015381118\n",
      "          policy_loss: -0.07549912601869593\n",
      "          total_loss: 1.154233759971616\n",
      "          vf_explained_var: 0.9334818675312945\n",
      "          vf_loss: 1.217202201998362\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.8\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09877596029096353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08632607794315898\n",
      "    mean_inference_ms: 0.9776287965451502\n",
      "    mean_raw_obs_processing_ms: 0.08210508390700523\n",
      "  time_since_restore: 998.3789393901825\n",
      "  time_this_iter_s: 8.264699220657349\n",
      "  time_total_s: 998.3789393901825\n",
      "  timers:\n",
      "    learn_throughput: 655.15\n",
      "    learn_time_ms: 6105.469\n",
      "    load_throughput: 14033639.481\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 460.54\n",
      "    sample_time_ms: 8685.455\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1643384713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:17 (running for 00:16:49.62)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         998.379</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">-0.0144601</td><td style=\"text-align: right;\">             9.53099</td><td style=\"text-align: right;\">            -10.3884</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-45-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.179580122232437\n",
      "  episode_reward_mean: -0.03218446746468544\n",
      "  episode_reward_min: -10.388380616903305\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.673010282106297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018654643258885207\n",
      "          policy_loss: -0.09750364763131465\n",
      "          total_loss: 2.3197156663363176\n",
      "          vf_explained_var: 0.9030182171893376\n",
      "          vf_loss: 2.4046274296779147\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.991666666666664\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0987889360418731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08634014404398183\n",
      "    mean_inference_ms: 0.9776910088901946\n",
      "    mean_raw_obs_processing_ms: 0.08211139151011138\n",
      "  time_since_restore: 1006.8300380706787\n",
      "  time_this_iter_s: 8.451098680496216\n",
      "  time_total_s: 1006.8300380706787\n",
      "  timers:\n",
      "    learn_throughput: 657.034\n",
      "    learn_time_ms: 6087.962\n",
      "    load_throughput: 13713598.169\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 463.754\n",
      "    sample_time_ms: 8625.27\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1643384722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:23 (running for 00:16:55.09)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1006.83</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">-0.0321845</td><td style=\"text-align: right;\">             8.17958</td><td style=\"text-align: right;\">            -10.3884</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:28 (running for 00:17:00.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1006.83</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">-0.0321845</td><td style=\"text-align: right;\">             8.17958</td><td style=\"text-align: right;\">            -10.3884</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-45-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.179580122232437\n",
      "  episode_reward_mean: -0.08860349252820016\n",
      "  episode_reward_min: -10.388380616903305\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.735590452788978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017627185732913702\n",
      "          policy_loss: -0.10537905881722127\n",
      "          total_loss: 1.4178790546297746\n",
      "          vf_explained_var: 0.9312780994240956\n",
      "          vf_loss: 1.511359759219872\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.400000000000002\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09881993225815702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08637167175370684\n",
      "    mean_inference_ms: 0.9779554908980859\n",
      "    mean_raw_obs_processing_ms: 0.0821337678058555\n",
      "  time_since_restore: 1015.1625730991364\n",
      "  time_this_iter_s: 8.332535028457642\n",
      "  time_total_s: 1015.1625730991364\n",
      "  timers:\n",
      "    learn_throughput: 674.521\n",
      "    learn_time_ms: 5930.132\n",
      "    load_throughput: 13722571.569\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 462.763\n",
      "    sample_time_ms: 8643.736\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1643384730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:33 (running for 00:17:05.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1015.16</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">-0.0886035</td><td style=\"text-align: right;\">             8.17958</td><td style=\"text-align: right;\">            -10.3884</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:38 (running for 00:17:10.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1015.16</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">-0.0886035</td><td style=\"text-align: right;\">             8.17958</td><td style=\"text-align: right;\">            -10.3884</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-45-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.240166425704956\n",
      "  episode_reward_mean: -0.08454981535673141\n",
      "  episode_reward_min: -11.002935275435448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.629732971293952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01979401947924081\n",
      "          policy_loss: -0.08981263266691077\n",
      "          total_loss: 3.308470401501343\n",
      "          vf_explained_var: 0.86189064460416\n",
      "          vf_loss: 3.3849220530160014\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.558333333333334\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0988558663231893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08640448234360036\n",
      "    mean_inference_ms: 0.9782503117183796\n",
      "    mean_raw_obs_processing_ms: 0.08216324815816081\n",
      "  time_since_restore: 1023.5875172615051\n",
      "  time_this_iter_s: 8.424944162368774\n",
      "  time_total_s: 1023.5875172615051\n",
      "  timers:\n",
      "    learn_throughput: 672.59\n",
      "    learn_time_ms: 5947.159\n",
      "    load_throughput: 13667793.075\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 472.192\n",
      "    sample_time_ms: 8471.133\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643384739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:44 (running for 00:17:15.89)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1023.59</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">-0.0845498</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.240166425704956\n",
      "  episode_reward_mean: 0.05554038792848587\n",
      "  episode_reward_min: -11.002935275435448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.6416751041207265\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019689645903543385\n",
      "          policy_loss: -0.07273684709423012\n",
      "          total_loss: 2.093551645367356\n",
      "          vf_explained_var: 0.9185775106312126\n",
      "          vf_loss: 2.152997979698002\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.707692307692305\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09891437384153874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08644762906950007\n",
      "    mean_inference_ms: 0.9787413394540507\n",
      "    mean_raw_obs_processing_ms: 0.08220779386581051\n",
      "  time_since_restore: 1032.6970579624176\n",
      "  time_this_iter_s: 9.109540700912476\n",
      "  time_total_s: 1032.6970579624176\n",
      "  timers:\n",
      "    learn_throughput: 669.016\n",
      "    learn_time_ms: 5978.935\n",
      "    load_throughput: 13520199.855\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 465.024\n",
      "    sample_time_ms: 8601.708\n",
      "    update_time_ms: 1.455\n",
      "  timestamp: 1643384748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:49 (running for 00:17:21.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">          1032.7</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">0.0555404</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:54 (running for 00:17:26.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">          1032.7</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">0.0555404</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-45-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.240166425704956\n",
      "  episode_reward_mean: 0.011515514254570007\n",
      "  episode_reward_min: -11.002935275435448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.552245632807414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018724815786892616\n",
      "          policy_loss: -0.07340341315739939\n",
      "          total_loss: 2.531511074194925\n",
      "          vf_explained_var: 0.8862919931129742\n",
      "          vf_loss: 2.592275228487548\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.53846153846154\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899685356989316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0865036794867785\n",
      "    mean_inference_ms: 0.9794304707781838\n",
      "    mean_raw_obs_processing_ms: 0.08227029950986292\n",
      "  time_since_restore: 1041.2146327495575\n",
      "  time_this_iter_s: 8.517574787139893\n",
      "  time_total_s: 1041.2146327495575\n",
      "  timers:\n",
      "    learn_throughput: 675.99\n",
      "    learn_time_ms: 5917.244\n",
      "    load_throughput: 13476757.973\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 461.021\n",
      "    sample_time_ms: 8676.398\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1643384756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:45:59 (running for 00:17:31.57)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1041.21</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">0.0115155</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:04 (running for 00:17:36.58)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1041.21</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">0.0115155</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-46-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.240166425704956\n",
      "  episode_reward_mean: 0.12817358672618867\n",
      "  episode_reward_min: -11.002935275435448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.490152332859655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019709291957649054\n",
      "          policy_loss: -0.06394512527872638\n",
      "          total_loss: 2.4487111392518583\n",
      "          vf_explained_var: 0.9029311213442074\n",
      "          vf_loss: 2.499352496662127\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.375\n",
      "    ram_util_percent: 34.31666666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09907208395320044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08655690020412266\n",
      "    mean_inference_ms: 0.9800807372992719\n",
      "    mean_raw_obs_processing_ms: 0.08232962118741102\n",
      "  time_since_restore: 1049.8343958854675\n",
      "  time_this_iter_s: 8.619763135910034\n",
      "  time_total_s: 1049.8343958854675\n",
      "  timers:\n",
      "    learn_throughput: 677.727\n",
      "    learn_time_ms: 5902.077\n",
      "    load_throughput: 13687864.894\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 464.041\n",
      "    sample_time_ms: 8619.928\n",
      "    update_time_ms: 1.464\n",
      "  timestamp: 1643384765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:10 (running for 00:17:42.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1049.83</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">0.128174</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-46-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.240166425704956\n",
      "  episode_reward_mean: 0.08655726477503776\n",
      "  episode_reward_min: -11.002935275435448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.56327311915736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019995491273851165\n",
      "          policy_loss: -0.07227657824524387\n",
      "          total_loss: 0.9679895008568682\n",
      "          vf_explained_var: 0.9408841599059361\n",
      "          vf_loss: 1.0267691229139604\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.375\n",
      "    ram_util_percent: 34.36666666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0991345837399348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08659725067702004\n",
      "    mean_inference_ms: 0.9805930385017447\n",
      "    mean_raw_obs_processing_ms: 0.08237866116764395\n",
      "  time_since_restore: 1057.9782588481903\n",
      "  time_this_iter_s: 8.143862962722778\n",
      "  time_total_s: 1057.9782588481903\n",
      "  timers:\n",
      "    learn_throughput: 679.586\n",
      "    learn_time_ms: 5885.933\n",
      "    load_throughput: 13794783.753\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 464.357\n",
      "    sample_time_ms: 8614.069\n",
      "    update_time_ms: 1.507\n",
      "  timestamp: 1643384773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:15 (running for 00:17:47.38)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1057.98</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">0.0865573</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:20 (running for 00:17:52.38)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1057.98</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">0.0865573</td><td style=\"text-align: right;\">             9.24017</td><td style=\"text-align: right;\">            -11.0029</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-46-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.861924409866333\n",
      "  episode_reward_mean: -0.05722802743315697\n",
      "  episode_reward_min: -8.344149112701416\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.567083671528806\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019728065891534745\n",
      "          policy_loss: -0.09946531662817604\n",
      "          total_loss: 1.6492295735881173\n",
      "          vf_explained_var: 0.9296628728989632\n",
      "          vf_loss: 1.7353784526708305\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.5\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09922444208510622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08665556217602534\n",
      "    mean_inference_ms: 0.9813269252509811\n",
      "    mean_raw_obs_processing_ms: 0.08244196946081711\n",
      "  time_since_restore: 1066.6792268753052\n",
      "  time_this_iter_s: 8.700968027114868\n",
      "  time_total_s: 1066.6792268753052\n",
      "  timers:\n",
      "    learn_throughput: 686.908\n",
      "    learn_time_ms: 5823.196\n",
      "    load_throughput: 13463779.793\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 462.937\n",
      "    sample_time_ms: 8640.477\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1643384782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:26 (running for 00:17:58.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1066.68</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">-0.057228</td><td style=\"text-align: right;\">             7.86192</td><td style=\"text-align: right;\">            -8.34415</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-46-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.980486303567886\n",
      "  episode_reward_mean: -0.03111341059207916\n",
      "  episode_reward_min: -8.434592321515083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.514264417976461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01880681184995984\n",
      "          policy_loss: -0.10409707744575797\n",
      "          total_loss: 2.1461163982643834\n",
      "          vf_explained_var: 0.9119787891705831\n",
      "          vf_loss: 2.2375188843216947\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.025000000000002\n",
      "    ram_util_percent: 34.39166666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09926497644632618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08668332206206265\n",
      "    mean_inference_ms: 0.9816044795223223\n",
      "    mean_raw_obs_processing_ms: 0.08246405881521982\n",
      "  time_since_restore: 1075.0957503318787\n",
      "  time_this_iter_s: 8.416523456573486\n",
      "  time_total_s: 1075.0957503318787\n",
      "  timers:\n",
      "    learn_throughput: 689.11\n",
      "    learn_time_ms: 5804.592\n",
      "    load_throughput: 13582590.674\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 468.418\n",
      "    sample_time_ms: 8539.385\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1643384790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:31 (running for 00:18:03.54)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">          1075.1</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">-0.0311134</td><td style=\"text-align: right;\">             8.98049</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:36 (running for 00:18:08.54)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">          1075.1</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">-0.0311134</td><td style=\"text-align: right;\">             8.98049</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-46-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.106279820203781\n",
      "  episode_reward_mean: -0.03642188489437103\n",
      "  episode_reward_min: -8.434592321515083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.6750000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.420450177243961\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020738974639056346\n",
      "          policy_loss: -0.07301342273581653\n",
      "          total_loss: 2.496831057328851\n",
      "          vf_explained_var: 0.9044193362036059\n",
      "          vf_loss: 2.555845647088943\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.575\n",
      "    ram_util_percent: 34.39166666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09929391958635868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08670387664534011\n",
      "    mean_inference_ms: 0.981772674567213\n",
      "    mean_raw_obs_processing_ms: 0.08247831440444538\n",
      "  time_since_restore: 1083.4980764389038\n",
      "  time_this_iter_s: 8.402326107025146\n",
      "  time_total_s: 1083.4980764389038\n",
      "  timers:\n",
      "    learn_throughput: 689.522\n",
      "    learn_time_ms: 5801.12\n",
      "    load_throughput: 13325826.847\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 468.477\n",
      "    sample_time_ms: 8538.314\n",
      "    update_time_ms: 1.425\n",
      "  timestamp: 1643384799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:42 (running for 00:18:13.96)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">          1083.5</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">-0.0364219</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:47 (running for 00:18:18.97)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">          1083.5</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">-0.0364219</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.106279820203781\n",
      "  episode_reward_mean: -0.06950927972793579\n",
      "  episode_reward_min: -8.434592321515083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.39006292640522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015246034280559515\n",
      "          policy_loss: -0.06277566752847163\n",
      "          total_loss: 1.7819036681345233\n",
      "          vf_explained_var: 0.9135773233188096\n",
      "          vf_loss: 1.8292427258866448\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.083333333333332\n",
      "    ram_util_percent: 34.383333333333326\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0993181111010524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08672335011078754\n",
      "    mean_inference_ms: 0.981912318743666\n",
      "    mean_raw_obs_processing_ms: 0.08249233629772476\n",
      "  time_since_restore: 1091.8560285568237\n",
      "  time_this_iter_s: 8.357952117919922\n",
      "  time_total_s: 1091.8560285568237\n",
      "  timers:\n",
      "    learn_throughput: 689.446\n",
      "    learn_time_ms: 5801.761\n",
      "    load_throughput: 13417479.207\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 469.236\n",
      "    sample_time_ms: 8524.502\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1643384807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:52 (running for 00:18:24.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         1091.86</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">-0.0695093</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.106279820203781\n",
      "  episode_reward_mean: -0.025441020727157593\n",
      "  episode_reward_min: -8.434592321515083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.392976042532152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016772378266364497\n",
      "          policy_loss: -0.08734622375378685\n",
      "          total_loss: 1.1752348843651512\n",
      "          vf_explained_var: 0.9347454058226718\n",
      "          vf_loss: 1.2455990680843911\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.166666666666668\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09934653678904372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0867506931974916\n",
      "    mean_inference_ms: 0.9821088843330739\n",
      "    mean_raw_obs_processing_ms: 0.08251361136685077\n",
      "  time_since_restore: 1099.9967048168182\n",
      "  time_this_iter_s: 8.140676259994507\n",
      "  time_total_s: 1099.9967048168182\n",
      "  timers:\n",
      "    learn_throughput: 690.437\n",
      "    learn_time_ms: 5793.436\n",
      "    load_throughput: 13412116.076\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 469.785\n",
      "    sample_time_ms: 8514.542\n",
      "    update_time_ms: 1.46\n",
      "  timestamp: 1643384815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:46:57 (running for 00:18:29.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">            1100</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">-0.025441</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:02 (running for 00:18:34.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">            1100</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">-0.025441</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-47-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.106279820203781\n",
      "  episode_reward_mean: 0.09881302639842034\n",
      "  episode_reward_min: -8.434592321515083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.377346868412469\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015508915531300498\n",
      "          policy_loss: -0.08051337419698636\n",
      "          total_loss: 2.039296702045186\n",
      "          vf_explained_var: 0.9197817994702246\n",
      "          vf_loss: 2.104107307298209\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.166666666666664\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09935866459013748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08676871282403767\n",
      "    mean_inference_ms: 0.9821925937543505\n",
      "    mean_raw_obs_processing_ms: 0.08252722783976635\n",
      "  time_since_restore: 1108.6785140037537\n",
      "  time_this_iter_s: 8.681809186935425\n",
      "  time_total_s: 1108.6785140037537\n",
      "  timers:\n",
      "    learn_throughput: 689.481\n",
      "    learn_time_ms: 5801.467\n",
      "    load_throughput: 13269964.407\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 469.274\n",
      "    sample_time_ms: 8523.801\n",
      "    update_time_ms: 1.455\n",
      "  timestamp: 1643384824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:08 (running for 00:18:40.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1108.68</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">0.098813</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">            -8.43459</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-47-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.106279820203781\n",
      "  episode_reward_mean: 0.03452660143375397\n",
      "  episode_reward_min: -8.110801339149475\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.27634672657136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015876061399561603\n",
      "          policy_loss: -0.07806951057526373\n",
      "          total_loss: 1.272419406317665\n",
      "          vf_explained_var: 0.9279988356815871\n",
      "          vf_loss: 1.3344144037814551\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.025\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09937489448248202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08678062023915757\n",
      "    mean_inference_ms: 0.9822767443952752\n",
      "    mean_raw_obs_processing_ms: 0.08254137935146431\n",
      "  time_since_restore: 1116.9816718101501\n",
      "  time_this_iter_s: 8.303157806396484\n",
      "  time_total_s: 1116.9816718101501\n",
      "  timers:\n",
      "    learn_throughput: 690.641\n",
      "    learn_time_ms: 5791.72\n",
      "    load_throughput: 13336419.714\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 472.753\n",
      "    sample_time_ms: 8461.075\n",
      "    update_time_ms: 1.41\n",
      "  timestamp: 1643384832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:13 (running for 00:18:45.53)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1116.98</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">0.0345266</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">             -8.1108</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:18 (running for 00:18:50.54)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1116.98</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">0.0345266</td><td style=\"text-align: right;\">             11.1063</td><td style=\"text-align: right;\">             -8.1108</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-47-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.108543008565903\n",
      "  episode_reward_mean: 0.004086019992828369\n",
      "  episode_reward_min: -8.110801339149475\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.220453727886241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015291131399918135\n",
      "          policy_loss: -0.09102847569682185\n",
      "          total_loss: 1.2874848680235245\n",
      "          vf_explained_var: 0.9354983038799737\n",
      "          vf_loss: 1.3630310716167573\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.750000000000004\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0993885131217835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08679222978550812\n",
      "    mean_inference_ms: 0.9823476231038798\n",
      "    mean_raw_obs_processing_ms: 0.08255273451244804\n",
      "  time_since_restore: 1125.253127336502\n",
      "  time_this_iter_s: 8.271455526351929\n",
      "  time_total_s: 1125.253127336502\n",
      "  timers:\n",
      "    learn_throughput: 691.439\n",
      "    learn_time_ms: 5785.036\n",
      "    load_throughput: 13478923.435\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 474.345\n",
      "    sample_time_ms: 8432.681\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1643384841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:24 (running for 00:18:55.83)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1125.25</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">0.00408602</td><td style=\"text-align: right;\">             7.10854</td><td style=\"text-align: right;\">             -8.1108</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:29 (running for 00:19:00.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1125.25</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">0.00408602</td><td style=\"text-align: right;\">             7.10854</td><td style=\"text-align: right;\">             -8.1108</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-47-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.324531681835651\n",
      "  episode_reward_mean: 0.03223154962062836\n",
      "  episode_reward_min: -8.110801339149475\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.2135340388103195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015240337154080533\n",
      "          policy_loss: -0.07278081605220915\n",
      "          total_loss: 2.2725439254439848\n",
      "          vf_explained_var: 0.9099036553854584\n",
      "          vf_loss: 2.3298939093226387\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.825\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09940223215648826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08680558492199136\n",
      "    mean_inference_ms: 0.9824500227611288\n",
      "    mean_raw_obs_processing_ms: 0.08256544793931336\n",
      "  time_since_restore: 1133.7769322395325\n",
      "  time_this_iter_s: 8.523804903030396\n",
      "  time_total_s: 1133.7769322395325\n",
      "  timers:\n",
      "    learn_throughput: 692.669\n",
      "    learn_time_ms: 5774.766\n",
      "    load_throughput: 13437898.278\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 474.68\n",
      "    sample_time_ms: 8426.731\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1643384849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:34 (running for 00:19:06.38)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         1133.78</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">0.0322315</td><td style=\"text-align: right;\">             8.32453</td><td style=\"text-align: right;\">             -8.1108</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-47-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.324531681835651\n",
      "  episode_reward_mean: 0.10271442979574204\n",
      "  episode_reward_min: -8.370570361614227\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.197165896815639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014880408878846696\n",
      "          policy_loss: -0.06689697878575453\n",
      "          total_loss: 2.6021254437034247\n",
      "          vf_explained_var: 0.9141454694732543\n",
      "          vf_loss: 2.653955989739587\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.675\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09941988935409878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08681695152869139\n",
      "    mean_inference_ms: 0.9825805343589092\n",
      "    mean_raw_obs_processing_ms: 0.08257769019651268\n",
      "  time_since_restore: 1142.167021036148\n",
      "  time_this_iter_s: 8.3900887966156\n",
      "  time_total_s: 1142.167021036148\n",
      "  timers:\n",
      "    learn_throughput: 691.427\n",
      "    learn_time_ms: 5785.138\n",
      "    load_throughput: 13256333.755\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 474.469\n",
      "    sample_time_ms: 8430.469\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1643384858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:40 (running for 00:19:11.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1142.17</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">0.102714</td><td style=\"text-align: right;\">             8.32453</td><td style=\"text-align: right;\">            -8.37057</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:45 (running for 00:19:16.79)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1142.17</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">0.102714</td><td style=\"text-align: right;\">             8.32453</td><td style=\"text-align: right;\">            -8.37057</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-47-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.324531681835651\n",
      "  episode_reward_mean: -0.035646930634975434\n",
      "  episode_reward_min: -13.232992827892303\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.1594571518641645\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015368369420970748\n",
      "          policy_loss: -0.0761948012316259\n",
      "          total_loss: 1.4273520274585993\n",
      "          vf_explained_var: 0.9123546334364081\n",
      "          vf_loss: 1.4879863545298577\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.90769230769231\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09944705693756287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08683506370030329\n",
      "    mean_inference_ms: 0.9828099323084308\n",
      "    mean_raw_obs_processing_ms: 0.08259541863953217\n",
      "  time_since_restore: 1150.8979647159576\n",
      "  time_this_iter_s: 8.73094367980957\n",
      "  time_total_s: 1150.8979647159576\n",
      "  timers:\n",
      "    learn_throughput: 690.832\n",
      "    learn_time_ms: 5790.12\n",
      "    load_throughput: 13300472.491\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 474.021\n",
      "    sample_time_ms: 8438.438\n",
      "    update_time_ms: 1.446\n",
      "  timestamp: 1643384866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:50 (running for 00:19:22.53)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">          1150.9</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">-0.0356469</td><td style=\"text-align: right;\">             8.32453</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.324531681835651\n",
      "  episode_reward_mean: -0.03057931050658226\n",
      "  episode_reward_min: -13.232992827892303\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.100483106285013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016736507015320472\n",
      "          policy_loss: -0.08326951571369684\n",
      "          total_loss: 1.7109186324846721\n",
      "          vf_explained_var: 0.9118920217278184\n",
      "          vf_loss: 1.7772424364362354\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.150000000000002\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09947186990046637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08685471437336044\n",
      "    mean_inference_ms: 0.9830573896376058\n",
      "    mean_raw_obs_processing_ms: 0.08261402521842266\n",
      "  time_since_restore: 1159.352692604065\n",
      "  time_this_iter_s: 8.4547278881073\n",
      "  time_total_s: 1159.352692604065\n",
      "  timers:\n",
      "    learn_throughput: 690.825\n",
      "    learn_time_ms: 5790.175\n",
      "    load_throughput: 13441128.024\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 473.525\n",
      "    sample_time_ms: 8447.286\n",
      "    update_time_ms: 1.462\n",
      "  timestamp: 1643384875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:47:56 (running for 00:19:28.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1159.35</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">-0.0305793</td><td style=\"text-align: right;\">             8.32453</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:01 (running for 00:19:33.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1159.35</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">-0.0305793</td><td style=\"text-align: right;\">             8.32453</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.52856719493866\n",
      "  episode_reward_mean: 0.028079008758068083\n",
      "  episode_reward_min: -13.232992827892303\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.130295936010217\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015760522863783703\n",
      "          policy_loss: -0.07469157642404478\n",
      "          total_loss: 2.2335420075493553\n",
      "          vf_explained_var: 0.904814177431086\n",
      "          vf_loss: 2.292276049749826\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.38333333333333\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09950410520883278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08687701705895413\n",
      "    mean_inference_ms: 0.9833715601858758\n",
      "    mean_raw_obs_processing_ms: 0.08263766912428638\n",
      "  time_since_restore: 1168.0676538944244\n",
      "  time_this_iter_s: 8.714961290359497\n",
      "  time_total_s: 1168.0676538944244\n",
      "  timers:\n",
      "    learn_throughput: 687.97\n",
      "    learn_time_ms: 5814.21\n",
      "    load_throughput: 13509313.149\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 473.11\n",
      "    sample_time_ms: 8454.69\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1643384884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:07 (running for 00:19:38.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         1168.07</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">0.028079</td><td style=\"text-align: right;\">             9.52857</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:12 (running for 00:19:43.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         1168.07</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">0.028079</td><td style=\"text-align: right;\">             9.52857</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.962835848331451\n",
      "  episode_reward_mean: -0.01146398201584816\n",
      "  episode_reward_min: -13.232992827892303\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.108221031517111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015507012924714406\n",
      "          policy_loss: -0.08333668076703625\n",
      "          total_loss: 4.091701062687332\n",
      "          vf_explained_var: 0.8641512218982943\n",
      "          vf_loss: 4.159336906599422\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.283333333333335\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09953225029822431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08689487042070276\n",
      "    mean_inference_ms: 0.9836105543573853\n",
      "    mean_raw_obs_processing_ms: 0.08265707303638288\n",
      "  time_since_restore: 1176.4355237483978\n",
      "  time_this_iter_s: 8.367869853973389\n",
      "  time_total_s: 1176.4355237483978\n",
      "  timers:\n",
      "    learn_throughput: 687.095\n",
      "    learn_time_ms: 5821.614\n",
      "    load_throughput: 13782318.245\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 472.12\n",
      "    sample_time_ms: 8472.428\n",
      "    update_time_ms: 1.42\n",
      "  timestamp: 1643384892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:17 (running for 00:19:49.14)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         1176.44</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">-0.011464</td><td style=\"text-align: right;\">             9.96284</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.458552837371826\n",
      "  episode_reward_mean: -0.08706230461597443\n",
      "  episode_reward_min: -13.232992827892303\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.082923048286028\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015323238337355941\n",
      "          policy_loss: -0.07890203244263126\n",
      "          total_loss: 2.4081614112110947\n",
      "          vf_explained_var: 0.9052051050688631\n",
      "          vf_loss: 2.4715486689681008\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.058333333333334\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09955527790381716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08690871862992092\n",
      "    mean_inference_ms: 0.9837934120184366\n",
      "    mean_raw_obs_processing_ms: 0.08267140671873449\n",
      "  time_since_restore: 1184.7786803245544\n",
      "  time_this_iter_s: 8.343156576156616\n",
      "  time_total_s: 1184.7786803245544\n",
      "  timers:\n",
      "    learn_throughput: 684.125\n",
      "    learn_time_ms: 5846.881\n",
      "    load_throughput: 13406757.232\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 471.997\n",
      "    sample_time_ms: 8474.622\n",
      "    update_time_ms: 1.398\n",
      "  timestamp: 1643384900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:22 (running for 00:19:54.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         1184.78</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">-0.0870623</td><td style=\"text-align: right;\">             10.4586</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:27 (running for 00:19:59.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         1184.78</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">-0.0870623</td><td style=\"text-align: right;\">             10.4586</td><td style=\"text-align: right;\">             -13.233</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-48-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.458552837371826\n",
      "  episode_reward_mean: 0.04880751103162766\n",
      "  episode_reward_min: -12.589800149202347\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.051367319783857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015574000572025104\n",
      "          policy_loss: -0.0656502381728221\n",
      "          total_loss: 1.658431977837757\n",
      "          vf_explained_var: 0.9273841705373539\n",
      "          vf_loss: 1.708313537749552\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.758333333333333\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0995652753389911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08691361755561616\n",
      "    mean_inference_ms: 0.9838434667081768\n",
      "    mean_raw_obs_processing_ms: 0.08267629273160149\n",
      "  time_since_restore: 1193.327633857727\n",
      "  time_this_iter_s: 8.548953533172607\n",
      "  time_total_s: 1193.327633857727\n",
      "  timers:\n",
      "    learn_throughput: 684.954\n",
      "    learn_time_ms: 5839.807\n",
      "    load_throughput: 13612345.639\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 470.915\n",
      "    sample_time_ms: 8494.098\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1643384909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:33 (running for 00:20:05.08)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         1193.33</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">0.0488075</td><td style=\"text-align: right;\">             10.4586</td><td style=\"text-align: right;\">            -12.5898</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-48-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.458552837371826\n",
      "  episode_reward_mean: -0.027875255793333054\n",
      "  episode_reward_min: -12.589800149202347\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 6.03168445146212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015489049960708757\n",
      "          policy_loss: -0.08370343689075721\n",
      "          total_loss: 1.6479538193368102\n",
      "          vf_explained_var: 0.922602697982583\n",
      "          vf_loss: 1.7159745905187822\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.958333333333332\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09956937676552041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0869129214581233\n",
      "    mean_inference_ms: 0.9838178701037745\n",
      "    mean_raw_obs_processing_ms: 0.08267692004914534\n",
      "  time_since_restore: 1201.558845281601\n",
      "  time_this_iter_s: 8.231211423873901\n",
      "  time_total_s: 1201.558845281601\n",
      "  timers:\n",
      "    learn_throughput: 684.536\n",
      "    learn_time_ms: 5843.375\n",
      "    load_throughput: 13424994.799\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 471.9\n",
      "    sample_time_ms: 8476.381\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1643384917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:38 (running for 00:20:10.33)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         1201.56</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">-0.0278753</td><td style=\"text-align: right;\">             10.4586</td><td style=\"text-align: right;\">            -12.5898</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:43 (running for 00:20:15.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         1201.56</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">-0.0278753</td><td style=\"text-align: right;\">             10.4586</td><td style=\"text-align: right;\">            -12.5898</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-48-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.458552837371826\n",
      "  episode_reward_mean: 0.027669977843761445\n",
      "  episode_reward_min: -12.589800149202347\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.945245029080299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01693194372848947\n",
      "          policy_loss: -0.06881933620159504\n",
      "          total_loss: 1.714936491343323\n",
      "          vf_explained_var: 0.9232686004331035\n",
      "          vf_loss: 1.766612231202664\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.708333333333336\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0995699512107919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08691362166231435\n",
      "    mean_inference_ms: 0.9837862231622845\n",
      "    mean_raw_obs_processing_ms: 0.08267673822003237\n",
      "  time_since_restore: 1209.8615446090698\n",
      "  time_this_iter_s: 8.302699327468872\n",
      "  time_total_s: 1209.8615446090698\n",
      "  timers:\n",
      "    learn_throughput: 685.28\n",
      "    learn_time_ms: 5837.032\n",
      "    load_throughput: 13364040.147\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 471.177\n",
      "    sample_time_ms: 8489.373\n",
      "    update_time_ms: 1.423\n",
      "  timestamp: 1643384925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:48 (running for 00:20:20.66)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1209.86</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> 0.02767</td><td style=\"text-align: right;\">             10.4586</td><td style=\"text-align: right;\">            -12.5898</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:54 (running for 00:20:25.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1209.86</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> 0.02767</td><td style=\"text-align: right;\">             10.4586</td><td style=\"text-align: right;\">            -12.5898</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-48-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.64237404242158\n",
      "  episode_reward_mean: -0.10800465896725654\n",
      "  episode_reward_min: -12.589800149202347\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.932922867292999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016405004869254468\n",
      "          policy_loss: -0.08784741137917805\n",
      "          total_loss: 1.8051918938898692\n",
      "          vf_explained_var: 0.9096339719910775\n",
      "          vf_loss: 1.876429234212765\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.675\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09957905615797728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08692509939298519\n",
      "    mean_inference_ms: 0.9838861802040095\n",
      "    mean_raw_obs_processing_ms: 0.08269499479414359\n",
      "  time_since_restore: 1218.4608294963837\n",
      "  time_this_iter_s: 8.599284887313843\n",
      "  time_total_s: 1218.4608294963837\n",
      "  timers:\n",
      "    learn_throughput: 685.756\n",
      "    learn_time_ms: 5832.981\n",
      "    load_throughput: 13209366.192\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 470.864\n",
      "    sample_time_ms: 8495.014\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1643384934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:48:59 (running for 00:20:31.29)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         1218.46</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">-0.108005</td><td style=\"text-align: right;\">             10.6424</td><td style=\"text-align: right;\">            -12.5898</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-49-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.64237404242158\n",
      "  episode_reward_mean: -0.11095226943492889\n",
      "  episode_reward_min: -11.708702400326729\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.8773367112682715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015567972348017331\n",
      "          policy_loss: -0.11518023332921408\n",
      "          total_loss: 2.3314805248887427\n",
      "          vf_explained_var: 0.9095974163983458\n",
      "          vf_loss: 2.4308981740827202\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.083333333333332\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09959000790814018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08693784593670191\n",
      "    mean_inference_ms: 0.9840029328907783\n",
      "    mean_raw_obs_processing_ms: 0.08271517774440339\n",
      "  time_since_restore: 1227.0830607414246\n",
      "  time_this_iter_s: 8.622231245040894\n",
      "  time_total_s: 1227.0830607414246\n",
      "  timers:\n",
      "    learn_throughput: 682.269\n",
      "    learn_time_ms: 5862.788\n",
      "    load_throughput: 13318421.846\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 471.429\n",
      "    sample_time_ms: 8484.842\n",
      "    update_time_ms: 1.409\n",
      "  timestamp: 1643384943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:05 (running for 00:20:36.93)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         1227.08</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">-0.110952</td><td style=\"text-align: right;\">             10.6424</td><td style=\"text-align: right;\">            -11.7087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:10 (running for 00:20:41.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         1227.08</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">-0.110952</td><td style=\"text-align: right;\">             10.6424</td><td style=\"text-align: right;\">            -11.7087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.070515751838684\n",
      "  episode_reward_mean: -0.0011495235562324524\n",
      "  episode_reward_min: -11.708702400326729\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.871186801951419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016079155013268603\n",
      "          policy_loss: -0.06833215443747899\n",
      "          total_loss: 1.9847286435985758\n",
      "          vf_explained_var: 0.9177125287312333\n",
      "          vf_loss: 2.036780650400987\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.9\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09960623672382356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08695527071337551\n",
      "    mean_inference_ms: 0.9842179516286268\n",
      "    mean_raw_obs_processing_ms: 0.08274061596933734\n",
      "  time_since_restore: 1235.9091038703918\n",
      "  time_this_iter_s: 8.826043128967285\n",
      "  time_total_s: 1235.9091038703918\n",
      "  timers:\n",
      "    learn_throughput: 680.296\n",
      "    learn_time_ms: 5879.794\n",
      "    load_throughput: 13223942.618\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 470.176\n",
      "    sample_time_ms: 8507.452\n",
      "    update_time_ms: 1.386\n",
      "  timestamp: 1643384952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:16 (running for 00:20:47.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         1235.91</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\">-0.00114952</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -11.7087</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-49-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.070515751838684\n",
      "  episode_reward_mean: -0.0024492746591567994\n",
      "  episode_reward_min: -9.024889796972275\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.84062747750231\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016587333106176595\n",
      "          policy_loss: -0.09758639419150929\n",
      "          total_loss: 2.019995609685899\n",
      "          vf_explained_var: 0.9090001462608255\n",
      "          vf_loss: 2.100787321398015\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.491666666666667\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09963092229686386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08698016769544374\n",
      "    mean_inference_ms: 0.9845056973640467\n",
      "    mean_raw_obs_processing_ms: 0.08277205264000298\n",
      "  time_since_restore: 1244.493162870407\n",
      "  time_this_iter_s: 8.584059000015259\n",
      "  time_total_s: 1244.493162870407\n",
      "  timers:\n",
      "    learn_throughput: 678.863\n",
      "    learn_time_ms: 5892.206\n",
      "    load_throughput: 13103105.28\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 469.202\n",
      "    sample_time_ms: 8525.119\n",
      "    update_time_ms: 1.369\n",
      "  timestamp: 1643384960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:21 (running for 00:20:53.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         1244.49</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">-0.00244927</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -9.02489</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:26 (running for 00:20:58.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         1244.49</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">-0.00244927</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -9.02489</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-49-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.070515751838684\n",
      "  episode_reward_mean: -0.04657616257667541\n",
      "  episode_reward_min: -9.024889796972275\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.786895405861639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016465755682974204\n",
      "          policy_loss: -0.08918760419893329\n",
      "          total_loss: 2.2154869229122194\n",
      "          vf_explained_var: 0.8978151517529641\n",
      "          vf_loss: 2.288002949224044\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.807692307692307\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.099659115504071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08700528768757955\n",
      "    mean_inference_ms: 0.9848021454967555\n",
      "    mean_raw_obs_processing_ms: 0.0828067109071509\n",
      "  time_since_restore: 1253.1228761672974\n",
      "  time_this_iter_s: 8.629713296890259\n",
      "  time_total_s: 1253.1228761672974\n",
      "  timers:\n",
      "    learn_throughput: 680.061\n",
      "    learn_time_ms: 5881.821\n",
      "    load_throughput: 13017703.29\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 468.423\n",
      "    sample_time_ms: 8539.291\n",
      "    update_time_ms: 1.371\n",
      "  timestamp: 1643384969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:32 (running for 00:21:04.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         1253.12</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">-0.0465762</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -9.02489</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:37 (running for 00:21:09.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         1253.12</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">-0.0465762</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -9.02489</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-49-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.070515751838684\n",
      "  episode_reward_mean: 0.13594647914171218\n",
      "  episode_reward_min: -9.024889796972275\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.739993678882558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016499877620894332\n",
      "          policy_loss: -0.06684216815798033\n",
      "          total_loss: 2.8236606087094995\n",
      "          vf_explained_var: 0.8769419197754194\n",
      "          vf_loss: 2.8737966317483172\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.26666666666667\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09968325585065539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08702416190935873\n",
      "    mean_inference_ms: 0.9850354901911911\n",
      "    mean_raw_obs_processing_ms: 0.08282931347285355\n",
      "  time_since_restore: 1261.5138521194458\n",
      "  time_this_iter_s: 8.390975952148438\n",
      "  time_total_s: 1261.5138521194458\n",
      "  timers:\n",
      "    learn_throughput: 681.057\n",
      "    learn_time_ms: 5873.222\n",
      "    load_throughput: 12825637.184\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 468.391\n",
      "    sample_time_ms: 8539.879\n",
      "    update_time_ms: 1.384\n",
      "  timestamp: 1643384977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:42 (running for 00:21:14.46)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         1261.51</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">0.135946</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -9.02489</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-49-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.070515751838684\n",
      "  episode_reward_mean: 0.02966838225722313\n",
      "  episode_reward_min: -9.024889796972275\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.733891353812269\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016616224329114825\n",
      "          policy_loss: -0.09576805464733111\n",
      "          total_loss: 1.408916374495984\n",
      "          vf_explained_var: 0.9283740886436995\n",
      "          vf_loss: 1.4878605098413524\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.841666666666665\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09971110854162532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08705053078283687\n",
      "    mean_inference_ms: 0.9852750872184021\n",
      "    mean_raw_obs_processing_ms: 0.08285858304584741\n",
      "  time_since_restore: 1269.7621126174927\n",
      "  time_this_iter_s: 8.248260498046875\n",
      "  time_total_s: 1269.7621126174927\n",
      "  timers:\n",
      "    learn_throughput: 682.812\n",
      "    learn_time_ms: 5858.131\n",
      "    load_throughput: 12955379.151\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 468.567\n",
      "    sample_time_ms: 8536.667\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643384986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:48 (running for 00:21:19.72)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         1269.76</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\">0.0296684</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -9.02489</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:53 (running for 00:21:24.73)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         1269.76</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\">0.0296684</td><td style=\"text-align: right;\">             12.0705</td><td style=\"text-align: right;\">            -9.02489</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-49-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.214343160390854\n",
      "  episode_reward_mean: -0.05141240954399109\n",
      "  episode_reward_min: -9.179724991321564\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.734714694176951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01718342178127271\n",
      "          policy_loss: -0.09073801646910368\n",
      "          total_loss: 2.5695205514464448\n",
      "          vf_explained_var: 0.893208763548123\n",
      "          vf_loss: 2.6428603623182543\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.975\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09973692908618573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08707583209139226\n",
      "    mean_inference_ms: 0.9854519261462772\n",
      "    mean_raw_obs_processing_ms: 0.0828838935480124\n",
      "  time_since_restore: 1278.2159316539764\n",
      "  time_this_iter_s: 8.453819036483765\n",
      "  time_total_s: 1278.2159316539764\n",
      "  timers:\n",
      "    learn_throughput: 684.668\n",
      "    learn_time_ms: 5842.246\n",
      "    load_throughput: 12574738.42\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 469.062\n",
      "    sample_time_ms: 8527.649\n",
      "    update_time_ms: 1.438\n",
      "  timestamp: 1643384994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:49:58 (running for 00:21:30.20)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         1278.22</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">-0.0514124</td><td style=\"text-align: right;\">             11.2143</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.214343160390854\n",
      "  episode_reward_mean: 0.010440875291824341\n",
      "  episode_reward_min: -9.179724991321564\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.730941423805811\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016567297360606175\n",
      "          policy_loss: -0.07254324545182528\n",
      "          total_loss: 1.8029334198468194\n",
      "          vf_explained_var: 0.9122938540674025\n",
      "          vf_loss: 1.8587022731541305\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.783333333333335\n",
      "    ram_util_percent: 34.408333333333324\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09975534373771713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0870982657043615\n",
      "    mean_inference_ms: 0.985587037632406\n",
      "    mean_raw_obs_processing_ms: 0.0829035013023555\n",
      "  time_since_restore: 1286.878039598465\n",
      "  time_this_iter_s: 8.662107944488525\n",
      "  time_total_s: 1286.878039598465\n",
      "  timers:\n",
      "    learn_throughput: 680.243\n",
      "    learn_time_ms: 5880.255\n",
      "    load_throughput: 12735096.402\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 469.663\n",
      "    sample_time_ms: 8516.736\n",
      "    update_time_ms: 1.418\n",
      "  timestamp: 1643385003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:04 (running for 00:21:35.88)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         1286.88</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">0.0104409</td><td style=\"text-align: right;\">             11.2143</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:09 (running for 00:21:40.88)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         1286.88</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">0.0104409</td><td style=\"text-align: right;\">             11.2143</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-50-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.214343160390854\n",
      "  episode_reward_mean: 0.006665391325950623\n",
      "  episode_reward_min: -9.179724991321564\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.732299161726429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01576702365542562\n",
      "          policy_loss: -0.08326338230862573\n",
      "          total_loss: 2.0610939099648666\n",
      "          vf_explained_var: 0.9239989540910208\n",
      "          vf_loss: 2.1283931788097146\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.208333333333332\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09977211115041072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08712026091941581\n",
      "    mean_inference_ms: 0.9856986300401377\n",
      "    mean_raw_obs_processing_ms: 0.08292062672785193\n",
      "  time_since_restore: 1295.4802441596985\n",
      "  time_this_iter_s: 8.60220456123352\n",
      "  time_total_s: 1295.4802441596985\n",
      "  timers:\n",
      "    learn_throughput: 676.744\n",
      "    learn_time_ms: 5910.656\n",
      "    load_throughput: 12670656.295\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 467.604\n",
      "    sample_time_ms: 8554.24\n",
      "    update_time_ms: 1.409\n",
      "  timestamp: 1643385011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:14 (running for 00:21:46.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         1295.48</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">0.00666539</td><td style=\"text-align: right;\">             11.2143</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:19 (running for 00:21:51.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         1295.48</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">0.00666539</td><td style=\"text-align: right;\">             11.2143</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-50-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.221844613552094\n",
      "  episode_reward_mean: -0.11473837494850159\n",
      "  episode_reward_min: -9.179724991321564\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.680195415660899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016867017466137307\n",
      "          policy_loss: -0.11261049923637222\n",
      "          total_loss: 1.724029640832876\n",
      "          vf_explained_var: 0.9100806466994747\n",
      "          vf_loss: 1.819562281091367\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.478571428571424\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09978610741249991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0871403893399966\n",
      "    mean_inference_ms: 0.9857688016922413\n",
      "    mean_raw_obs_processing_ms: 0.08293470473474537\n",
      "  time_since_restore: 1305.2392086982727\n",
      "  time_this_iter_s: 9.758964538574219\n",
      "  time_total_s: 1305.2392086982727\n",
      "  timers:\n",
      "    learn_throughput: 661.48\n",
      "    learn_time_ms: 6047.05\n",
      "    load_throughput: 12838396.082\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 467.08\n",
      "    sample_time_ms: 8563.84\n",
      "    update_time_ms: 1.423\n",
      "  timestamp: 1643385021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:25 (running for 00:21:57.29)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         1305.24</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">-0.114738</td><td style=\"text-align: right;\">             9.22184</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:30 (running for 00:22:02.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         1305.24</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">-0.114738</td><td style=\"text-align: right;\">             9.22184</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-50-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.221844613552094\n",
      "  episode_reward_mean: 0.05439905315637589\n",
      "  episode_reward_min: -9.179724991321564\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.656396052657917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01651517628331165\n",
      "          policy_loss: -0.09175832917393055\n",
      "          total_loss: 1.4888486251122348\n",
      "          vf_explained_var: 0.9340725735951495\n",
      "          vf_loss: 1.5638853490032176\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.95\n",
      "    ram_util_percent: 34.40625\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09983402911335722\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0871883597324762\n",
      "    mean_inference_ms: 0.9862086367342284\n",
      "    mean_raw_obs_processing_ms: 0.08297404365944981\n",
      "  time_since_restore: 1316.350209236145\n",
      "  time_this_iter_s: 11.111000537872314\n",
      "  time_total_s: 1316.350209236145\n",
      "  timers:\n",
      "    learn_throughput: 643.182\n",
      "    learn_time_ms: 6219.078\n",
      "    load_throughput: 12433093.227\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 455.716\n",
      "    sample_time_ms: 8777.395\n",
      "    update_time_ms: 1.425\n",
      "  timestamp: 1643385032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:35 (running for 00:22:07.43)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         1316.35</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">0.0543991</td><td style=\"text-align: right;\">             9.22184</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:40 (running for 00:22:12.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         1316.35</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">0.0543991</td><td style=\"text-align: right;\">             9.22184</td><td style=\"text-align: right;\">            -9.17972</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-50-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.221844613552094\n",
      "  episode_reward_mean: 0.05522390842437744\n",
      "  episode_reward_min: -8.31297418475151\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.679104123064267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01650639036730622\n",
      "          policy_loss: -0.0654591027246688\n",
      "          total_loss: 1.2360861532930885\n",
      "          vf_explained_var: 0.9324362174798083\n",
      "          vf_loss: 1.2848325328160357\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.411764705882355\n",
      "    ram_util_percent: 34.4\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09992292640874234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08727022118451462\n",
      "    mean_inference_ms: 0.9870882907392772\n",
      "    mean_raw_obs_processing_ms: 0.08304732513335734\n",
      "  time_since_restore: 1327.8508627414703\n",
      "  time_this_iter_s: 11.500653505325317\n",
      "  time_total_s: 1327.8508627414703\n",
      "  timers:\n",
      "    learn_throughput: 624.345\n",
      "    learn_time_ms: 6406.713\n",
      "    load_throughput: 12524048.97\n",
      "    load_time_ms: 0.319\n",
      "    sample_throughput: 442.966\n",
      "    sample_time_ms: 9030.04\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1643385044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:46 (running for 00:22:17.96)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         1327.85</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">0.0552239</td><td style=\"text-align: right;\">             9.22184</td><td style=\"text-align: right;\">            -8.31297</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:51 (running for 00:22:22.97)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         1327.85</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">0.0552239</td><td style=\"text-align: right;\">             9.22184</td><td style=\"text-align: right;\">            -8.31297</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.181817382574081\n",
      "  episode_reward_mean: 0.09208339303731919\n",
      "  episode_reward_min: -11.497322410345078\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.736479340830157\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01564006943808686\n",
      "          policy_loss: -0.06125406589964905\n",
      "          total_loss: 2.9916208880562936\n",
      "          vf_explained_var: 0.8986291912935114\n",
      "          vf_loss: 3.0370393722608524\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.386666666666667\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10009444393878049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08741253756551189\n",
      "    mean_inference_ms: 0.9887433319401118\n",
      "    mean_raw_obs_processing_ms: 0.08318052879970972\n",
      "  time_since_restore: 1338.6803295612335\n",
      "  time_this_iter_s: 10.829466819763184\n",
      "  time_total_s: 1338.6803295612335\n",
      "  timers:\n",
      "    learn_throughput: 617.108\n",
      "    learn_time_ms: 6481.845\n",
      "    load_throughput: 12351627.77\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 426.993\n",
      "    sample_time_ms: 9367.827\n",
      "    update_time_ms: 1.407\n",
      "  timestamp: 1643385055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:50:57 (running for 00:22:28.82)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         1338.68</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">0.0920834</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:02 (running for 00:22:33.82)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         1338.68</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">0.0920834</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-51-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.181817382574081\n",
      "  episode_reward_mean: 0.03696979105472565\n",
      "  episode_reward_min: -11.497322410345078\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.697638827498241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015671599280075246\n",
      "          policy_loss: -0.0728843088211712\n",
      "          total_loss: 1.884587559852028\n",
      "          vf_explained_var: 0.9158008581207645\n",
      "          vf_loss: 1.9416043629248938\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.87142857142857\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10030020344691348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08758701745877334\n",
      "    mean_inference_ms: 0.990769217127717\n",
      "    mean_raw_obs_processing_ms: 0.08334225365737145\n",
      "  time_since_restore: 1348.645682811737\n",
      "  time_this_iter_s: 9.96535325050354\n",
      "  time_total_s: 1348.645682811737\n",
      "  timers:\n",
      "    learn_throughput: 611.647\n",
      "    learn_time_ms: 6539.716\n",
      "    load_throughput: 12355266.22\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 420.224\n",
      "    sample_time_ms: 9518.735\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1643385065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:08 (running for 00:22:39.81)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         1348.65</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">0.0369698</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:13 (running for 00:22:44.81)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         1348.65</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">0.0369698</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-51-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.181817382574081\n",
      "  episode_reward_mean: 0.09118962347507477\n",
      "  episode_reward_min: -11.497322410345078\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.6336915226392845\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017075476692805337\n",
      "          policy_loss: -0.08611231975818193\n",
      "          total_loss: 1.7734839473024853\n",
      "          vf_explained_var: 0.9105992930550729\n",
      "          vf_loss: 1.842307350968802\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.16923076923077\n",
      "    ram_util_percent: 34.407692307692294\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10051367855236851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08776763970934458\n",
      "    mean_inference_ms: 0.9928697374514188\n",
      "    mean_raw_obs_processing_ms: 0.08350868642691361\n",
      "  time_since_restore: 1357.5333924293518\n",
      "  time_this_iter_s: 8.887709617614746\n",
      "  time_total_s: 1357.5333924293518\n",
      "  timers:\n",
      "    learn_throughput: 608.018\n",
      "    learn_time_ms: 6578.752\n",
      "    load_throughput: 12403678.841\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 417.22\n",
      "    sample_time_ms: 9587.27\n",
      "    update_time_ms: 1.434\n",
      "  timestamp: 1643385074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:19 (running for 00:22:50.73)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         1357.53</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\">0.0911896</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-51-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.181817382574081\n",
      "  episode_reward_mean: -0.004337922316044569\n",
      "  episode_reward_min: -11.497322410345078\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.599913675554337\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01702879801535306\n",
      "          policy_loss: -0.09544447318385406\n",
      "          total_loss: 1.6740476977956351\n",
      "          vf_explained_var: 0.9163819951395835\n",
      "          vf_loss: 1.752250517343962\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.775000000000002\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10069854538240036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08792152486219493\n",
      "    mean_inference_ms: 0.9946796487688511\n",
      "    mean_raw_obs_processing_ms: 0.08365121822008537\n",
      "  time_since_restore: 1366.2261233329773\n",
      "  time_this_iter_s: 8.692730903625488\n",
      "  time_total_s: 1366.2261233329773\n",
      "  timers:\n",
      "    learn_throughput: 605.621\n",
      "    learn_time_ms: 6604.794\n",
      "    load_throughput: 12617294.126\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 414.696\n",
      "    sample_time_ms: 9645.626\n",
      "    update_time_ms: 1.4\n",
      "  timestamp: 1643385082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:24 (running for 00:22:56.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         1366.23</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">-0.00433792</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:29 (running for 00:23:01.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         1366.23</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">-0.00433792</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-51-31\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.181817382574081\n",
      "  episode_reward_mean: -0.055591155290603635\n",
      "  episode_reward_min: -11.497322410345078\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.51449665049071\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01705769936689376\n",
      "          policy_loss: -0.07672755239501355\n",
      "          total_loss: 2.356491016041768\n",
      "          vf_explained_var: 0.8966575025230326\n",
      "          vf_loss: 2.4159476288784574\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.653846153846153\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10085613901738846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08804526922258939\n",
      "    mean_inference_ms: 0.9961213255615791\n",
      "    mean_raw_obs_processing_ms: 0.08376787401075393\n",
      "  time_since_restore: 1375.0940687656403\n",
      "  time_this_iter_s: 8.867945432662964\n",
      "  time_total_s: 1375.0940687656403\n",
      "  timers:\n",
      "    learn_throughput: 603.516\n",
      "    learn_time_ms: 6627.83\n",
      "    load_throughput: 12788486.927\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 412.786\n",
      "    sample_time_ms: 9690.245\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1643385091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:35 (running for 00:23:07.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         1375.09</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">-0.0555912</td><td style=\"text-align: right;\">             10.1818</td><td style=\"text-align: right;\">            -11.4973</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.787297368049622\n",
      "  episode_reward_mean: -0.12669585675001144\n",
      "  episode_reward_min: -10.997580409049988\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.540692724720124\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017390969856605158\n",
      "          policy_loss: -0.09719910031004299\n",
      "          total_loss: 2.1723991440078345\n",
      "          vf_explained_var: 0.9070832808812459\n",
      "          vf_loss: 2.2519898896736485\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.958333333333336\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10093604580107048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08811608774902371\n",
      "    mean_inference_ms: 0.9968129204847079\n",
      "    mean_raw_obs_processing_ms: 0.08383038446787353\n",
      "  time_since_restore: 1383.51948928833\n",
      "  time_this_iter_s: 8.42542052268982\n",
      "  time_total_s: 1383.51948928833\n",
      "  timers:\n",
      "    learn_throughput: 606.533\n",
      "    learn_time_ms: 6594.856\n",
      "    load_throughput: 12794338.443\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 411.384\n",
      "    sample_time_ms: 9723.268\n",
      "    update_time_ms: 1.423\n",
      "  timestamp: 1643385100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:41 (running for 00:23:12.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         1383.52</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">-0.126696</td><td style=\"text-align: right;\">              8.7873</td><td style=\"text-align: right;\">            -10.9976</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:46 (running for 00:23:17.79)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         1383.52</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">-0.126696</td><td style=\"text-align: right;\">              8.7873</td><td style=\"text-align: right;\">            -10.9976</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-51-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.639105908572674\n",
      "  episode_reward_mean: -0.05991887152194977\n",
      "  episode_reward_min: -10.997580409049988\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.405537228943199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015840279315448617\n",
      "          policy_loss: -0.09004320400586772\n",
      "          total_loss: 1.991526201740408\n",
      "          vf_explained_var: 0.9150401753123089\n",
      "          vf_loss: 2.065531125576586\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.73333333333333\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10097401105601016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08814734457327415\n",
      "    mean_inference_ms: 0.9970629657706763\n",
      "    mean_raw_obs_processing_ms: 0.0838575419794837\n",
      "  time_since_restore: 1392.0216822624207\n",
      "  time_this_iter_s: 8.502192974090576\n",
      "  time_total_s: 1392.0216822624207\n",
      "  timers:\n",
      "    learn_throughput: 606.429\n",
      "    learn_time_ms: 6595.995\n",
      "    load_throughput: 12826617.737\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 413.261\n",
      "    sample_time_ms: 9679.119\n",
      "    update_time_ms: 1.427\n",
      "  timestamp: 1643385108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:51 (running for 00:23:23.31)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         1392.02</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">-0.0599189</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">            -10.9976</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:51:56 (running for 00:23:28.31)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         1392.02</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">-0.0599189</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">            -10.9976</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-51-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.639105908572674\n",
      "  episode_reward_mean: -0.08417647898197174\n",
      "  episode_reward_min: -10.997580409049988\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.335152684488604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017139799219571702\n",
      "          policy_loss: -0.0906671740440151\n",
      "          total_loss: 1.7212414138680023\n",
      "          vf_explained_var: 0.9167114172571449\n",
      "          vf_loss: 1.7945545388966477\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_agent_steps_trained: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.874999999999996\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1010017319715171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08816953290377992\n",
      "    mean_inference_ms: 0.997207796914875\n",
      "    mean_raw_obs_processing_ms: 0.08387584150382064\n",
      "  time_since_restore: 1400.3083183765411\n",
      "  time_this_iter_s: 8.286636114120483\n",
      "  time_total_s: 1400.3083183765411\n",
      "  timers:\n",
      "    learn_throughput: 619.923\n",
      "    learn_time_ms: 6452.417\n",
      "    load_throughput: 12651546.641\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 413.359\n",
      "    sample_time_ms: 9676.825\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643385116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:01 (running for 00:23:33.62)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         1400.31</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\">-0.0841765</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">            -10.9976</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-52-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.639105908572674\n",
      "  episode_reward_mean: -0.009272330570966006\n",
      "  episode_reward_min: -12.705048590898514\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.3114666410671765\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017308495741044476\n",
      "          policy_loss: -0.08981251548575138\n",
      "          total_loss: 3.50180895595601\n",
      "          vf_explained_var: 0.8560766461074993\n",
      "          vf_loss: 3.5740966263157063\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.183333333333334\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10102270502465464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08818218730052912\n",
      "    mean_inference_ms: 0.9972601153291294\n",
      "    mean_raw_obs_processing_ms: 0.08388681110356908\n",
      "  time_since_restore: 1408.7930085659027\n",
      "  time_this_iter_s: 8.484690189361572\n",
      "  time_total_s: 1408.7930085659027\n",
      "  timers:\n",
      "    learn_throughput: 638.374\n",
      "    learn_time_ms: 6265.917\n",
      "    load_throughput: 13168929.356\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 422.979\n",
      "    sample_time_ms: 9456.74\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1643385125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:07 (running for 00:23:39.13)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         1408.79</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">-0.00927233</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:12 (running for 00:23:44.14)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         1408.79</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">-0.00927233</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-52-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.639105908572674\n",
      "  episode_reward_mean: 0.025604296326637268\n",
      "  episode_reward_min: -12.705048590898514\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.372033681151687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016084652603463635\n",
      "          policy_loss: -0.083170528625769\n",
      "          total_loss: 1.7258435729367818\n",
      "          vf_explained_var: 0.920869888349246\n",
      "          vf_loss: 1.7927283864107824\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.89230769230769\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10102869299837192\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08819268156839527\n",
      "    mean_inference_ms: 0.997267242127078\n",
      "    mean_raw_obs_processing_ms: 0.08389112490810735\n",
      "  time_since_restore: 1417.314523935318\n",
      "  time_this_iter_s: 8.521515369415283\n",
      "  time_total_s: 1417.314523935318\n",
      "  timers:\n",
      "    learn_throughput: 661.257\n",
      "    learn_time_ms: 6049.084\n",
      "    load_throughput: 13290989.464\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 435.321\n",
      "    sample_time_ms: 9188.628\n",
      "    update_time_ms: 1.464\n",
      "  timestamp: 1643385133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:18 (running for 00:23:49.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         1417.31</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\">0.0256043</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-52-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.639105908572674\n",
      "  episode_reward_mean: 0.06948013782501221\n",
      "  episode_reward_min: -12.705048590898514\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.348614747037169\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0179430193951239\n",
      "          policy_loss: -0.07641974062388462\n",
      "          total_loss: 1.8613671233907583\n",
      "          vf_explained_var: 0.9181664379053218\n",
      "          vf_loss: 1.919619553391972\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.008333333333333\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10103417034567126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08819454060925218\n",
      "    mean_inference_ms: 0.9972528786852165\n",
      "    mean_raw_obs_processing_ms: 0.08388996935318961\n",
      "  time_since_restore: 1425.7209577560425\n",
      "  time_this_iter_s: 8.406433820724487\n",
      "  time_total_s: 1425.7209577560425\n",
      "  timers:\n",
      "    learn_throughput: 671.025\n",
      "    learn_time_ms: 5961.028\n",
      "    load_throughput: 13605722.164\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 453.66\n",
      "    sample_time_ms: 8817.179\n",
      "    update_time_ms: 1.464\n",
      "  timestamp: 1643385142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:23 (running for 00:23:55.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         1425.72</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">0.0694801</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:28 (running for 00:24:00.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         1425.72</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">0.0694801</td><td style=\"text-align: right;\">             12.6391</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-52-31\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.742764711380005\n",
      "  episode_reward_mean: 0.0162104395031929\n",
      "  episode_reward_min: -12.705048590898514\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.30873178051364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017240471957140544\n",
      "          policy_loss: -0.09594341729096667\n",
      "          total_loss: 1.562512297266894\n",
      "          vf_explained_var: 0.929977576258362\n",
      "          vf_loss: 1.640999730259821\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_agent_steps_trained: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.216666666666665\n",
      "    ram_util_percent: 34.408333333333324\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10104333896517226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08820134674455084\n",
      "    mean_inference_ms: 0.9972735619738998\n",
      "    mean_raw_obs_processing_ms: 0.08389469692805847\n",
      "  time_since_restore: 1434.38596534729\n",
      "  time_this_iter_s: 8.665007591247559\n",
      "  time_total_s: 1434.38596534729\n",
      "  timers:\n",
      "    learn_throughput: 676.488\n",
      "    learn_time_ms: 5912.89\n",
      "    load_throughput: 13815230.567\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 462.591\n",
      "    sample_time_ms: 8646.952\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1643385151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:34 (running for 00:24:05.79)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         1434.39</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\">0.0162104</td><td style=\"text-align: right;\">             10.7428</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:39 (running for 00:24:10.80)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         1434.39</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\">0.0162104</td><td style=\"text-align: right;\">             10.7428</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.742764711380005\n",
      "  episode_reward_mean: 0.10466268599033356\n",
      "  episode_reward_min: -12.705048590898514\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.279049177067254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01764464590051372\n",
      "          policy_loss: -0.08872518231992119\n",
      "          total_loss: 2.3485099387963513\n",
      "          vf_explained_var: 0.9036848405996959\n",
      "          vf_loss: 2.4193699016205725\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.183333333333334\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10105441987521081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08820946984553717\n",
      "    mean_inference_ms: 0.9973120298564812\n",
      "    mean_raw_obs_processing_ms: 0.08390022294868699\n",
      "  time_since_restore: 1442.9839580059052\n",
      "  time_this_iter_s: 8.597992658615112\n",
      "  time_total_s: 1442.9839580059052\n",
      "  timers:\n",
      "    learn_throughput: 677.772\n",
      "    learn_time_ms: 5901.691\n",
      "    load_throughput: 13921845.49\n",
      "    load_time_ms: 0.287\n",
      "    sample_throughput: 466.143\n",
      "    sample_time_ms: 8581.052\n",
      "    update_time_ms: 1.435\n",
      "  timestamp: 1643385159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:44 (running for 00:24:16.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         1442.98</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\">0.104663</td><td style=\"text-align: right;\">             10.7428</td><td style=\"text-align: right;\">             -12.705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-52-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.742764711380005\n",
      "  episode_reward_mean: 0.04043939054012299\n",
      "  episode_reward_min: -7.827045500278473\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.138830184936523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017163272457378177\n",
      "          policy_loss: -0.09534893757713739\n",
      "          total_loss: 1.8733204223661213\n",
      "          vf_explained_var: 0.9100141559877704\n",
      "          vf_loss: 1.9512915373168966\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_agent_steps_trained: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.707692307692305\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10106461795898508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08822146180193587\n",
      "    mean_inference_ms: 0.9973669590263836\n",
      "    mean_raw_obs_processing_ms: 0.08390652245247378\n",
      "  time_since_restore: 1451.7411060333252\n",
      "  time_this_iter_s: 8.757148027420044\n",
      "  time_total_s: 1451.7411060333252\n",
      "  timers:\n",
      "    learn_throughput: 675.215\n",
      "    learn_time_ms: 5924.035\n",
      "    load_throughput: 13456220.725\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 467.657\n",
      "    sample_time_ms: 8553.279\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1643385168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:50 (running for 00:24:22.20)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         1451.74</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\">0.0404394</td><td style=\"text-align: right;\">             10.7428</td><td style=\"text-align: right;\">            -7.82705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:52:55 (running for 00:24:27.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         1451.74</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\">0.0404394</td><td style=\"text-align: right;\">             10.7428</td><td style=\"text-align: right;\">            -7.82705</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-52-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.742764711380005\n",
      "  episode_reward_mean: -0.04698267340660095\n",
      "  episode_reward_min: -8.118774563074112\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.078325050107894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015596063712127928\n",
      "          policy_loss: -0.11677458753348678\n",
      "          total_loss: 1.7545189237367282\n",
      "          vf_explained_var: 0.9168184176568062\n",
      "          vf_loss: 1.8555024834570064\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.084615384615383\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10110141526724274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08825635412647279\n",
      "    mean_inference_ms: 0.9976742791493715\n",
      "    mean_raw_obs_processing_ms: 0.08393212138600599\n",
      "  time_since_restore: 1461.0188372135162\n",
      "  time_this_iter_s: 9.27773118019104\n",
      "  time_total_s: 1461.0188372135162\n",
      "  timers:\n",
      "    learn_throughput: 675.878\n",
      "    learn_time_ms: 5918.224\n",
      "    load_throughput: 13457300.072\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 463.891\n",
      "    sample_time_ms: 8622.714\n",
      "    update_time_ms: 1.479\n",
      "  timestamp: 1643385177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:00 (running for 00:24:32.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1461.02</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\">-0.0469827</td><td style=\"text-align: right;\">             10.7428</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:05 (running for 00:24:37.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1461.02</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\">-0.0469827</td><td style=\"text-align: right;\">             10.7428</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.0357586145401\n",
      "  episode_reward_mean: 0.025639976859092712\n",
      "  episode_reward_min: -8.118774563074112\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.007595758027928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017251893290023693\n",
      "          policy_loss: -0.07219303780716033\n",
      "          total_loss: 2.5016514866662924\n",
      "          vf_explained_var: 0.8897895700188093\n",
      "          vf_loss: 2.5563769975496875\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_agent_steps_trained: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.807692307692307\n",
      "    ram_util_percent: 34.45384615384615\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10114576304381806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08830088798302535\n",
      "    mean_inference_ms: 0.9980559929162552\n",
      "    mean_raw_obs_processing_ms: 0.08396453014363925\n",
      "  time_since_restore: 1470.1008501052856\n",
      "  time_this_iter_s: 9.08201289176941\n",
      "  time_total_s: 1470.1008501052856\n",
      "  timers:\n",
      "    learn_throughput: 670.061\n",
      "    learn_time_ms: 5969.608\n",
      "    load_throughput: 13290989.464\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 463.451\n",
      "    sample_time_ms: 8630.903\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1643385186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:10 (running for 00:24:42.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">          1470.1</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\"> 0.02564</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-53-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.0357586145401\n",
      "  episode_reward_mean: 0.03152356415987015\n",
      "  episode_reward_min: -8.118774563074112\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 5.001441175706925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017072136570250713\n",
      "          policy_loss: -0.10012363687518143\n",
      "          total_loss: 1.8059617655757334\n",
      "          vf_explained_var: 0.9024954034436133\n",
      "          vf_loss: 1.8887998742121521\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.56923076923077\n",
      "    ram_util_percent: 34.48461538461539\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10119488321017797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08835085410425701\n",
      "    mean_inference_ms: 0.9985225867929886\n",
      "    mean_raw_obs_processing_ms: 0.08400296482496707\n",
      "  time_since_restore: 1478.8846609592438\n",
      "  time_this_iter_s: 8.78381085395813\n",
      "  time_total_s: 1478.8846609592438\n",
      "  timers:\n",
      "    learn_throughput: 670.045\n",
      "    learn_time_ms: 5969.747\n",
      "    load_throughput: 13289936.629\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 459.212\n",
      "    sample_time_ms: 8710.574\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1643385195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:16 (running for 00:24:48.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         1478.88</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\">0.0315236</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:21 (running for 00:24:53.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         1478.88</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\">0.0315236</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-53-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.0357586145401\n",
      "  episode_reward_mean: -0.06366835787892342\n",
      "  episode_reward_min: -8.118774563074112\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.9623130849612656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016338324489358007\n",
      "          policy_loss: -0.08297976198335809\n",
      "          total_loss: 1.527398515026015\n",
      "          vf_explained_var: 0.9277660250663757\n",
      "          vf_loss: 1.5938357178882887\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_agent_steps_trained: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.46153846153846\n",
      "    ram_util_percent: 34.46153846153846\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10127148808577281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08842149780755125\n",
      "    mean_inference_ms: 0.999214589933443\n",
      "    mean_raw_obs_processing_ms: 0.08406012778071009\n",
      "  time_since_restore: 1488.5818943977356\n",
      "  time_this_iter_s: 9.697233438491821\n",
      "  time_total_s: 1488.5818943977356\n",
      "  timers:\n",
      "    learn_throughput: 660.543\n",
      "    learn_time_ms: 6055.625\n",
      "    load_throughput: 12569086.005\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 456.333\n",
      "    sample_time_ms: 8765.534\n",
      "    update_time_ms: 1.519\n",
      "  timestamp: 1643385205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:27 (running for 00:24:59.13)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         1488.58</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">-0.0636684</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:32 (running for 00:25:04.14)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         1488.58</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">-0.0636684</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.0357586145401\n",
      "  episode_reward_mean: -0.008511840254068375\n",
      "  episode_reward_min: -8.118774563074112\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.935252764404461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017021594786959526\n",
      "          policy_loss: -0.07806021056528534\n",
      "          total_loss: 1.7310179645581152\n",
      "          vf_explained_var: 0.9219138426165427\n",
      "          vf_loss: 1.7918438078735464\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.758823529411767\n",
      "    ram_util_percent: 34.48235294117647\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10142517888525976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08855765742070405\n",
      "    mean_inference_ms: 1.0006357675059945\n",
      "    mean_raw_obs_processing_ms: 0.08417133476106145\n",
      "  time_since_restore: 1500.5255069732666\n",
      "  time_this_iter_s: 11.943612575531006\n",
      "  time_total_s: 1500.5255069732666\n",
      "  timers:\n",
      "    learn_throughput: 641.748\n",
      "    learn_time_ms: 6232.972\n",
      "    load_throughput: 12206050.2\n",
      "    load_time_ms: 0.328\n",
      "    sample_throughput: 443.465\n",
      "    sample_time_ms: 9019.884\n",
      "    update_time_ms: 1.522\n",
      "  timestamp: 1643385217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:38 (running for 00:25:10.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         1500.53</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\">-0.00851184</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:43 (running for 00:25:15.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         1500.53</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\">-0.00851184</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">            -8.11877</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.0357586145401\n",
      "  episode_reward_mean: 0.04454379320144653\n",
      "  episode_reward_min: -10.60498833656311\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.915537523454235\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017496623051189382\n",
      "          policy_loss: -0.09358714939625834\n",
      "          total_loss: 1.5338411184875245\n",
      "          vf_explained_var: 0.9173557887795151\n",
      "          vf_loss: 1.6097129360642484\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_agent_steps_trained: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.94615384615384\n",
      "    ram_util_percent: 34.47692307692307\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10156108182573848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08867304188891556\n",
      "    mean_inference_ms: 1.0018216121604682\n",
      "    mean_raw_obs_processing_ms: 0.08426930299916545\n",
      "  time_since_restore: 1509.4368114471436\n",
      "  time_this_iter_s: 8.911304473876953\n",
      "  time_total_s: 1509.4368114471436\n",
      "  timers:\n",
      "    learn_throughput: 638.739\n",
      "    learn_time_ms: 6262.338\n",
      "    load_throughput: 12237210.795\n",
      "    load_time_ms: 0.327\n",
      "    sample_throughput: 434.469\n",
      "    sample_time_ms: 9206.647\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1643385226\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:49 (running for 00:25:21.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         1509.44</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">0.0445438</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:54 (running for 00:25:26.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         1509.44</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">0.0445438</td><td style=\"text-align: right;\">             11.0358</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.21650655567646\n",
      "  episode_reward_mean: -0.02047245591878891\n",
      "  episode_reward_min: -10.60498833656311\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.827652411307058\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017853738293358973\n",
      "          policy_loss: -0.08198484176900038\n",
      "          total_loss: 2.2376097422598873\n",
      "          vf_explained_var: 0.9211747489949709\n",
      "          vf_loss: 2.301517670401322\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.308333333333334\n",
      "    ram_util_percent: 34.44166666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10168893570193369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08877744277141114\n",
      "    mean_inference_ms: 1.0029273118430493\n",
      "    mean_raw_obs_processing_ms: 0.08435976377703196\n",
      "  time_since_restore: 1517.9644711017609\n",
      "  time_this_iter_s: 8.52765965461731\n",
      "  time_total_s: 1517.9644711017609\n",
      "  timers:\n",
      "    learn_throughput: 637.563\n",
      "    learn_time_ms: 6273.893\n",
      "    load_throughput: 12115262.854\n",
      "    load_time_ms: 0.33\n",
      "    sample_throughput: 433.075\n",
      "    sample_time_ms: 9236.276\n",
      "    update_time_ms: 1.505\n",
      "  timestamp: 1643385234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:53:59 (running for 00:25:31.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         1517.96</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\">-0.0204725</td><td style=\"text-align: right;\">             7.21651</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-54-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.21650655567646\n",
      "  episode_reward_mean: -0.02926121711730957\n",
      "  episode_reward_min: -10.60498833656311\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.712036553249564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019150464838216117\n",
      "          policy_loss: -0.09658738939311876\n",
      "          total_loss: 1.346302911810707\n",
      "          vf_explained_var: 0.9323485639146579\n",
      "          vf_loss: 1.4235004537608675\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_agent_steps_trained: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.741666666666664\n",
      "    ram_util_percent: 34.49166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10180366389734606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08887003177620766\n",
      "    mean_inference_ms: 1.0038763504595343\n",
      "    mean_raw_obs_processing_ms: 0.08443568028900325\n",
      "  time_since_restore: 1526.439383506775\n",
      "  time_this_iter_s: 8.474912405014038\n",
      "  time_total_s: 1526.439383506775\n",
      "  timers:\n",
      "    learn_throughput: 638.016\n",
      "    learn_time_ms: 6269.437\n",
      "    load_throughput: 11916482.705\n",
      "    load_time_ms: 0.336\n",
      "    sample_throughput: 433.2\n",
      "    sample_time_ms: 9233.616\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1643385243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:05 (running for 00:25:37.09)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         1526.44</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\">-0.0292612</td><td style=\"text-align: right;\">             7.21651</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:10 (running for 00:25:42.09)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         1526.44</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\">-0.0292612</td><td style=\"text-align: right;\">             7.21651</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-54-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.21650655567646\n",
      "  episode_reward_mean: 0.018723642975091933\n",
      "  episode_reward_min: -10.60498833656311\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.664960755071332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018935524110535236\n",
      "          policy_loss: -0.09057111895908551\n",
      "          total_loss: 1.5692363550305688\n",
      "          vf_explained_var: 0.9276543147461389\n",
      "          vf_loss: 1.6406352594334592\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.45384615384615\n",
      "    ram_util_percent: 34.42307692307691\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.101908082108839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08895341521770071\n",
      "    mean_inference_ms: 1.0047362680313483\n",
      "    mean_raw_obs_processing_ms: 0.08450518205715697\n",
      "  time_since_restore: 1535.299572467804\n",
      "  time_this_iter_s: 8.860188961029053\n",
      "  time_total_s: 1535.299572467804\n",
      "  timers:\n",
      "    learn_throughput: 638.719\n",
      "    learn_time_ms: 6262.538\n",
      "    load_throughput: 11885247.946\n",
      "    load_time_ms: 0.337\n",
      "    sample_throughput: 431.855\n",
      "    sample_time_ms: 9262.371\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1643385252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:16 (running for 00:25:47.97)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">          1535.3</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\">0.0187236</td><td style=\"text-align: right;\">             7.21651</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-54-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.21650655567646\n",
      "  episode_reward_mean: -0.013432518094778061\n",
      "  episode_reward_min: -10.60498833656311\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.623353263895999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018786627350010174\n",
      "          policy_loss: -0.09595913838955664\n",
      "          total_loss: 2.8383167366847717\n",
      "          vf_explained_var: 0.8766205144825802\n",
      "          vf_loss: 2.9152544175504995\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_agent_steps_trained: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.683333333333334\n",
      "    ram_util_percent: 34.449999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10192920287958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08896570367446444\n",
      "    mean_inference_ms: 1.0048173290719515\n",
      "    mean_raw_obs_processing_ms: 0.08451631663636817\n",
      "  time_since_restore: 1543.5718817710876\n",
      "  time_this_iter_s: 8.272309303283691\n",
      "  time_total_s: 1543.5718817710876\n",
      "  timers:\n",
      "    learn_throughput: 642.642\n",
      "    learn_time_ms: 6224.303\n",
      "    load_throughput: 12256879.018\n",
      "    load_time_ms: 0.326\n",
      "    sample_throughput: 432.649\n",
      "    sample_time_ms: 9245.37\n",
      "    update_time_ms: 1.456\n",
      "  timestamp: 1643385260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:21 (running for 00:25:53.26)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         1543.57</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\">-0.0134325</td><td style=\"text-align: right;\">             7.21651</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:26 (running for 00:25:58.27)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         1543.57</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\">-0.0134325</td><td style=\"text-align: right;\">             7.21651</td><td style=\"text-align: right;\">             -10.605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-54-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.373699069023132\n",
      "  episode_reward_mean: -0.00020559906959533692\n",
      "  episode_reward_min: -9.481960415840149\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.696990525337958\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018084306445634395\n",
      "          policy_loss: -0.09276354039658702\n",
      "          total_loss: 2.465177201723895\n",
      "          vf_explained_var: 0.8794835312392122\n",
      "          vf_loss: 2.5396303762191086\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.241666666666667\n",
      "    ram_util_percent: 34.46666666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10192848522439897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08896369417328236\n",
      "    mean_inference_ms: 1.0047319543883315\n",
      "    mean_raw_obs_processing_ms: 0.08451125656424816\n",
      "  time_since_restore: 1551.7456970214844\n",
      "  time_this_iter_s: 8.173815250396729\n",
      "  time_total_s: 1551.7456970214844\n",
      "  timers:\n",
      "    learn_throughput: 645.019\n",
      "    learn_time_ms: 6201.369\n",
      "    load_throughput: 12396346.978\n",
      "    load_time_ms: 0.323\n",
      "    sample_throughput: 438.647\n",
      "    sample_time_ms: 9118.954\n",
      "    update_time_ms: 1.444\n",
      "  timestamp: 1643385268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:31 (running for 00:26:03.46)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         1551.75</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">-0.000205599</td><td style=\"text-align: right;\">              7.3737</td><td style=\"text-align: right;\">            -9.48196</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:36 (running for 00:26:08.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         1551.75</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">-0.000205599</td><td style=\"text-align: right;\">              7.3737</td><td style=\"text-align: right;\">            -9.48196</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-54-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.373699069023132\n",
      "  episode_reward_mean: -0.03679962545633316\n",
      "  episode_reward_min: -9.481960415840149\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.751073091773577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018645653252801056\n",
      "          policy_loss: -0.08995880405869215\n",
      "          total_loss: 1.844274006287257\n",
      "          vf_explained_var: 0.920072308855672\n",
      "          vf_loss: 1.9153540862904441\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_agent_steps_trained: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.736363636363635\n",
      "    ram_util_percent: 34.472727272727276\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10191764002826431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08895654590956965\n",
      "    mean_inference_ms: 1.004565887101498\n",
      "    mean_raw_obs_processing_ms: 0.08450110978751138\n",
      "  time_since_restore: 1559.8690724372864\n",
      "  time_this_iter_s: 8.123375415802002\n",
      "  time_total_s: 1559.8690724372864\n",
      "  timers:\n",
      "    learn_throughput: 651.108\n",
      "    learn_time_ms: 6143.378\n",
      "    load_throughput: 12364371.73\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 441.6\n",
      "    sample_time_ms: 9057.981\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1643385276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:41 (running for 00:26:13.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         1559.87</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\">-0.0367996</td><td style=\"text-align: right;\">              7.3737</td><td style=\"text-align: right;\">            -9.48196</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-54-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.373699069023132\n",
      "  episode_reward_mean: -0.033923279494047165\n",
      "  episode_reward_min: -9.481960415840149\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.655287441643336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017560157893706334\n",
      "          policy_loss: -0.09634149818548492\n",
      "          total_loss: 1.8582700085260415\n",
      "          vf_explained_var: 0.9177020689492584\n",
      "          vf_loss: 1.9368318499336319\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.69166666666667\n",
      "    ram_util_percent: 34.425\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1019049325996832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08894879566424027\n",
      "    mean_inference_ms: 1.004397953509708\n",
      "    mean_raw_obs_processing_ms: 0.08448970570949793\n",
      "  time_since_restore: 1568.1677486896515\n",
      "  time_this_iter_s: 8.298676252365112\n",
      "  time_total_s: 1568.1677486896515\n",
      "  timers:\n",
      "    learn_throughput: 652.383\n",
      "    learn_time_ms: 6131.37\n",
      "    load_throughput: 12432171.916\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 446.239\n",
      "    sample_time_ms: 8963.814\n",
      "    update_time_ms: 1.412\n",
      "  timestamp: 1643385285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:47 (running for 00:26:18.93)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         1568.17</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">-0.0339233</td><td style=\"text-align: right;\">              7.3737</td><td style=\"text-align: right;\">            -9.48196</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:52 (running for 00:26:23.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         1568.17</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">-0.0339233</td><td style=\"text-align: right;\">              7.3737</td><td style=\"text-align: right;\">            -9.48196</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-54-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.379141241312027\n",
      "  episode_reward_mean: 0.054810067415237425\n",
      "  episode_reward_min: -13.130625754594803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.602797220086539\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01781971962654392\n",
      "          policy_loss: -0.08347919577412227\n",
      "          total_loss: 2.9183667932206423\n",
      "          vf_explained_var: 0.8953294281677533\n",
      "          vf_loss: 2.983803522763073\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_agent_steps_trained: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.66923076923077\n",
      "    ram_util_percent: 34.44615384615384\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10189048609777827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08894153540416923\n",
      "    mean_inference_ms: 1.0042642873535774\n",
      "    mean_raw_obs_processing_ms: 0.08447980366954791\n",
      "  time_since_restore: 1577.3084847927094\n",
      "  time_this_iter_s: 9.140736103057861\n",
      "  time_total_s: 1577.3084847927094\n",
      "  timers:\n",
      "    learn_throughput: 657.149\n",
      "    learn_time_ms: 6086.897\n",
      "    load_throughput: 13143138.269\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 447.394\n",
      "    sample_time_ms: 8940.668\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1643385294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:54:57 (running for 00:26:29.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         1577.31</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">0.0548101</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:02 (running for 00:26:34.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         1577.31</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">0.0548101</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-55-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.379141241312027\n",
      "  episode_reward_mean: 0.05106874495744705\n",
      "  episode_reward_min: -13.130625754594803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.709715073083037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018476400304003045\n",
      "          policy_loss: -0.08732847989166295\n",
      "          total_loss: 1.5794521871047915\n",
      "          vf_explained_var: 0.9259952082428881\n",
      "          vf_loss: 1.6480733103809817\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.43076923076923\n",
      "    ram_util_percent: 34.43846153846153\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188246776672516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08894023965562506\n",
      "    mean_inference_ms: 1.0041801223104918\n",
      "    mean_raw_obs_processing_ms: 0.08447122566230314\n",
      "  time_since_restore: 1586.0444362163544\n",
      "  time_this_iter_s: 8.73595142364502\n",
      "  time_total_s: 1586.0444362163544\n",
      "  timers:\n",
      "    learn_throughput: 674.677\n",
      "    learn_time_ms: 5928.765\n",
      "    load_throughput: 13441128.024\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 458.027\n",
      "    sample_time_ms: 8733.102\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1643385303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:08 (running for 00:26:39.86)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         1586.04</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\">0.0510687</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-55-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.379141241312027\n",
      "  episode_reward_mean: 0.021465066820383072\n",
      "  episode_reward_min: -13.130625754594803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.584494248256888\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018817192529405793\n",
      "          policy_loss: -0.09151306144983297\n",
      "          total_loss: 0.9028535467385995\n",
      "          vf_explained_var: 0.9516094138545375\n",
      "          vf_loss: 0.9753141968400889\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_agent_steps_trained: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.458333333333332\n",
      "    ram_util_percent: 34.44166666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188652543028966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0889522744974317\n",
      "    mean_inference_ms: 1.0042160248037972\n",
      "    mean_raw_obs_processing_ms: 0.08447733081136156\n",
      "  time_since_restore: 1594.6469264030457\n",
      "  time_this_iter_s: 8.602490186691284\n",
      "  time_total_s: 1594.6469264030457\n",
      "  timers:\n",
      "    learn_throughput: 677.095\n",
      "    learn_time_ms: 5907.589\n",
      "    load_throughput: 13326885.376\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 466.987\n",
      "    sample_time_ms: 8565.546\n",
      "    update_time_ms: 1.334\n",
      "  timestamp: 1643385311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:13 (running for 00:26:45.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         1594.65</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\">0.0214651</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:18 (running for 00:26:50.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         1594.65</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\">0.0214651</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-55-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.379141241312027\n",
      "  episode_reward_mean: 0.046807957291603086\n",
      "  episode_reward_min: -13.130625754594803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.416974183051817\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01937549985092202\n",
      "          policy_loss: -0.09459135847226266\n",
      "          total_loss: 1.4952241505834183\n",
      "          vf_explained_var: 0.9306596834813395\n",
      "          vf_loss: 1.5701978202468605\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.253846153846155\n",
      "    ram_util_percent: 34.46153846153846\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10189868331019353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08897337487537538\n",
      "    mean_inference_ms: 1.0043437327892861\n",
      "    mean_raw_obs_processing_ms: 0.08448983641347237\n",
      "  time_since_restore: 1603.3791272640228\n",
      "  time_this_iter_s: 8.732200860977173\n",
      "  time_total_s: 1603.3791272640228\n",
      "  timers:\n",
      "    learn_throughput: 675.066\n",
      "    learn_time_ms: 5925.345\n",
      "    load_throughput: 13360847.336\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 468.023\n",
      "    sample_time_ms: 8546.581\n",
      "    update_time_ms: 1.326\n",
      "  timestamp: 1643385320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:24 (running for 00:26:56.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         1603.38</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\">0.046808</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:29 (running for 00:27:01.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         1603.38</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\">0.046808</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-55-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.379141241312027\n",
      "  episode_reward_mean: 0.04140717670321464\n",
      "  episode_reward_min: -13.130625754594803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.323822658036344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018061898508696887\n",
      "          policy_loss: -0.09971393656025651\n",
      "          total_loss: 2.0133548284267446\n",
      "          vf_explained_var: 0.9143513656431629\n",
      "          vf_loss: 2.094781083076872\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_agent_steps_trained: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.441666666666666\n",
      "    ram_util_percent: 34.41666666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10191928744285757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08900093148256782\n",
      "    mean_inference_ms: 1.0045419068979335\n",
      "    mean_raw_obs_processing_ms: 0.08450943268785922\n",
      "  time_since_restore: 1612.4653010368347\n",
      "  time_this_iter_s: 9.08617377281189\n",
      "  time_total_s: 1612.4653010368347\n",
      "  timers:\n",
      "    learn_throughput: 669.883\n",
      "    learn_time_ms: 5971.19\n",
      "    load_throughput: 13543119.148\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 466.24\n",
      "    sample_time_ms: 8579.278\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1643385329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:34 (running for 00:27:06.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         1612.47</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\">0.0414072</td><td style=\"text-align: right;\">             9.37914</td><td style=\"text-align: right;\">            -13.1306</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-55-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.326399505138397\n",
      "  episode_reward_mean: -0.058655228614807126\n",
      "  episode_reward_min: -9.711094737052917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.301499321127451\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018447619933549243\n",
      "          policy_loss: -0.09625384112599716\n",
      "          total_loss: 1.143050409798881\n",
      "          vf_explained_var: 0.9403786105494345\n",
      "          vf_loss: 1.22062603038966\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.724999999999998\n",
      "    ram_util_percent: 34.49166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10192054168866627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08901666545056078\n",
      "    mean_inference_ms: 1.0045566861142077\n",
      "    mean_raw_obs_processing_ms: 0.08451342333975806\n",
      "  time_since_restore: 1620.6821279525757\n",
      "  time_this_iter_s: 8.216826915740967\n",
      "  time_total_s: 1620.6821279525757\n",
      "  timers:\n",
      "    learn_throughput: 673.102\n",
      "    learn_time_ms: 5942.633\n",
      "    load_throughput: 13631147.221\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 465.636\n",
      "    sample_time_ms: 8590.398\n",
      "    update_time_ms: 1.364\n",
      "  timestamp: 1643385337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:39 (running for 00:27:11.58)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         1620.68</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\">-0.0586552</td><td style=\"text-align: right;\">              7.3264</td><td style=\"text-align: right;\">            -9.71109</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:44 (running for 00:27:16.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         1620.68</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\">-0.0586552</td><td style=\"text-align: right;\">              7.3264</td><td style=\"text-align: right;\">            -9.71109</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-55-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.462371110916138\n",
      "  episode_reward_mean: -0.06185120403766632\n",
      "  episode_reward_min: -9.711094737052917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.251833947499593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01880518234742094\n",
      "          policy_loss: -0.0898153330089264\n",
      "          total_loss: 2.3624847879180164\n",
      "          vf_explained_var: 0.9047570702209268\n",
      "          vf_loss: 2.433259870632682\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_agent_steps_trained: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.741666666666664\n",
      "    ram_util_percent: 34.475\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10191613578459835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08902702190320558\n",
      "    mean_inference_ms: 1.004540421478211\n",
      "    mean_raw_obs_processing_ms: 0.08451691645556841\n",
      "  time_since_restore: 1628.920637845993\n",
      "  time_this_iter_s: 8.238509893417358\n",
      "  time_total_s: 1628.920637845993\n",
      "  timers:\n",
      "    learn_throughput: 673.917\n",
      "    learn_time_ms: 5935.445\n",
      "    load_throughput: 13360847.336\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 467.005\n",
      "    sample_time_ms: 8565.221\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1643385346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:50 (running for 00:27:21.85)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         1628.92</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\">-0.0618512</td><td style=\"text-align: right;\">             7.46237</td><td style=\"text-align: right;\">            -9.71109</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-55-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.737580299377441\n",
      "  episode_reward_mean: -0.05313633784651756\n",
      "  episode_reward_min: -9.711094737052917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.252903997257191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01777969536522435\n",
      "          policy_loss: -0.09300771484837457\n",
      "          total_loss: 1.5848840813887286\n",
      "          vf_explained_var: 0.9277069222542548\n",
      "          vf_loss: 1.6598898566458173\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.516666666666666\n",
      "    ram_util_percent: 34.49166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10190342100784129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08903189714445758\n",
      "    mean_inference_ms: 1.004460852561328\n",
      "    mean_raw_obs_processing_ms: 0.08451085467413293\n",
      "  time_since_restore: 1637.053198337555\n",
      "  time_this_iter_s: 8.13256049156189\n",
      "  time_total_s: 1637.053198337555\n",
      "  timers:\n",
      "    learn_throughput: 675.881\n",
      "    learn_time_ms: 5918.199\n",
      "    load_throughput: 13441128.024\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 466.647\n",
      "    sample_time_ms: 8571.788\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1643385354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:55:55 (running for 00:27:27.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         1637.05</td><td style=\"text-align: right;\">760000</td><td style=\"text-align: right;\">-0.0531363</td><td style=\"text-align: right;\">             8.73758</td><td style=\"text-align: right;\">            -9.71109</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:00 (running for 00:27:32.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         1637.05</td><td style=\"text-align: right;\">760000</td><td style=\"text-align: right;\">-0.0531363</td><td style=\"text-align: right;\">             8.73758</td><td style=\"text-align: right;\">            -9.71109</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-56-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.737580299377441\n",
      "  episode_reward_mean: 0.021406830418854952\n",
      "  episode_reward_min: -9.711094737052917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.143197428282871\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018833450515525492\n",
      "          policy_loss: -0.08438046690757557\n",
      "          total_loss: 1.256448068510821\n",
      "          vf_explained_var: 0.9280527673741823\n",
      "          vf_loss: 1.3217596603657609\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_agent_steps_trained: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.418181818181818\n",
      "    ram_util_percent: 34.481818181818184\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10187983331010869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08902541504406343\n",
      "    mean_inference_ms: 1.0042658793065147\n",
      "    mean_raw_obs_processing_ms: 0.08449570763440589\n",
      "  time_since_restore: 1645.1445662975311\n",
      "  time_this_iter_s: 8.091367959976196\n",
      "  time_total_s: 1645.1445662975311\n",
      "  timers:\n",
      "    learn_throughput: 675.403\n",
      "    learn_time_ms: 5922.394\n",
      "    load_throughput: 13371495.975\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 468.0\n",
      "    sample_time_ms: 8547.011\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1643385362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:05 (running for 00:27:37.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         1645.14</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\">0.0214068</td><td style=\"text-align: right;\">             8.73758</td><td style=\"text-align: right;\">            -9.71109</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:10 (running for 00:27:42.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         1645.14</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\">0.0214068</td><td style=\"text-align: right;\">             8.73758</td><td style=\"text-align: right;\">            -9.71109</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-56-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.737580299377441\n",
      "  episode_reward_mean: -0.037941989004611966\n",
      "  episode_reward_min: -9.56969302892685\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.146963922951811\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01723071869484079\n",
      "          policy_loss: -0.10835993165771167\n",
      "          total_loss: 1.1634393627948119\n",
      "          vf_explained_var: 0.936530157891653\n",
      "          vf_loss: 1.2543531949481657\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.700000000000003\n",
      "    ram_util_percent: 34.46923076923077\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10184621350020898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08901010980468226\n",
      "    mean_inference_ms: 1.0039752958252472\n",
      "    mean_raw_obs_processing_ms: 0.08447464423770094\n",
      "  time_since_restore: 1653.7703683376312\n",
      "  time_this_iter_s: 8.625802040100098\n",
      "  time_total_s: 1653.7703683376312\n",
      "  timers:\n",
      "    learn_throughput: 671.053\n",
      "    learn_time_ms: 5960.783\n",
      "    load_throughput: 13393913.46\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 468.121\n",
      "    sample_time_ms: 8544.807\n",
      "    update_time_ms: 1.412\n",
      "  timestamp: 1643385371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:16 (running for 00:27:47.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         1653.77</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\">-0.037942</td><td style=\"text-align: right;\">             8.73758</td><td style=\"text-align: right;\">            -9.56969</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 772000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-56-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.737580299377441\n",
      "  episode_reward_mean: -0.029611632525920868\n",
      "  episode_reward_min: -9.56969302892685\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.0697010665811515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01836617886506954\n",
      "          policy_loss: -0.08357529221202738\n",
      "          total_loss: 2.1715822635594035\n",
      "          vf_explained_var: 0.9106894381584659\n",
      "          vf_loss: 2.2365617907015225\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 772000\n",
      "    num_agent_steps_trained: 772000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.272727272727277\n",
      "    ram_util_percent: 34.45454545454545\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1018134507445552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08899485119129696\n",
      "    mean_inference_ms: 1.003689402647881\n",
      "    mean_raw_obs_processing_ms: 0.08445585370740386\n",
      "  time_since_restore: 1662.0586757659912\n",
      "  time_this_iter_s: 8.288307428359985\n",
      "  time_total_s: 1662.0586757659912\n",
      "  timers:\n",
      "    learn_throughput: 675.972\n",
      "    learn_time_ms: 5917.407\n",
      "    load_throughput: 13372561.773\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 468.306\n",
      "    sample_time_ms: 8541.415\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643385379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:21 (running for 00:27:53.07)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         1662.06</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\">-0.0296116</td><td style=\"text-align: right;\">             8.73758</td><td style=\"text-align: right;\">            -9.56969</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:26 (running for 00:27:58.08)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         1662.06</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\">-0.0296116</td><td style=\"text-align: right;\">             8.73758</td><td style=\"text-align: right;\">            -9.56969</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-56-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.552460968494415\n",
      "  episode_reward_mean: -0.0024753810465335847\n",
      "  episode_reward_min: -10.368322789669037\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.008907993121814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01965713260841864\n",
      "          policy_loss: -0.08791207091742625\n",
      "          total_loss: 2.316418837216903\n",
      "          vf_explained_var: 0.9252208389902628\n",
      "          vf_loss: 2.384428066023255\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.825\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10178248575148174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08897968711327658\n",
      "    mean_inference_ms: 1.003385495571664\n",
      "    mean_raw_obs_processing_ms: 0.08443452485621616\n",
      "  time_since_restore: 1670.3990383148193\n",
      "  time_this_iter_s: 8.340362548828125\n",
      "  time_total_s: 1670.3990383148193\n",
      "  timers:\n",
      "    learn_throughput: 678.978\n",
      "    learn_time_ms: 5891.211\n",
      "    load_throughput: 13173065.327\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 471.43\n",
      "    sample_time_ms: 8484.827\n",
      "    update_time_ms: 1.417\n",
      "  timestamp: 1643385387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:31 (running for 00:28:03.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">          1670.4</td><td style=\"text-align: right;\">776000</td><td style=\"text-align: right;\">-0.00247538</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-56-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.552460968494415\n",
      "  episode_reward_mean: 0.06169526845216751\n",
      "  episode_reward_min: -10.368322789669037\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 4.039105010801746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018921705082395154\n",
      "          policy_loss: -0.0928111234759932\n",
      "          total_loss: 2.2407545446298056\n",
      "          vf_explained_var: 0.8730259010868687\n",
      "          vf_loss: 2.3144074366538114\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_agent_steps_trained: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.06153846153846\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10176365677209383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08897129955124942\n",
      "    mean_inference_ms: 1.00313821448472\n",
      "    mean_raw_obs_processing_ms: 0.08441302520514238\n",
      "  time_since_restore: 1678.9567396640778\n",
      "  time_this_iter_s: 8.557701349258423\n",
      "  time_total_s: 1678.9567396640778\n",
      "  timers:\n",
      "    learn_throughput: 679.17\n",
      "    learn_time_ms: 5889.545\n",
      "    load_throughput: 13267865.56\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 473.065\n",
      "    sample_time_ms: 8455.505\n",
      "    update_time_ms: 1.435\n",
      "  timestamp: 1643385396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:37 (running for 00:28:09.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         1678.96</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\">0.0616953</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:42 (running for 00:28:14.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         1678.96</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\">0.0616953</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.552460968494415\n",
      "  episode_reward_mean: -0.041744268406182526\n",
      "  episode_reward_min: -10.368322789669037\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.9351735135560393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01934508858251795\n",
      "          policy_loss: -0.10242959584900609\n",
      "          total_loss: 1.3479200066172428\n",
      "          vf_explained_var: 0.9168023075467797\n",
      "          vf_loss: 1.4307627037007322\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.73333333333333\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10175055160671405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08896784618502891\n",
      "    mean_inference_ms: 1.002947730690203\n",
      "    mean_raw_obs_processing_ms: 0.08439642754155194\n",
      "  time_since_restore: 1687.3481287956238\n",
      "  time_this_iter_s: 8.39138913154602\n",
      "  time_total_s: 1687.3481287956238\n",
      "  timers:\n",
      "    learn_throughput: 681.385\n",
      "    learn_time_ms: 5870.4\n",
      "    load_throughput: 13314194.112\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 473.957\n",
      "    sample_time_ms: 8439.586\n",
      "    update_time_ms: 1.434\n",
      "  timestamp: 1643385404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:47 (running for 00:28:19.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         1687.35</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\">-0.0417443</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:52 (running for 00:28:24.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         1687.35</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\">-0.0417443</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 788000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-56-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.552460968494415\n",
      "  episode_reward_mean: -0.056843708455562594\n",
      "  episode_reward_min: -10.368322789669037\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.8886160537760746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017886100535895743\n",
      "          policy_loss: -0.11610569698355531\n",
      "          total_loss: 1.7306547464432336\n",
      "          vf_explained_var: 0.9162295715783232\n",
      "          vf_loss: 1.828650761211431\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 788000\n",
      "    num_agent_steps_trained: 788000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.416666666666668\n",
      "    ram_util_percent: 34.30833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10173764695068352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08896462940046584\n",
      "    mean_inference_ms: 1.0027567161880424\n",
      "    mean_raw_obs_processing_ms: 0.08437784352584059\n",
      "  time_since_restore: 1695.8459250926971\n",
      "  time_this_iter_s: 8.497796297073364\n",
      "  time_total_s: 1695.8459250926971\n",
      "  timers:\n",
      "    learn_throughput: 685.419\n",
      "    learn_time_ms: 5835.843\n",
      "    load_throughput: 13163763.044\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 476.393\n",
      "    sample_time_ms: 8396.424\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1643385413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:56:58 (running for 00:28:29.96)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         1695.85</td><td style=\"text-align: right;\">788000</td><td style=\"text-align: right;\">-0.0568437</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.552460968494415\n",
      "  episode_reward_mean: 0.10023305088281631\n",
      "  episode_reward_min: -10.368322789669037\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.8746549296122725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01789361746914222\n",
      "          policy_loss: -0.057424289787248256\n",
      "          total_loss: 0.9584404380746706\n",
      "          vf_explained_var: 0.9566300893342623\n",
      "          vf_loss: 0.9977474372954138\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.98333333333333\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10172551267178659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08896672700487056\n",
      "    mean_inference_ms: 1.0025837030603035\n",
      "    mean_raw_obs_processing_ms: 0.08436095947475007\n",
      "  time_since_restore: 1704.2561609745026\n",
      "  time_this_iter_s: 8.41023588180542\n",
      "  time_total_s: 1704.2561609745026\n",
      "  timers:\n",
      "    learn_throughput: 683.682\n",
      "    learn_time_ms: 5850.672\n",
      "    load_throughput: 13124631.151\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 478.158\n",
      "    sample_time_ms: 8365.427\n",
      "    update_time_ms: 1.41\n",
      "  timestamp: 1643385421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:03 (running for 00:28:35.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         1704.26</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\">0.100233</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:08 (running for 00:28:40.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         1704.26</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\">0.100233</td><td style=\"text-align: right;\">             10.5525</td><td style=\"text-align: right;\">            -10.3683</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 796000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.918220609426498\n",
      "  episode_reward_mean: 0.017701018005609512\n",
      "  episode_reward_min: -11.226848796010017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.7860185056604365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019061406395494022\n",
      "          policy_loss: -0.08825718357518154\n",
      "          total_loss: 2.947562519567067\n",
      "          vf_explained_var: 0.8892601899562343\n",
      "          vf_loss: 3.016520029290389\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 796000\n",
      "    num_agent_steps_trained: 796000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.191666666666666\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10171899219011618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08897076716870438\n",
      "    mean_inference_ms: 1.0024216269306745\n",
      "    mean_raw_obs_processing_ms: 0.08434685983633977\n",
      "  time_since_restore: 1712.6880872249603\n",
      "  time_this_iter_s: 8.431926250457764\n",
      "  time_total_s: 1712.6880872249603\n",
      "  timers:\n",
      "    learn_throughput: 681.357\n",
      "    learn_time_ms: 5870.64\n",
      "    load_throughput: 13293095.634\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 477.341\n",
      "    sample_time_ms: 8379.754\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643385430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:14 (running for 00:28:45.87)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         1712.69</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\">0.017701</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.918220609426498\n",
      "  episode_reward_mean: -0.00043411701917648313\n",
      "  episode_reward_min: -11.226848796010017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.750467655479267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01949408094418154\n",
      "          policy_loss: -0.09078112405033842\n",
      "          total_loss: 2.130155207708438\n",
      "          vf_explained_var: 0.9050726745077359\n",
      "          vf_loss: 2.2011985697092546\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_agent_steps_trained: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.841666666666665\n",
      "    ram_util_percent: 34.324999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1017039478513965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08896611835809923\n",
      "    mean_inference_ms: 1.002212574820873\n",
      "    mean_raw_obs_processing_ms: 0.08433082640714862\n",
      "  time_since_restore: 1721.0523025989532\n",
      "  time_this_iter_s: 8.36421537399292\n",
      "  time_total_s: 1721.0523025989532\n",
      "  timers:\n",
      "    learn_throughput: 678.804\n",
      "    learn_time_ms: 5892.72\n",
      "    load_throughput: 13144167.972\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 476.142\n",
      "    sample_time_ms: 8400.859\n",
      "    update_time_ms: 1.4\n",
      "  timestamp: 1643385438\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:19 (running for 00:28:51.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         1721.05</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\">-0.000434117</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:24 (running for 00:28:56.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         1721.05</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\">-0.000434117</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 804000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.918220609426498\n",
      "  episode_reward_mean: 0.0008834385871887207\n",
      "  episode_reward_min: -11.226848796010017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.6337662094382828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019362671503886233\n",
      "          policy_loss: -0.11353723610681231\n",
      "          total_loss: 1.7592502010851017\n",
      "          vf_explained_var: 0.9148094189423387\n",
      "          vf_loss: 1.85318273230586\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 804000\n",
      "    num_agent_steps_trained: 804000\n",
      "    num_steps_sampled: 804000\n",
      "    num_steps_trained: 804000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.075\n",
      "    ram_util_percent: 34.31666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10168827028698939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08896098876478459\n",
      "    mean_inference_ms: 1.0019996251953764\n",
      "    mean_raw_obs_processing_ms: 0.08431502312200248\n",
      "  time_since_restore: 1729.3608667850494\n",
      "  time_this_iter_s: 8.308564186096191\n",
      "  time_total_s: 1729.3608667850494\n",
      "  timers:\n",
      "    learn_throughput: 678.432\n",
      "    learn_time_ms: 5895.947\n",
      "    load_throughput: 13043003.965\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 473.857\n",
      "    sample_time_ms: 8441.373\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1643385446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 804000\n",
      "  training_iteration: 201\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:29 (running for 00:29:01.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         1729.36</td><td style=\"text-align: right;\">804000</td><td style=\"text-align: right;\">0.000883439</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:34 (running for 00:29:06.57)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         1729.36</td><td style=\"text-align: right;\">804000</td><td style=\"text-align: right;\">0.000883439</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.918220609426498\n",
      "  episode_reward_mean: 0.09135107696056366\n",
      "  episode_reward_min: -11.226848796010017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.600736471658112\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019119936664755772\n",
      "          policy_loss: -0.11201572180024638\n",
      "          total_loss: 1.932198668612788\n",
      "          vf_explained_var: 0.9217858103013807\n",
      "          vf_loss: 2.024855450620895\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.036363636363635\n",
      "    ram_util_percent: 34.309090909090905\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10166999976324621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08895348030501724\n",
      "    mean_inference_ms: 1.0017631677865213\n",
      "    mean_raw_obs_processing_ms: 0.08429729306772991\n",
      "  time_since_restore: 1737.4052829742432\n",
      "  time_this_iter_s: 8.044416189193726\n",
      "  time_total_s: 1737.4052829742432\n",
      "  timers:\n",
      "    learn_throughput: 684.321\n",
      "    learn_time_ms: 5845.21\n",
      "    load_throughput: 13005593.798\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 474.082\n",
      "    sample_time_ms: 8437.358\n",
      "    update_time_ms: 1.566\n",
      "  timestamp: 1643385454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:39 (running for 00:29:11.63)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         1737.41</td><td style=\"text-align: right;\">808000</td><td style=\"text-align: right;\">0.0913511</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 812000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-43\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.918220609426498\n",
      "  episode_reward_mean: -0.11093325674533844\n",
      "  episode_reward_min: -11.226848796010017\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.698597138671465\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01924900269537732\n",
      "          policy_loss: -0.10963138604276283\n",
      "          total_loss: 1.4340796142537398\n",
      "          vf_explained_var: 0.9237156774408074\n",
      "          vf_loss: 1.5242213760412509\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 812000\n",
      "    num_agent_steps_trained: 812000\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.041666666666668\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10164644056293877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08893590850741727\n",
      "    mean_inference_ms: 1.0014537678186408\n",
      "    mean_raw_obs_processing_ms: 0.08427326137259165\n",
      "  time_since_restore: 1745.487004995346\n",
      "  time_this_iter_s: 8.081722021102905\n",
      "  time_total_s: 1745.487004995346\n",
      "  timers:\n",
      "    learn_throughput: 684.825\n",
      "    learn_time_ms: 5840.905\n",
      "    load_throughput: 13150349.585\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 477.864\n",
      "    sample_time_ms: 8370.575\n",
      "    update_time_ms: 1.567\n",
      "  timestamp: 1643385463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 203\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:45 (running for 00:29:16.74)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         1745.49</td><td style=\"text-align: right;\">812000</td><td style=\"text-align: right;\">-0.110933</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:50 (running for 00:29:21.74)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         1745.49</td><td style=\"text-align: right;\">812000</td><td style=\"text-align: right;\">-0.110933</td><td style=\"text-align: right;\">             11.9182</td><td style=\"text-align: right;\">            -11.2268</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 816000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.826999485492706\n",
      "  episode_reward_mean: -0.0480599532276392\n",
      "  episode_reward_min: -7.568265318870544\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.7226385785687355\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01769393820444608\n",
      "          policy_loss: -0.11468230881678161\n",
      "          total_loss: 1.4714636785018267\n",
      "          vf_explained_var: 0.923908245050779\n",
      "          vf_loss: 1.5682308685234798\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 816000\n",
      "    num_agent_steps_trained: 816000\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.45\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10161778815817897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08891481058290922\n",
      "    mean_inference_ms: 1.0011337725097502\n",
      "    mean_raw_obs_processing_ms: 0.08424522565507653\n",
      "  time_since_restore: 1753.7110197544098\n",
      "  time_this_iter_s: 8.22401475906372\n",
      "  time_total_s: 1753.7110197544098\n",
      "  timers:\n",
      "    learn_throughput: 686.059\n",
      "    learn_time_ms: 5830.4\n",
      "    load_throughput: 13400332.268\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 478.157\n",
      "    sample_time_ms: 8365.46\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1643385471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 204\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:57:55 (running for 00:29:26.99)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         1753.71</td><td style=\"text-align: right;\">816000</td><td style=\"text-align: right;\">-0.04806</td><td style=\"text-align: right;\">               8.827</td><td style=\"text-align: right;\">            -7.56827</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-57-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.826999485492706\n",
      "  episode_reward_mean: -0.1041722397506237\n",
      "  episode_reward_min: -7.568265318870544\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.680090763748333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019501518778317763\n",
      "          policy_loss: -0.10648738415711509\n",
      "          total_loss: 1.767945457824696\n",
      "          vf_explained_var: 0.9135312620670565\n",
      "          vf_loss: 1.8546875508482097\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 820000\n",
      "    num_agent_steps_trained: 820000\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.81666666666667\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159921750713821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08889512793033053\n",
      "    mean_inference_ms: 1.0008313804877567\n",
      "    mean_raw_obs_processing_ms: 0.08421848451396496\n",
      "  time_since_restore: 1762.1288757324219\n",
      "  time_this_iter_s: 8.417855978012085\n",
      "  time_total_s: 1762.1288757324219\n",
      "  timers:\n",
      "    learn_throughput: 686.888\n",
      "    learn_time_ms: 5823.365\n",
      "    load_throughput: 13486508.039\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 479.145\n",
      "    sample_time_ms: 8348.201\n",
      "    update_time_ms: 1.531\n",
      "  timestamp: 1643385479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 205\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:00 (running for 00:29:32.43)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         1762.13</td><td style=\"text-align: right;\">820000</td><td style=\"text-align: right;\">-0.104172</td><td style=\"text-align: right;\">               8.827</td><td style=\"text-align: right;\">            -7.56827</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:05 (running for 00:29:37.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         1762.13</td><td style=\"text-align: right;\">820000</td><td style=\"text-align: right;\">-0.104172</td><td style=\"text-align: right;\">               8.827</td><td style=\"text-align: right;\">            -7.56827</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-58-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.826999485492706\n",
      "  episode_reward_mean: -0.007227692902088165\n",
      "  episode_reward_min: -7.568265318870544\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.5549650053824147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01997314504590669\n",
      "          policy_loss: -0.09608573488470527\n",
      "          total_loss: 1.8191674147414605\n",
      "          vf_explained_var: 0.9215525862350259\n",
      "          vf_loss: 1.8950303304419722\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.658333333333335\n",
      "    ram_util_percent: 34.30833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10158559538911559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08888024906704807\n",
      "    mean_inference_ms: 1.000577461917046\n",
      "    mean_raw_obs_processing_ms: 0.0841965524254013\n",
      "  time_since_restore: 1770.7906527519226\n",
      "  time_this_iter_s: 8.661777019500732\n",
      "  time_total_s: 1770.7906527519226\n",
      "  timers:\n",
      "    learn_throughput: 685.053\n",
      "    learn_time_ms: 5838.962\n",
      "    load_throughput: 13353403.375\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 478.912\n",
      "    sample_time_ms: 8352.265\n",
      "    update_time_ms: 1.517\n",
      "  timestamp: 1643385488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 206\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:11 (running for 00:29:43.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         1770.79</td><td style=\"text-align: right;\">824000</td><td style=\"text-align: right;\">-0.00722769</td><td style=\"text-align: right;\">               8.827</td><td style=\"text-align: right;\">            -7.56827</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:16 (running for 00:29:48.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         1770.79</td><td style=\"text-align: right;\">824000</td><td style=\"text-align: right;\">-0.00722769</td><td style=\"text-align: right;\">               8.827</td><td style=\"text-align: right;\">            -7.56827</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 828000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-58-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.341386198997498\n",
      "  episode_reward_mean: -0.006400400400161743\n",
      "  episode_reward_min: -5.96499639749527\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.6004162603808987\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019225841545862432\n",
      "          policy_loss: -0.12007748944414479\n",
      "          total_loss: 1.0834103438284708\n",
      "          vf_explained_var: 0.9425262963900002\n",
      "          vf_loss: 1.1840216675272552\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 828000\n",
      "    num_agent_steps_trained: 828000\n",
      "    num_steps_sampled: 828000\n",
      "    num_steps_trained: 828000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.791666666666668\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10158435340628863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08887424215422274\n",
      "    mean_inference_ms: 1.0004247660999153\n",
      "    mean_raw_obs_processing_ms: 0.08418433063598357\n",
      "  time_since_restore: 1779.3028726577759\n",
      "  time_this_iter_s: 8.512219905853271\n",
      "  time_total_s: 1779.3028726577759\n",
      "  timers:\n",
      "    learn_throughput: 687.344\n",
      "    learn_time_ms: 5819.504\n",
      "    load_throughput: 13511489.088\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 476.831\n",
      "    sample_time_ms: 8388.717\n",
      "    update_time_ms: 1.55\n",
      "  timestamp: 1643385496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 828000\n",
      "  training_iteration: 207\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:21 (running for 00:29:53.65)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">          1779.3</td><td style=\"text-align: right;\">828000</td><td style=\"text-align: right;\">-0.0064004</td><td style=\"text-align: right;\">             8.34139</td><td style=\"text-align: right;\">              -5.965</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 832000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-58-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.310214035212994\n",
      "  episode_reward_mean: 0.021427293419837953\n",
      "  episode_reward_min: -8.518956504762173\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0124999999999997\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.5049245070385675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020259454909104933\n",
      "          policy_loss: -0.09812597252368446\n",
      "          total_loss: 2.0871564478393125\n",
      "          vf_explained_var: 0.9125299368494301\n",
      "          vf_loss: 2.1647697164887383\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 832000\n",
      "    num_agent_steps_trained: 832000\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.916666666666664\n",
      "    ram_util_percent: 34.30833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159159557085896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08887863400173768\n",
      "    mean_inference_ms: 1.0003329393265203\n",
      "    mean_raw_obs_processing_ms: 0.08417699520748709\n",
      "  time_since_restore: 1787.6746220588684\n",
      "  time_this_iter_s: 8.37174940109253\n",
      "  time_total_s: 1787.6746220588684\n",
      "  timers:\n",
      "    learn_throughput: 687.663\n",
      "    learn_time_ms: 5816.799\n",
      "    load_throughput: 13455141.551\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 478.02\n",
      "    sample_time_ms: 8367.853\n",
      "    update_time_ms: 1.557\n",
      "  timestamp: 1643385505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 208\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:27 (running for 00:29:59.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         1787.67</td><td style=\"text-align: right;\">832000</td><td style=\"text-align: right;\">0.0214273</td><td style=\"text-align: right;\">             8.31021</td><td style=\"text-align: right;\">            -8.51896</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:32 (running for 00:30:04.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         1787.67</td><td style=\"text-align: right;\">832000</td><td style=\"text-align: right;\">0.0214273</td><td style=\"text-align: right;\">             8.31021</td><td style=\"text-align: right;\">            -8.51896</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 836000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.310214035212994\n",
      "  episode_reward_mean: 0.01794965095818043\n",
      "  episode_reward_min: -8.518956504762173\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.5011736090465257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015675884149424876\n",
      "          policy_loss: -0.10701517289634593\n",
      "          total_loss: 1.108513179765163\n",
      "          vf_explained_var: 0.9376636361563078\n",
      "          vf_loss: 1.191720601804154\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 836000\n",
      "    num_agent_steps_trained: 836000\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.308333333333334\n",
      "    ram_util_percent: 34.349999999999994\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159689921668445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08888520495609711\n",
      "    mean_inference_ms: 1.0003196181089542\n",
      "    mean_raw_obs_processing_ms: 0.08417254171548592\n",
      "  time_since_restore: 1796.2640368938446\n",
      "  time_this_iter_s: 8.589414834976196\n",
      "  time_total_s: 1796.2640368938446\n",
      "  timers:\n",
      "    learn_throughput: 687.225\n",
      "    learn_time_ms: 5820.507\n",
      "    load_throughput: 13429293.204\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 477.48\n",
      "    sample_time_ms: 8377.307\n",
      "    update_time_ms: 1.538\n",
      "  timestamp: 1643385513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 209\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:37 (running for 00:30:09.66)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         1796.26</td><td style=\"text-align: right;\">836000</td><td style=\"text-align: right;\">0.0179497</td><td style=\"text-align: right;\">             8.31021</td><td style=\"text-align: right;\">            -8.51896</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-58-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.350282907485962\n",
      "  episode_reward_mean: 0.024533400088548662\n",
      "  episode_reward_min: -8.518956504762173\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.474586595514769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015386394714944962\n",
      "          policy_loss: -0.10927411996388949\n",
      "          total_loss: 3.3193518395986286\n",
      "          vf_explained_var: 0.8835168495614042\n",
      "          vf_loss: 3.4052578775232196\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.792307692307695\n",
      "    ram_util_percent: 34.307692307692314\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159535292813592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08889446397978247\n",
      "    mean_inference_ms: 1.0003443363827944\n",
      "    mean_raw_obs_processing_ms: 0.08417280573379486\n",
      "  time_since_restore: 1804.8252437114716\n",
      "  time_this_iter_s: 8.561206817626953\n",
      "  time_total_s: 1804.8252437114716\n",
      "  timers:\n",
      "    learn_throughput: 686.615\n",
      "    learn_time_ms: 5825.678\n",
      "    load_throughput: 13484340.138\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 476.446\n",
      "    sample_time_ms: 8395.494\n",
      "    update_time_ms: 1.553\n",
      "  timestamp: 1643385522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 210\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:43 (running for 00:30:15.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         1804.83</td><td style=\"text-align: right;\">840000</td><td style=\"text-align: right;\">0.0245334</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -8.51896</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:48 (running for 00:30:20.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         1804.83</td><td style=\"text-align: right;\">840000</td><td style=\"text-align: right;\">0.0245334</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -8.51896</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 844000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.350282907485962\n",
      "  episode_reward_mean: -0.03753458082675934\n",
      "  episode_reward_min: -10.41817781329155\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.3978322903315226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015090392480707955\n",
      "          policy_loss: -0.10970926780393848\n",
      "          total_loss: 1.965287927532148\n",
      "          vf_explained_var: 0.9140591661135355\n",
      "          vf_loss: 2.0520786728429536\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 844000\n",
      "    num_agent_steps_trained: 844000\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.341666666666665\n",
      "    ram_util_percent: 34.31666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10158886437092782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08889868738785739\n",
      "    mean_inference_ms: 1.0003237454510683\n",
      "    mean_raw_obs_processing_ms: 0.08416765081406975\n",
      "  time_since_restore: 1813.2223389148712\n",
      "  time_this_iter_s: 8.397095203399658\n",
      "  time_total_s: 1813.2223389148712\n",
      "  timers:\n",
      "    learn_throughput: 684.761\n",
      "    learn_time_ms: 5841.45\n",
      "    load_throughput: 13745056.53\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 476.534\n",
      "    sample_time_ms: 8393.953\n",
      "    update_time_ms: 1.558\n",
      "  timestamp: 1643385530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 211\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:53 (running for 00:30:25.66)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         1813.22</td><td style=\"text-align: right;\">844000</td><td style=\"text-align: right;\">-0.0375346</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:58:59 (running for 00:30:30.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         1813.22</td><td style=\"text-align: right;\">844000</td><td style=\"text-align: right;\">-0.0375346</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 848000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.350282907485962\n",
      "  episode_reward_mean: -0.0005908259749412537\n",
      "  episode_reward_min: -10.41817781329155\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.4678813585671047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014903131088083756\n",
      "          policy_loss: -0.1054219022734962\n",
      "          total_loss: 1.0752122852500647\n",
      "          vf_explained_var: 0.9487545349264658\n",
      "          vf_loss: 1.1580000675173217\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 848000\n",
      "    num_agent_steps_trained: 848000\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.323076923076922\n",
      "    ram_util_percent: 34.315384615384616\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10158691190526216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08891216670364123\n",
      "    mean_inference_ms: 1.0003779604587597\n",
      "    mean_raw_obs_processing_ms: 0.08416859067647066\n",
      "  time_since_restore: 1822.3144226074219\n",
      "  time_this_iter_s: 9.09208369255066\n",
      "  time_total_s: 1822.3144226074219\n",
      "  timers:\n",
      "    learn_throughput: 677.982\n",
      "    learn_time_ms: 5899.864\n",
      "    load_throughput: 13569407.959\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 473.022\n",
      "    sample_time_ms: 8456.265\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1643385540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 212\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:04 (running for 00:30:35.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         1822.31</td><td style=\"text-align: right;\">848000</td><td style=\"text-align: right;\">-0.000590826</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 852000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-59-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.350282907485962\n",
      "  episode_reward_mean: -0.07106994591653347\n",
      "  episode_reward_min: -10.41817781329155\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.42006819094381\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014752101877805816\n",
      "          policy_loss: -0.11160762749452104\n",
      "          total_loss: 1.7773982625975404\n",
      "          vf_explained_var: 0.9186253732250583\n",
      "          vf_loss: 1.866601141005434\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 852000\n",
      "    num_agent_steps_trained: 852000\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.791666666666668\n",
      "    ram_util_percent: 34.31666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1016026952460616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08893424373110775\n",
      "    mean_inference_ms: 1.0005743822954463\n",
      "    mean_raw_obs_processing_ms: 0.0841789864074642\n",
      "  time_since_restore: 1831.269710302353\n",
      "  time_this_iter_s: 8.95528769493103\n",
      "  time_total_s: 1831.269710302353\n",
      "  timers:\n",
      "    learn_throughput: 674.543\n",
      "    learn_time_ms: 5929.944\n",
      "    load_throughput: 13476757.973\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 466.642\n",
      "    sample_time_ms: 8571.875\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1643385549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 213\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:10 (running for 00:30:41.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         1831.27</td><td style=\"text-align: right;\">852000</td><td style=\"text-align: right;\">-0.0710699</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:15 (running for 00:30:46.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         1831.27</td><td style=\"text-align: right;\">852000</td><td style=\"text-align: right;\">-0.0710699</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-59-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.350282907485962\n",
      "  episode_reward_mean: -0.0022429689764976502\n",
      "  episode_reward_min: -10.41817781329155\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.3995131008086665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015492759032366013\n",
      "          policy_loss: -0.11472251661727706\n",
      "          total_loss: 1.9009014901614958\n",
      "          vf_explained_var: 0.9192928368045438\n",
      "          vf_loss: 1.9920943773722135\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.784615384615385\n",
      "    ram_util_percent: 34.323076923076925\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10161998563532493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08895774781767353\n",
      "    mean_inference_ms: 1.0007174369878646\n",
      "    mean_raw_obs_processing_ms: 0.08419072165846928\n",
      "  time_since_restore: 1839.7795791625977\n",
      "  time_this_iter_s: 8.509868860244751\n",
      "  time_total_s: 1839.7795791625977\n",
      "  timers:\n",
      "    learn_throughput: 672.124\n",
      "    learn_time_ms: 5951.285\n",
      "    load_throughput: 13469184.329\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 464.614\n",
      "    sample_time_ms: 8609.301\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1643385557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 214\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:20 (running for 00:30:52.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         1839.78</td><td style=\"text-align: right;\">856000</td><td style=\"text-align: right;\">-0.00224297</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:25 (running for 00:30:57.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         1839.78</td><td style=\"text-align: right;\">856000</td><td style=\"text-align: right;\">-0.00224297</td><td style=\"text-align: right;\">             10.3503</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.304918587207794\n",
      "  episode_reward_mean: 0.026406646072864533\n",
      "  episode_reward_min: -10.41817781329155\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.363257549911417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015960026403854865\n",
      "          policy_loss: -0.10509416127442232\n",
      "          total_loss: 1.2216527535780384\n",
      "          vf_explained_var: 0.9363262826396573\n",
      "          vf_loss: 1.3025076279396652\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 860000\n",
      "    num_agent_steps_trained: 860000\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.541666666666668\n",
      "    ram_util_percent: 34.31666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1016304621645531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08897426661298795\n",
      "    mean_inference_ms: 1.00077824663901\n",
      "    mean_raw_obs_processing_ms: 0.08419513406864568\n",
      "  time_since_restore: 1848.3582973480225\n",
      "  time_this_iter_s: 8.578718185424805\n",
      "  time_total_s: 1848.3582973480225\n",
      "  timers:\n",
      "    learn_throughput: 668.683\n",
      "    learn_time_ms: 5981.903\n",
      "    load_throughput: 13434670.083\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 464.236\n",
      "    sample_time_ms: 8616.303\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1643385566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 215\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:31 (running for 00:31:02.90)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         1848.36</td><td style=\"text-align: right;\">860000</td><td style=\"text-align: right;\">0.0264066</td><td style=\"text-align: right;\">             8.30492</td><td style=\"text-align: right;\">            -10.4182</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 864000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-59-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.974793739616871\n",
      "  episode_reward_mean: -0.07485957682132721\n",
      "  episode_reward_min: -9.589561738073826\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.3729438312592044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013844097922172014\n",
      "          policy_loss: -0.11771626353103627\n",
      "          total_loss: 3.2431820946365035\n",
      "          vf_explained_var: 0.8881579405517989\n",
      "          vf_loss: 3.3398726204550395\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 864000\n",
      "    num_agent_steps_trained: 864000\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.958333333333332\n",
      "    ram_util_percent: 34.30833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1016545242282378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0889961678981369\n",
      "    mean_inference_ms: 1.00086957011855\n",
      "    mean_raw_obs_processing_ms: 0.0842045991001952\n",
      "  time_since_restore: 1856.9025733470917\n",
      "  time_this_iter_s: 8.544275999069214\n",
      "  time_total_s: 1856.9025733470917\n",
      "  timers:\n",
      "    learn_throughput: 669.684\n",
      "    learn_time_ms: 5972.965\n",
      "    load_throughput: 13570505.541\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 462.737\n",
      "    sample_time_ms: 8644.227\n",
      "    update_time_ms: 1.444\n",
      "  timestamp: 1643385574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 216\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:36 (running for 00:31:08.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">          1856.9</td><td style=\"text-align: right;\">864000</td><td style=\"text-align: right;\">-0.0748596</td><td style=\"text-align: right;\">             7.97479</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:41 (running for 00:31:13.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">          1856.9</td><td style=\"text-align: right;\">864000</td><td style=\"text-align: right;\">-0.0748596</td><td style=\"text-align: right;\">             7.97479</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 868000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-59-43\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.974793739616871\n",
      "  episode_reward_mean: 0.06560067564249039\n",
      "  episode_reward_min: -9.589561738073826\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.2886462211608887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01607802774126966\n",
      "          policy_loss: -0.09410848265503763\n",
      "          total_loss: 1.8595854974121497\n",
      "          vf_explained_var: 0.901223159605457\n",
      "          vf_loss: 1.9292754721096768\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 868000\n",
      "    num_agent_steps_trained: 868000\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.349999999999998\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10167345849200904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08901051555712663\n",
      "    mean_inference_ms: 1.0008857953092625\n",
      "    mean_raw_obs_processing_ms: 0.08420569512015814\n",
      "  time_since_restore: 1865.4751954078674\n",
      "  time_this_iter_s: 8.572622060775757\n",
      "  time_total_s: 1865.4751954078674\n",
      "  timers:\n",
      "    learn_throughput: 668.76\n",
      "    learn_time_ms: 5981.221\n",
      "    load_throughput: 13359783.405\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 463.341\n",
      "    sample_time_ms: 8632.946\n",
      "    update_time_ms: 1.443\n",
      "  timestamp: 1643385583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 217\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:47 (running for 00:31:19.06)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         1865.48</td><td style=\"text-align: right;\">868000</td><td style=\"text-align: right;\">0.0656007</td><td style=\"text-align: right;\">             7.97479</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_16-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.974793739616871\n",
      "  episode_reward_mean: 0.08014021836221218\n",
      "  episode_reward_min: -9.589561738073826\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.273364261145233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016663207519225707\n",
      "          policy_loss: -0.09620780778112471\n",
      "          total_loss: 1.7621170086105185\n",
      "          vf_explained_var: 0.934191180236878\n",
      "          vf_loss: 1.833017568915121\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.976923076923075\n",
      "    ram_util_percent: 34.33846153846153\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10167606579007854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08901612109462687\n",
      "    mean_inference_ms: 1.0008214701943128\n",
      "    mean_raw_obs_processing_ms: 0.08420315106399535\n",
      "  time_since_restore: 1874.0904605388641\n",
      "  time_this_iter_s: 8.615265130996704\n",
      "  time_total_s: 1874.0904605388641\n",
      "  timers:\n",
      "    learn_throughput: 667.742\n",
      "    learn_time_ms: 5990.334\n",
      "    load_throughput: 13448670.14\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 462.072\n",
      "    sample_time_ms: 8656.656\n",
      "    update_time_ms: 1.418\n",
      "  timestamp: 1643385592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 218\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:53 (running for 00:31:24.70)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         1874.09</td><td style=\"text-align: right;\">872000</td><td style=\"text-align: right;\">0.0801402</td><td style=\"text-align: right;\">             7.97479</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 16:59:58 (running for 00:31:29.70)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         1874.09</td><td style=\"text-align: right;\">872000</td><td style=\"text-align: right;\">0.0801402</td><td style=\"text-align: right;\">             7.97479</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 876000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-00-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.25796702504158\n",
      "  episode_reward_mean: 0.05153500616550446\n",
      "  episode_reward_min: -9.589561738073826\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.226359648345619\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015031552732665076\n",
      "          policy_loss: -0.09517979740455586\n",
      "          total_loss: 2.332182669057761\n",
      "          vf_explained_var: 0.9176014832271042\n",
      "          vf_loss: 2.4045332951491236\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 876000\n",
      "    num_agent_steps_trained: 876000\n",
      "    num_steps_sampled: 876000\n",
      "    num_steps_trained: 876000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.325\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10167681109832202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08902002217511218\n",
      "    mean_inference_ms: 1.0007373938788722\n",
      "    mean_raw_obs_processing_ms: 0.08419831891564247\n",
      "  time_since_restore: 1882.5985052585602\n",
      "  time_this_iter_s: 8.508044719696045\n",
      "  time_total_s: 1882.5985052585602\n",
      "  timers:\n",
      "    learn_throughput: 666.785\n",
      "    learn_time_ms: 5998.938\n",
      "    load_throughput: 13441128.024\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 462.483\n",
      "    sample_time_ms: 8648.97\n",
      "    update_time_ms: 1.434\n",
      "  timestamp: 1643385600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 876000\n",
      "  training_iteration: 219\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:03 (running for 00:31:35.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">          1882.6</td><td style=\"text-align: right;\">876000</td><td style=\"text-align: right;\">0.051535</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:08 (running for 00:31:40.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">          1882.6</td><td style=\"text-align: right;\">876000</td><td style=\"text-align: right;\">0.051535</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.25796702504158\n",
      "  episode_reward_mean: 0.07395346373319626\n",
      "  episode_reward_min: -9.589561738073826\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.1980910298644853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014770196740715365\n",
      "          policy_loss: -0.08938187067522116\n",
      "          total_loss: 1.7902973903833779\n",
      "          vf_explained_var: 0.9159958872102922\n",
      "          vf_loss: 1.8572470168993678\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_agent_steps_trained: 880000\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.7\n",
      "    ram_util_percent: 34.31666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10169149449486911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08903466576001134\n",
      "    mean_inference_ms: 1.000764493233501\n",
      "    mean_raw_obs_processing_ms: 0.08420318581573305\n",
      "  time_since_restore: 1891.4947350025177\n",
      "  time_this_iter_s: 8.89622974395752\n",
      "  time_total_s: 1891.4947350025177\n",
      "  timers:\n",
      "    learn_throughput: 664.051\n",
      "    learn_time_ms: 6023.637\n",
      "    load_throughput: 13440051.27\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 461.552\n",
      "    sample_time_ms: 8666.404\n",
      "    update_time_ms: 1.413\n",
      "  timestamp: 1643385609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 220\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:14 (running for 00:31:46.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         1891.49</td><td style=\"text-align: right;\">880000</td><td style=\"text-align: right;\">0.0739535</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">            -9.58956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 884000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-00-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.25796702504158\n",
      "  episode_reward_mean: 0.12247319877147675\n",
      "  episode_reward_min: -10.40096339583397\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.1230803025666103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016252562314553305\n",
      "          policy_loss: -0.08879698484495122\n",
      "          total_loss: 2.4471732576337635\n",
      "          vf_explained_var: 0.9016864832370511\n",
      "          vf_loss: 2.511286669873422\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 884000\n",
      "    num_agent_steps_trained: 884000\n",
      "    num_steps_sampled: 884000\n",
      "    num_steps_trained: 884000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.10769230769231\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10172137321854802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08906448311043652\n",
      "    mean_inference_ms: 1.0010056708286796\n",
      "    mean_raw_obs_processing_ms: 0.08422365897260217\n",
      "  time_since_restore: 1900.4167642593384\n",
      "  time_this_iter_s: 8.922029256820679\n",
      "  time_total_s: 1900.4167642593384\n",
      "  timers:\n",
      "    learn_throughput: 666.481\n",
      "    learn_time_ms: 6001.674\n",
      "    load_throughput: 12358906.814\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 456.347\n",
      "    sample_time_ms: 8765.252\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1643385618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 884000\n",
      "  training_iteration: 221\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:20 (running for 00:31:52.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         1900.42</td><td style=\"text-align: right;\">884000</td><td style=\"text-align: right;\">0.122473</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:25 (running for 00:31:57.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         1900.42</td><td style=\"text-align: right;\">884000</td><td style=\"text-align: right;\">0.122473</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.25796702504158\n",
      "  episode_reward_mean: -0.0689191085100174\n",
      "  episode_reward_min: -10.40096339583397\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.081258183653637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016437885955368783\n",
      "          policy_loss: -0.09903594804126568\n",
      "          total_loss: 1.3857871815001452\n",
      "          vf_explained_var: 0.9290307940334402\n",
      "          vf_loss: 1.459858082547303\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.683333333333334\n",
      "    ram_util_percent: 34.30833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10174686283945945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08908869082202373\n",
      "    mean_inference_ms: 1.0011950267347105\n",
      "    mean_raw_obs_processing_ms: 0.08424217403981388\n",
      "  time_since_restore: 1908.7313611507416\n",
      "  time_this_iter_s: 8.314596891403198\n",
      "  time_total_s: 1908.7313611507416\n",
      "  timers:\n",
      "    learn_throughput: 671.267\n",
      "    learn_time_ms: 5958.883\n",
      "    load_throughput: 12471911.983\n",
      "    load_time_ms: 0.321\n",
      "    sample_throughput: 459.336\n",
      "    sample_time_ms: 8708.228\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1643385626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 222\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:30 (running for 00:32:02.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         1908.73</td><td style=\"text-align: right;\">888000</td><td style=\"text-align: right;\">-0.0689191</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:35 (running for 00:32:07.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         1908.73</td><td style=\"text-align: right;\">888000</td><td style=\"text-align: right;\">-0.0689191</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 892000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-00-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.25796702504158\n",
      "  episode_reward_mean: 0.002964763045310974\n",
      "  episode_reward_min: -10.40096339583397\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.040260789471288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015808348538231674\n",
      "          policy_loss: -0.10100743412490815\n",
      "          total_loss: 2.7412091301153265\n",
      "          vf_explained_var: 0.888452894905562\n",
      "          vf_loss: 2.8182076485086514\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 892000\n",
      "    num_agent_steps_trained: 892000\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 892000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.907692307692304\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10179403527261757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08913212847786271\n",
      "    mean_inference_ms: 1.0015819066901361\n",
      "    mean_raw_obs_processing_ms: 0.08427539381875328\n",
      "  time_since_restore: 1917.7453610897064\n",
      "  time_this_iter_s: 9.013999938964844\n",
      "  time_total_s: 1917.7453610897064\n",
      "  timers:\n",
      "    learn_throughput: 674.905\n",
      "    learn_time_ms: 5926.756\n",
      "    load_throughput: 12445082.709\n",
      "    load_time_ms: 0.321\n",
      "    sample_throughput: 459.563\n",
      "    sample_time_ms: 8703.915\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1643385635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 223\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:40 (running for 00:32:12.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         1917.75</td><td style=\"text-align: right;\">892000</td><td style=\"text-align: right;\">0.00296476</td><td style=\"text-align: right;\">              10.258</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.578680008649826\n",
      "  episode_reward_mean: 0.01978636011481285\n",
      "  episode_reward_min: -10.40096339583397\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.0301961086129627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015000080810819992\n",
      "          policy_loss: -0.07837051758392444\n",
      "          total_loss: 1.9566001769403116\n",
      "          vf_explained_var: 0.9098723774956119\n",
      "          vf_loss: 2.012189325089416\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.381818181818183\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10183450048412558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0891695749528829\n",
      "    mean_inference_ms: 1.001900496387649\n",
      "    mean_raw_obs_processing_ms: 0.08430391233717266\n",
      "  time_since_restore: 1925.7660841941833\n",
      "  time_this_iter_s: 8.020723104476929\n",
      "  time_total_s: 1925.7660841941833\n",
      "  timers:\n",
      "    learn_throughput: 677.608\n",
      "    learn_time_ms: 5903.116\n",
      "    load_throughput: 12512840.095\n",
      "    load_time_ms: 0.32\n",
      "    sample_throughput: 462.625\n",
      "    sample_time_ms: 8646.316\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1643385643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 224\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:45 (running for 00:32:17.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         1925.77</td><td style=\"text-align: right;\">896000</td><td style=\"text-align: right;\">0.0197864</td><td style=\"text-align: right;\">             8.57868</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:50 (running for 00:32:22.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         1925.77</td><td style=\"text-align: right;\">896000</td><td style=\"text-align: right;\">0.0197864</td><td style=\"text-align: right;\">             8.57868</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-00-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.578680008649826\n",
      "  episode_reward_mean: -0.03884396836161613\n",
      "  episode_reward_min: -10.40096339583397\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.982706530119783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015846230431651055\n",
      "          policy_loss: -0.09873128790278188\n",
      "          total_loss: 1.5029164466053067\n",
      "          vf_explained_var: 0.9226986219806056\n",
      "          vf_loss: 1.577581272386415\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_agent_steps_trained: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.407692307692308\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1018705054835964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919814671203763\n",
      "    mean_inference_ms: 1.0021553732159403\n",
      "    mean_raw_obs_processing_ms: 0.08432519115950247\n",
      "  time_since_restore: 1934.3046300411224\n",
      "  time_this_iter_s: 8.538545846939087\n",
      "  time_total_s: 1934.3046300411224\n",
      "  timers:\n",
      "    learn_throughput: 679.755\n",
      "    learn_time_ms: 5884.469\n",
      "    load_throughput: 12496064.353\n",
      "    load_time_ms: 0.32\n",
      "    sample_throughput: 463.142\n",
      "    sample_time_ms: 8636.669\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1643385652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 225\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:00:56 (running for 00:32:28.08)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">          1934.3</td><td style=\"text-align: right;\">900000</td><td style=\"text-align: right;\">-0.038844</td><td style=\"text-align: right;\">             8.57868</td><td style=\"text-align: right;\">             -10.401</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-01-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.578680008649826\n",
      "  episode_reward_mean: -0.040307197570800785\n",
      "  episode_reward_min: -10.327819049358368\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.956019898383848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01513990953192956\n",
      "          policy_loss: -0.09037113951228719\n",
      "          total_loss: 1.8713375889986594\n",
      "          vf_explained_var: 0.9197008877672175\n",
      "          vf_loss: 1.9387150008832255\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.516666666666666\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1018757261847674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08920453866569229\n",
      "    mean_inference_ms: 1.002147411256346\n",
      "    mean_raw_obs_processing_ms: 0.08432496213257445\n",
      "  time_since_restore: 1942.7634406089783\n",
      "  time_this_iter_s: 8.458810567855835\n",
      "  time_total_s: 1942.7634406089783\n",
      "  timers:\n",
      "    learn_throughput: 678.797\n",
      "    learn_time_ms: 5892.781\n",
      "    load_throughput: 12507243.179\n",
      "    load_time_ms: 0.32\n",
      "    sample_throughput: 465.07\n",
      "    sample_time_ms: 8600.854\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1643385660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 226\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:01 (running for 00:32:33.57)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         1942.76</td><td style=\"text-align: right;\">904000</td><td style=\"text-align: right;\">-0.0403072</td><td style=\"text-align: right;\">             8.57868</td><td style=\"text-align: right;\">            -10.3278</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:06 (running for 00:32:38.58)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         1942.76</td><td style=\"text-align: right;\">904000</td><td style=\"text-align: right;\">-0.0403072</td><td style=\"text-align: right;\">             8.57868</td><td style=\"text-align: right;\">            -10.3278</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 908000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.578680008649826\n",
      "  episode_reward_mean: -0.0062540282309055326\n",
      "  episode_reward_min: -10.327819049358368\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.9237873702920893\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01517451744259462\n",
      "          policy_loss: -0.10116340755710318\n",
      "          total_loss: 1.4803548507066944\n",
      "          vf_explained_var: 0.9293552501868176\n",
      "          vf_loss: 1.5584719693468463\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 908000\n",
      "    num_agent_steps_trained: 908000\n",
      "    num_steps_sampled: 908000\n",
      "    num_steps_trained: 908000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.61666666666667\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10189306110770621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08922192988431639\n",
      "    mean_inference_ms: 1.0022603709679203\n",
      "    mean_raw_obs_processing_ms: 0.08433249696847848\n",
      "  time_since_restore: 1951.535305738449\n",
      "  time_this_iter_s: 8.771865129470825\n",
      "  time_total_s: 1951.535305738449\n",
      "  timers:\n",
      "    learn_throughput: 679.045\n",
      "    learn_time_ms: 5890.627\n",
      "    load_throughput: 12712901.417\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 463.38\n",
      "    sample_time_ms: 8632.219\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1643385669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 908000\n",
      "  training_iteration: 227\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:12 (running for 00:32:44.37)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         1951.54</td><td style=\"text-align: right;\">908000</td><td style=\"text-align: right;\">-0.00625403</td><td style=\"text-align: right;\">             8.57868</td><td style=\"text-align: right;\">            -10.3278</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:17 (running for 00:32:49.37)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         1951.54</td><td style=\"text-align: right;\">908000</td><td style=\"text-align: right;\">-0.00625403</td><td style=\"text-align: right;\">             8.57868</td><td style=\"text-align: right;\">            -10.3278</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 912000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-01-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.457637578248978\n",
      "  episode_reward_mean: 0.036482839435338976\n",
      "  episode_reward_min: -8.997089505195618\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8912138603066886\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015501461862333801\n",
      "          policy_loss: -0.06869183061694506\n",
      "          total_loss: 2.1992951906470193\n",
      "          vf_explained_var: 0.9074968220085227\n",
      "          vf_loss: 2.2444441675579037\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 912000\n",
      "    num_agent_steps_trained: 912000\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.984615384615385\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188480206927959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.089214495602577\n",
      "    mean_inference_ms: 1.0021128108423483\n",
      "    mean_raw_obs_processing_ms: 0.0843193550612705\n",
      "  time_since_restore: 1960.0797247886658\n",
      "  time_this_iter_s: 8.544419050216675\n",
      "  time_total_s: 1960.0797247886658\n",
      "  timers:\n",
      "    learn_throughput: 677.585\n",
      "    learn_time_ms: 5903.315\n",
      "    load_throughput: 12679274.486\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 464.555\n",
      "    sample_time_ms: 8610.396\n",
      "    update_time_ms: 1.463\n",
      "  timestamp: 1643385678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 228\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:23 (running for 00:32:54.93)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         1960.08</td><td style=\"text-align: right;\">912000</td><td style=\"text-align: right;\">0.0364828</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -8.99709</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 916000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.457637578248978\n",
      "  episode_reward_mean: -0.1395193900167942\n",
      "  episode_reward_min: -9.282794505357742\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.916110248463128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013379691508524432\n",
      "          policy_loss: -0.11828778114330063\n",
      "          total_loss: 2.128735584139283\n",
      "          vf_explained_var: 0.9225246426238809\n",
      "          vf_loss: 2.226702965531618\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 916000\n",
      "    num_agent_steps_trained: 916000\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.366666666666667\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188456808551666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08921417226461077\n",
      "    mean_inference_ms: 1.002056735753575\n",
      "    mean_raw_obs_processing_ms: 0.08431485652725486\n",
      "  time_since_restore: 1968.724228143692\n",
      "  time_this_iter_s: 8.644503355026245\n",
      "  time_total_s: 1968.724228143692\n",
      "  timers:\n",
      "    learn_throughput: 676.794\n",
      "    learn_time_ms: 5910.222\n",
      "    load_throughput: 12702313.749\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 463.498\n",
      "    sample_time_ms: 8630.033\n",
      "    update_time_ms: 1.428\n",
      "  timestamp: 1643385686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 229\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:28 (running for 00:33:00.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         1968.72</td><td style=\"text-align: right;\">916000</td><td style=\"text-align: right;\">-0.139519</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -9.28279</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:33 (running for 00:33:05.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         1968.72</td><td style=\"text-align: right;\">916000</td><td style=\"text-align: right;\">-0.139519</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -9.28279</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-01-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.457637578248978\n",
      "  episode_reward_mean: -0.05021069630980492\n",
      "  episode_reward_min: -9.282794505357742\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8020052899596513\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014990006906565932\n",
      "          policy_loss: -0.09833349087325635\n",
      "          total_loss: 1.46108878629197\n",
      "          vf_explained_var: 0.9363992768590168\n",
      "          vf_loss: 1.5366561983541775\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.96923076923077\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1018852759035787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08921945328844513\n",
      "    mean_inference_ms: 1.0020619033951816\n",
      "    mean_raw_obs_processing_ms: 0.08431785184751096\n",
      "  time_since_restore: 1977.6310834884644\n",
      "  time_this_iter_s: 8.906855344772339\n",
      "  time_total_s: 1977.6310834884644\n",
      "  timers:\n",
      "    learn_throughput: 676.591\n",
      "    learn_time_ms: 5911.992\n",
      "    load_throughput: 12648685.163\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 463.191\n",
      "    sample_time_ms: 8635.747\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1643385695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 230\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:39 (running for 00:33:11.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         1977.63</td><td style=\"text-align: right;\">920000</td><td style=\"text-align: right;\">-0.0502107</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -9.28279</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 924000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-01-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.457637578248978\n",
      "  episode_reward_mean: 0.016550291776657105\n",
      "  episode_reward_min: -9.282794505357742\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7641590769572923\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014806396488172514\n",
      "          policy_loss: -0.10287818035851884\n",
      "          total_loss: 1.6147180962624148\n",
      "          vf_explained_var: 0.9240357664964532\n",
      "          vf_loss: 1.69510906206504\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 924000\n",
      "    num_agent_steps_trained: 924000\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.216666666666665\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10189430162071644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923145312603169\n",
      "    mean_inference_ms: 1.0021237935919372\n",
      "    mean_raw_obs_processing_ms: 0.08432440102143296\n",
      "  time_since_restore: 1986.1040453910828\n",
      "  time_this_iter_s: 8.472961902618408\n",
      "  time_total_s: 1986.1040453910828\n",
      "  timers:\n",
      "    learn_throughput: 674.465\n",
      "    learn_time_ms: 5930.627\n",
      "    load_throughput: 13494101.182\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 466.534\n",
      "    sample_time_ms: 8573.872\n",
      "    update_time_ms: 1.475\n",
      "  timestamp: 1643385704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 231\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:45 (running for 00:33:17.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">          1986.1</td><td style=\"text-align: right;\">924000</td><td style=\"text-align: right;\">0.0165503</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -9.28279</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:50 (running for 00:33:22.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">          1986.1</td><td style=\"text-align: right;\">924000</td><td style=\"text-align: right;\">0.0165503</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -9.28279</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 928000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-01-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.457637578248978\n",
      "  episode_reward_mean: 0.03856108024716377\n",
      "  episode_reward_min: -9.667256832122803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.826804760963686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01455856403339138\n",
      "          policy_loss: -0.07225419131297899\n",
      "          total_loss: 1.1753988945606335\n",
      "          vf_explained_var: 0.9472896976496583\n",
      "          vf_loss: 1.2255422636305773\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 928000\n",
      "    num_agent_steps_trained: 928000\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.499999999999996\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188815521150697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923103791615389\n",
      "    mean_inference_ms: 1.002054079310107\n",
      "    mean_raw_obs_processing_ms: 0.08432162490913359\n",
      "  time_since_restore: 1994.555569410324\n",
      "  time_this_iter_s: 8.451524019241333\n",
      "  time_total_s: 1994.555569410324\n",
      "  timers:\n",
      "    learn_throughput: 672.418\n",
      "    learn_time_ms: 5948.682\n",
      "    load_throughput: 13463779.793\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 465.742\n",
      "    sample_time_ms: 8588.445\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1643385712\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 232\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:01:55 (running for 00:33:27.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         1994.56</td><td style=\"text-align: right;\">928000</td><td style=\"text-align: right;\">0.0385611</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:00 (running for 00:33:32.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         1994.56</td><td style=\"text-align: right;\">928000</td><td style=\"text-align: right;\">0.0385611</td><td style=\"text-align: right;\">             10.4576</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 932000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.141925603151321\n",
      "  episode_reward_mean: -0.08766913667321205\n",
      "  episode_reward_min: -9.667256832122803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8059796164112707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01623347129454418\n",
      "          policy_loss: -0.09923058745729166\n",
      "          total_loss: 1.624466468236329\n",
      "          vf_explained_var: 0.9181530562780237\n",
      "          vf_loss: 1.6990424749351316\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 932000\n",
      "    num_agent_steps_trained: 932000\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.858333333333334\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188451939531218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923105355066543\n",
      "    mean_inference_ms: 1.0020118526144417\n",
      "    mean_raw_obs_processing_ms: 0.08431968728151266\n",
      "  time_since_restore: 2003.2902274131775\n",
      "  time_this_iter_s: 8.734658002853394\n",
      "  time_total_s: 2003.2902274131775\n",
      "  timers:\n",
      "    learn_throughput: 667.328\n",
      "    learn_time_ms: 5994.055\n",
      "    load_throughput: 13460539.153\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 468.81\n",
      "    sample_time_ms: 8532.248\n",
      "    update_time_ms: 1.5\n",
      "  timestamp: 1643385721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 233\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:06 (running for 00:33:38.26)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         2003.29</td><td style=\"text-align: right;\">932000</td><td style=\"text-align: right;\">-0.0876691</td><td style=\"text-align: right;\">             9.14193</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-02-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.010782837867737\n",
      "  episode_reward_mean: 0.11217072248458862\n",
      "  episode_reward_min: -9.667256832122803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.777698446858314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015582793986085254\n",
      "          policy_loss: -0.10971341407907906\n",
      "          total_loss: 2.175253761926746\n",
      "          vf_explained_var: 0.9063444092068621\n",
      "          vf_loss: 2.2613008052591357\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.5\n",
      "    ram_util_percent: 34.300000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188309690153766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923333866102422\n",
      "    mean_inference_ms: 1.0019922074090344\n",
      "    mean_raw_obs_processing_ms: 0.08431950761155203\n",
      "  time_since_restore: 2012.0215640068054\n",
      "  time_this_iter_s: 8.73133659362793\n",
      "  time_total_s: 2012.0215640068054\n",
      "  timers:\n",
      "    learn_throughput: 663.052\n",
      "    learn_time_ms: 6032.713\n",
      "    load_throughput: 13100035.918\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 464.573\n",
      "    sample_time_ms: 8610.052\n",
      "    update_time_ms: 1.524\n",
      "  timestamp: 1643385730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 234\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:12 (running for 00:33:44.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         2012.02</td><td style=\"text-align: right;\">936000</td><td style=\"text-align: right;\">0.112171</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:17 (running for 00:33:49.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         2012.02</td><td style=\"text-align: right;\">936000</td><td style=\"text-align: right;\">0.112171</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 940000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-02-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.010782837867737\n",
      "  episode_reward_mean: 0.048771647214889524\n",
      "  episode_reward_min: -9.667256832122803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7479753494262695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01690652597156165\n",
      "          policy_loss: -0.11241841927491208\n",
      "          total_loss: 2.0191052006552574\n",
      "          vf_explained_var: 0.9225010986610126\n",
      "          vf_loss: 2.1058468479382735\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 940000\n",
      "    num_agent_steps_trained: 940000\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.124999999999996\n",
      "    ram_util_percent: 34.24999999999999\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10187375419255076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923006859021403\n",
      "    mean_inference_ms: 1.0019073113085504\n",
      "    mean_raw_obs_processing_ms: 0.08431317552697802\n",
      "  time_since_restore: 2020.5137875080109\n",
      "  time_this_iter_s: 8.492223501205444\n",
      "  time_total_s: 2020.5137875080109\n",
      "  timers:\n",
      "    learn_throughput: 663.202\n",
      "    learn_time_ms: 6031.344\n",
      "    load_throughput: 12919464.038\n",
      "    load_time_ms: 0.31\n",
      "    sample_throughput: 462.646\n",
      "    sample_time_ms: 8645.922\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1643385738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 235\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:22 (running for 00:33:54.53)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         2020.51</td><td style=\"text-align: right;\">940000</td><td style=\"text-align: right;\">0.0487716</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 944000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-02-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.010782837867737\n",
      "  episode_reward_mean: 0.06908644914627075\n",
      "  episode_reward_min: -9.667256832122803\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7296503700235837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01579494905385585\n",
      "          policy_loss: -0.0735481739885384\n",
      "          total_loss: 1.7367891930684607\n",
      "          vf_explained_var: 0.9194289841959553\n",
      "          vf_loss: 1.7863487882479545\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 944000\n",
      "    num_agent_steps_trained: 944000\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.73333333333333\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10186877949928735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0892283617023458\n",
      "    mean_inference_ms: 1.0018469864174733\n",
      "    mean_raw_obs_processing_ms: 0.08430965790964023\n",
      "  time_since_restore: 2028.9691092967987\n",
      "  time_this_iter_s: 8.455321788787842\n",
      "  time_total_s: 2028.9691092967987\n",
      "  timers:\n",
      "    learn_throughput: 665.995\n",
      "    learn_time_ms: 6006.05\n",
      "    load_throughput: 12812903.62\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 461.368\n",
      "    sample_time_ms: 8669.878\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1643385747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 236\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:28 (running for 00:34:00.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         2028.97</td><td style=\"text-align: right;\">944000</td><td style=\"text-align: right;\">0.0690864</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:33 (running for 00:34:05.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         2028.97</td><td style=\"text-align: right;\">944000</td><td style=\"text-align: right;\">0.0690864</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">            -9.66726</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 948000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.010782837867737\n",
      "  episode_reward_mean: -0.10072674930095672\n",
      "  episode_reward_min: -8.58890488743782\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7120082227132656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016167535881890807\n",
      "          policy_loss: -0.10866912737068149\n",
      "          total_loss: 1.1692955261869957\n",
      "          vf_explained_var: 0.9385696470096547\n",
      "          vf_loss: 1.2534102091786041\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 948000\n",
      "    num_agent_steps_trained: 948000\n",
      "    num_steps_sampled: 948000\n",
      "    num_steps_trained: 948000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.916666666666668\n",
      "    ram_util_percent: 34.19166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1018691115722368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08922874747174099\n",
      "    mean_inference_ms: 1.0018268194145774\n",
      "    mean_raw_obs_processing_ms: 0.08430787494301634\n",
      "  time_since_restore: 2037.4052274227142\n",
      "  time_this_iter_s: 8.436118125915527\n",
      "  time_total_s: 2037.4052274227142\n",
      "  timers:\n",
      "    learn_throughput: 666.592\n",
      "    learn_time_ms: 6000.668\n",
      "    load_throughput: 12352537.182\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 464.29\n",
      "    sample_time_ms: 8615.313\n",
      "    update_time_ms: 1.506\n",
      "  timestamp: 1643385755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 948000\n",
      "  training_iteration: 237\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:38 (running for 00:34:10.46)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         2037.41</td><td style=\"text-align: right;\">948000</td><td style=\"text-align: right;\">-0.100727</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">             -8.5889</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:43 (running for 00:34:15.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         2037.41</td><td style=\"text-align: right;\">948000</td><td style=\"text-align: right;\">-0.100727</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">             -8.5889</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-02-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.010782837867737\n",
      "  episode_reward_mean: 0.04600032389163971\n",
      "  episode_reward_min: -8.828444212675095\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7005583329867293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015955839871544916\n",
      "          policy_loss: -0.11797790495548598\n",
      "          total_loss: 2.901907431775133\n",
      "          vf_explained_var: 0.8667298527174099\n",
      "          vf_loss: 2.995652396432174\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.283333333333335\n",
      "    ram_util_percent: 34.13333333333335\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10187120009113874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08922842255331669\n",
      "    mean_inference_ms: 1.0018018048171873\n",
      "    mean_raw_obs_processing_ms: 0.08430875457559722\n",
      "  time_since_restore: 2045.6734056472778\n",
      "  time_this_iter_s: 8.268178224563599\n",
      "  time_total_s: 2045.6734056472778\n",
      "  timers:\n",
      "    learn_throughput: 670.529\n",
      "    learn_time_ms: 5965.436\n",
      "    load_throughput: 12303619.83\n",
      "    load_time_ms: 0.325\n",
      "    sample_throughput: 464.187\n",
      "    sample_time_ms: 8617.208\n",
      "    update_time_ms: 1.439\n",
      "  timestamp: 1643385764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 238\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:49 (running for 00:34:20.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         2045.67</td><td style=\"text-align: right;\">952000</td><td style=\"text-align: right;\">0.0460003</td><td style=\"text-align: right;\">             10.0108</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 956000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.602211996912956\n",
      "  episode_reward_mean: 0.037433716356754305\n",
      "  episode_reward_min: -8.828444212675095\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6269555712258943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016555931927325518\n",
      "          policy_loss: -0.09644408320146863\n",
      "          total_loss: 1.713694542141441\n",
      "          vf_explained_var: 0.9105224594634066\n",
      "          vf_loss: 1.7849943069120249\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 956000\n",
      "    num_agent_steps_trained: 956000\n",
      "    num_steps_sampled: 956000\n",
      "    num_steps_trained: 956000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.458333333333332\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10187199285418243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08922284431170137\n",
      "    mean_inference_ms: 1.0017199140175466\n",
      "    mean_raw_obs_processing_ms: 0.08430521868325135\n",
      "  time_since_restore: 2053.838912010193\n",
      "  time_this_iter_s: 8.165506362915039\n",
      "  time_total_s: 2053.838912010193\n",
      "  timers:\n",
      "    learn_throughput: 674.907\n",
      "    learn_time_ms: 5926.742\n",
      "    load_throughput: 12339817.593\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 466.614\n",
      "    sample_time_ms: 8572.389\n",
      "    update_time_ms: 1.46\n",
      "  timestamp: 1643385772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 956000\n",
      "  training_iteration: 239\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:54 (running for 00:34:25.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         2053.84</td><td style=\"text-align: right;\">956000</td><td style=\"text-align: right;\">0.0374337</td><td style=\"text-align: right;\">             8.60221</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:02:59 (running for 00:34:30.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         2053.84</td><td style=\"text-align: right;\">956000</td><td style=\"text-align: right;\">0.0374337</td><td style=\"text-align: right;\">             8.60221</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.602211996912956\n",
      "  episode_reward_mean: -0.026897899508476257\n",
      "  episode_reward_min: -8.828444212675095\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6090029406291184\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016017812931152637\n",
      "          policy_loss: -0.10563800440079743\n",
      "          total_loss: 1.1204053687612696\n",
      "          vf_explained_var: 0.9299830005374006\n",
      "          vf_loss: 1.2017163195437002\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_agent_steps_trained: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.86666666666667\n",
      "    ram_util_percent: 34.15833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10187366030378087\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08921693585345342\n",
      "    mean_inference_ms: 1.0016256931380425\n",
      "    mean_raw_obs_processing_ms: 0.08430136851787813\n",
      "  time_since_restore: 2062.293002128601\n",
      "  time_this_iter_s: 8.454090118408203\n",
      "  time_total_s: 2062.293002128601\n",
      "  timers:\n",
      "    learn_throughput: 677.544\n",
      "    learn_time_ms: 5903.674\n",
      "    load_throughput: 12390853.767\n",
      "    load_time_ms: 0.323\n",
      "    sample_throughput: 469.941\n",
      "    sample_time_ms: 8511.699\n",
      "    update_time_ms: 1.458\n",
      "  timestamp: 1643385780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 240\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:04 (running for 00:34:36.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         2062.29</td><td style=\"text-align: right;\">960000</td><td style=\"text-align: right;\">-0.0268979</td><td style=\"text-align: right;\">             8.60221</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 964000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.602211996912956\n",
      "  episode_reward_mean: -0.03757261887192726\n",
      "  episode_reward_min: -8.828444212675095\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5781895058129423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016686946503779554\n",
      "          policy_loss: -0.0855181039280949\n",
      "          total_loss: 2.111557073612267\n",
      "          vf_explained_var: 0.8973631248038302\n",
      "          vf_loss: 2.171731876469748\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 964000\n",
      "    num_agent_steps_trained: 964000\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 964000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.633333333333336\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10186875817841204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08920643894792707\n",
      "    mean_inference_ms: 1.0015050411106312\n",
      "    mean_raw_obs_processing_ms: 0.08429876939341399\n",
      "  time_since_restore: 2070.7786536216736\n",
      "  time_this_iter_s: 8.48565149307251\n",
      "  time_total_s: 2070.7786536216736\n",
      "  timers:\n",
      "    learn_throughput: 677.343\n",
      "    learn_time_ms: 5905.424\n",
      "    load_throughput: 12742834.574\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 471.218\n",
      "    sample_time_ms: 8488.634\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1643385789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 241\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:10 (running for 00:34:41.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         2070.78</td><td style=\"text-align: right;\">964000</td><td style=\"text-align: right;\">-0.0375726</td><td style=\"text-align: right;\">             8.60221</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:15 (running for 00:34:46.93)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         2070.78</td><td style=\"text-align: right;\">964000</td><td style=\"text-align: right;\">-0.0375726</td><td style=\"text-align: right;\">             8.60221</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.269329622387886\n",
      "  episode_reward_mean: 0.11367586970329285\n",
      "  episode_reward_min: -8.828444212675095\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6744724450572845\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015640558451803104\n",
      "          policy_loss: -0.08928634115514816\n",
      "          total_loss: 2.442697542119441\n",
      "          vf_explained_var: 0.8922154450288383\n",
      "          vf_loss: 2.508229804311388\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.599999999999998\n",
      "    ram_util_percent: 34.154545454545456\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10185546935893923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919030625483038\n",
      "    mean_inference_ms: 1.0013100657848533\n",
      "    mean_raw_obs_processing_ms: 0.0842908735476794\n",
      "  time_since_restore: 2078.8629105091095\n",
      "  time_this_iter_s: 8.084256887435913\n",
      "  time_total_s: 2078.8629105091095\n",
      "  timers:\n",
      "    learn_throughput: 680.227\n",
      "    learn_time_ms: 5880.388\n",
      "    load_throughput: 12642966.089\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 471.768\n",
      "    sample_time_ms: 8478.735\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1643385797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 242\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:20 (running for 00:34:52.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         2078.86</td><td style=\"text-align: right;\">968000</td><td style=\"text-align: right;\">0.113676</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:25 (running for 00:34:57.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         2078.86</td><td style=\"text-align: right;\">968000</td><td style=\"text-align: right;\">0.113676</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -8.82844</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 972000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.269329622387886\n",
      "  episode_reward_mean: -0.05805252373218536\n",
      "  episode_reward_min: -10.387513548135757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.609685650692191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0161642519172738\n",
      "          policy_loss: -0.105487792690595\n",
      "          total_loss: 3.1310326218084303\n",
      "          vf_explained_var: 0.8863461202190769\n",
      "          vf_loss: 3.2119709561829284\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 972000\n",
      "    num_agent_steps_trained: 972000\n",
      "    num_steps_sampled: 972000\n",
      "    num_steps_trained: 972000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.95\n",
      "    ram_util_percent: 34.175000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10183494490443472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08917177897685682\n",
      "    mean_inference_ms: 1.0010642007721897\n",
      "    mean_raw_obs_processing_ms: 0.08427640239668106\n",
      "  time_since_restore: 2087.0538136959076\n",
      "  time_this_iter_s: 8.190903186798096\n",
      "  time_total_s: 2087.0538136959076\n",
      "  timers:\n",
      "    learn_throughput: 684.537\n",
      "    learn_time_ms: 5843.362\n",
      "    load_throughput: 12756399.027\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 474.124\n",
      "    sample_time_ms: 8436.614\n",
      "    update_time_ms: 1.488\n",
      "  timestamp: 1643385805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 972000\n",
      "  training_iteration: 243\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:30 (running for 00:35:02.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         2087.05</td><td style=\"text-align: right;\">972000</td><td style=\"text-align: right;\">-0.0580525</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 976000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.269329622387886\n",
      "  episode_reward_mean: -0.082488372027874\n",
      "  episode_reward_min: -10.387513548135757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.725985876719157\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015230923985828041\n",
      "          policy_loss: -0.12288723898681021\n",
      "          total_loss: 1.2950834282779045\n",
      "          vf_explained_var: 0.9232092354246365\n",
      "          vf_loss: 1.394838700163108\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 976000\n",
      "    num_agent_steps_trained: 976000\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.1\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1018171254704277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08915320203282445\n",
      "    mean_inference_ms: 1.0008220264727146\n",
      "    mean_raw_obs_processing_ms: 0.08425990394342607\n",
      "  time_since_restore: 2095.290569782257\n",
      "  time_this_iter_s: 8.236756086349487\n",
      "  time_total_s: 2095.290569782257\n",
      "  timers:\n",
      "    learn_throughput: 688.469\n",
      "    learn_time_ms: 5809.995\n",
      "    load_throughput: 12983451.478\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 477.139\n",
      "    sample_time_ms: 8383.296\n",
      "    update_time_ms: 1.452\n",
      "  timestamp: 1643385813\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 244\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:35 (running for 00:35:07.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         2095.29</td><td style=\"text-align: right;\">976000</td><td style=\"text-align: right;\">-0.0824884</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:40 (running for 00:35:12.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         2095.29</td><td style=\"text-align: right;\">976000</td><td style=\"text-align: right;\">-0.0824884</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.269329622387886\n",
      "  episode_reward_mean: 0.02300113320350647\n",
      "  episode_reward_min: -10.387513548135757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7452042920615085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015506521104685676\n",
      "          policy_loss: -0.09369753484604179\n",
      "          total_loss: 3.2681892364791603\n",
      "          vf_explained_var: 0.8749176559268788\n",
      "          vf_loss: 3.3383362281707027\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 980000\n",
      "    num_agent_steps_trained: 980000\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.566666666666666\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10179570238281781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08913328735700307\n",
      "    mean_inference_ms: 1.0005592323340138\n",
      "    mean_raw_obs_processing_ms: 0.08424031335489932\n",
      "  time_since_restore: 2103.6271023750305\n",
      "  time_this_iter_s: 8.336532592773438\n",
      "  time_total_s: 2103.6271023750305\n",
      "  timers:\n",
      "    learn_throughput: 689.002\n",
      "    learn_time_ms: 5805.5\n",
      "    load_throughput: 13153442.572\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 479.692\n",
      "    sample_time_ms: 8338.688\n",
      "    update_time_ms: 1.433\n",
      "  timestamp: 1643385822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 245\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:46 (running for 00:35:17.86)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         2103.63</td><td style=\"text-align: right;\">980000</td><td style=\"text-align: right;\">0.0230011</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.269329622387886\n",
      "  episode_reward_mean: -0.010954512506723404\n",
      "  episode_reward_min: -10.387513548135757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.646577312100318\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016283561292722466\n",
      "          policy_loss: -0.1123839510504597\n",
      "          total_loss: 1.6044543439817804\n",
      "          vf_explained_var: 0.9178565776476296\n",
      "          vf_loss: 1.692107642057442\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.041666666666668\n",
      "    ram_util_percent: 34.13333333333335\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10177746499072177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08911770150653835\n",
      "    mean_inference_ms: 1.0003203642443603\n",
      "    mean_raw_obs_processing_ms: 0.08421938260738251\n",
      "  time_since_restore: 2112.229291200638\n",
      "  time_this_iter_s: 8.6021888256073\n",
      "  time_total_s: 2112.229291200638\n",
      "  timers:\n",
      "    learn_throughput: 687.085\n",
      "    learn_time_ms: 5821.698\n",
      "    load_throughput: 13013664.288\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 480.051\n",
      "    sample_time_ms: 8332.452\n",
      "    update_time_ms: 1.42\n",
      "  timestamp: 1643385830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 246\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:51 (running for 00:35:23.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         2112.23</td><td style=\"text-align: right;\">984000</td><td style=\"text-align: right;\">-0.0109545</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:03:56 (running for 00:35:28.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         2112.23</td><td style=\"text-align: right;\">984000</td><td style=\"text-align: right;\">-0.0109545</td><td style=\"text-align: right;\">             12.2693</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 988000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-03-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.036246657371521\n",
      "  episode_reward_mean: -0.05986159473657608\n",
      "  episode_reward_min: -10.387513548135757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6178561790015107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016274507651331763\n",
      "          policy_loss: -0.10201611497629715\n",
      "          total_loss: 2.017452381035909\n",
      "          vf_explained_var: 0.9116489002781529\n",
      "          vf_loss: 2.0947515879186893\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 988000\n",
      "    num_agent_steps_trained: 988000\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 988000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.45\n",
      "    ram_util_percent: 34.15833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1017650482056943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0891074567792153\n",
      "    mean_inference_ms: 1.000138396515607\n",
      "    mean_raw_obs_processing_ms: 0.08420333612123233\n",
      "  time_since_restore: 2120.7790410518646\n",
      "  time_this_iter_s: 8.549749851226807\n",
      "  time_total_s: 2120.7790410518646\n",
      "  timers:\n",
      "    learn_throughput: 684.988\n",
      "    learn_time_ms: 5839.519\n",
      "    load_throughput: 13449748.276\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 479.497\n",
      "    sample_time_ms: 8342.07\n",
      "    update_time_ms: 1.398\n",
      "  timestamp: 1643385839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 247\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:02 (running for 00:35:34.06)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         2120.78</td><td style=\"text-align: right;\">988000</td><td style=\"text-align: right;\">-0.0598616</td><td style=\"text-align: right;\">             10.0362</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:07 (running for 00:35:39.07)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         2120.78</td><td style=\"text-align: right;\">988000</td><td style=\"text-align: right;\">-0.0598616</td><td style=\"text-align: right;\">             10.0362</td><td style=\"text-align: right;\">            -10.3875</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 992000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-04-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.036246657371521\n",
      "  episode_reward_mean: 0.06688511967658997\n",
      "  episode_reward_min: -8.029140010476112\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7061261600063693\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016400211029286356\n",
      "          policy_loss: -0.10128243314803287\n",
      "          total_loss: 1.5193164547009053\n",
      "          vf_explained_var: 0.9233962028898218\n",
      "          vf_loss: 1.5956910743428174\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 992000\n",
      "    num_agent_steps_trained: 992000\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.661538461538463\n",
      "    ram_util_percent: 34.16153846153846\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1017593291863311\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08910269794488646\n",
      "    mean_inference_ms: 1.000023273908514\n",
      "    mean_raw_obs_processing_ms: 0.08419338074751703\n",
      "  time_since_restore: 2129.448835372925\n",
      "  time_this_iter_s: 8.66979432106018\n",
      "  time_total_s: 2129.448835372925\n",
      "  timers:\n",
      "    learn_throughput: 680.767\n",
      "    learn_time_ms: 5875.725\n",
      "    load_throughput: 13524559.452\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 478.242\n",
      "    sample_time_ms: 8363.966\n",
      "    update_time_ms: 1.455\n",
      "  timestamp: 1643385848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 248\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:13 (running for 00:35:44.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         2129.45</td><td style=\"text-align: right;\">992000</td><td style=\"text-align: right;\">0.0668851</td><td style=\"text-align: right;\">             10.0362</td><td style=\"text-align: right;\">            -8.02914</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 996000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-04-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.036246657371521\n",
      "  episode_reward_mean: 0.08065620392560958\n",
      "  episode_reward_min: -8.029140010476112\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6243909279505413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01622330332237576\n",
      "          policy_loss: -0.10459137882796987\n",
      "          total_loss: 2.7342774517373534\n",
      "          vf_explained_var: 0.8732651598350976\n",
      "          vf_loss: 2.814229672994985\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 996000\n",
      "    num_agent_steps_trained: 996000\n",
      "    num_steps_sampled: 996000\n",
      "    num_steps_trained: 996000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.161538461538463\n",
      "    ram_util_percent: 34.13076923076923\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10175715721003115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08910596438223943\n",
      "    mean_inference_ms: 0.9999995549104571\n",
      "    mean_raw_obs_processing_ms: 0.08419087910786698\n",
      "  time_since_restore: 2138.34992313385\n",
      "  time_this_iter_s: 8.901087760925293\n",
      "  time_total_s: 2138.34992313385\n",
      "  timers:\n",
      "    learn_throughput: 675.569\n",
      "    learn_time_ms: 5920.933\n",
      "    load_throughput: 13350215.644\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 474.582\n",
      "    sample_time_ms: 8428.474\n",
      "    update_time_ms: 1.471\n",
      "  timestamp: 1643385856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 996000\n",
      "  training_iteration: 249\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:19 (running for 00:35:50.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         2138.35</td><td style=\"text-align: right;\">996000</td><td style=\"text-align: right;\">0.0806562</td><td style=\"text-align: right;\">             10.0362</td><td style=\"text-align: right;\">            -8.02914</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:24 (running for 00:35:55.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         2138.35</td><td style=\"text-align: right;\">996000</td><td style=\"text-align: right;\">0.0806562</td><td style=\"text-align: right;\">             10.0362</td><td style=\"text-align: right;\">            -8.02914</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-04-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.799011915922165\n",
      "  episode_reward_mean: 0.005098842978477478\n",
      "  episode_reward_min: -10.899794474244118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6885708480752926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015687167164976954\n",
      "          policy_loss: -0.1001789264025427\n",
      "          total_loss: 3.5043845363922657\n",
      "          vf_explained_var: 0.8887683911990094\n",
      "          vf_loss: 3.5807385787207595\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.674999999999997\n",
      "    ram_util_percent: 34.11875\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10178455494898593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0891327471171094\n",
      "    mean_inference_ms: 1.0002827821283216\n",
      "    mean_raw_obs_processing_ms: 0.08421394246086315\n",
      "  time_since_restore: 2149.8715360164642\n",
      "  time_this_iter_s: 11.521612882614136\n",
      "  time_total_s: 2149.8715360164642\n",
      "  timers:\n",
      "    learn_throughput: 651.763\n",
      "    learn_time_ms: 6137.202\n",
      "    load_throughput: 12926431.928\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 467.049\n",
      "    sample_time_ms: 8564.405\n",
      "    update_time_ms: 1.47\n",
      "  timestamp: 1643385868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 250\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:29 (running for 00:36:01.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         2149.87</td><td style=\"text-align: right;\">1000000</td><td style=\"text-align: right;\">0.00509884</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:34 (running for 00:36:06.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         2149.87</td><td style=\"text-align: right;\">1000000</td><td style=\"text-align: right;\">0.00509884</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1004000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.799011915922165\n",
      "  episode_reward_mean: 0.05228754580020904\n",
      "  episode_reward_min: -10.899794474244118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6675862278989566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015958633659388415\n",
      "          policy_loss: -0.06622118089656015\n",
      "          total_loss: 1.3180606440976463\n",
      "          vf_explained_var: 0.9371598177699633\n",
      "          vf_loss: 1.3600446477772727\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1004000\n",
      "    num_agent_steps_trained: 1004000\n",
      "    num_steps_sampled: 1004000\n",
      "    num_steps_trained: 1004000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.607142857142854\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10183589102935744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08917794659676861\n",
      "    mean_inference_ms: 1.000818177453008\n",
      "    mean_raw_obs_processing_ms: 0.08425812303190426\n",
      "  time_since_restore: 2159.4886326789856\n",
      "  time_this_iter_s: 9.617096662521362\n",
      "  time_total_s: 2159.4886326789856\n",
      "  timers:\n",
      "    learn_throughput: 649.241\n",
      "    learn_time_ms: 6161.043\n",
      "    load_throughput: 11205728.026\n",
      "    load_time_ms: 0.357\n",
      "    sample_throughput: 450.935\n",
      "    sample_time_ms: 8870.454\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1643385878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1004000\n",
      "  training_iteration: 251\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:40 (running for 00:36:11.87)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         2159.49</td><td style=\"text-align: right;\">1004000</td><td style=\"text-align: right;\">0.0522875</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:45 (running for 00:36:16.88)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         2159.49</td><td style=\"text-align: right;\">1004000</td><td style=\"text-align: right;\">0.0522875</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1008000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-04-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.799011915922165\n",
      "  episode_reward_mean: 0.01606586456298828\n",
      "  episode_reward_min: -10.899794474244118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7041850843737203\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01582327003621212\n",
      "          policy_loss: -0.08983307642641888\n",
      "          total_loss: 1.7964881703290108\n",
      "          vf_explained_var: 0.9089592493990416\n",
      "          vf_loss: 1.8622896640291138\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1008000\n",
      "    num_agent_steps_trained: 1008000\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.366666666666667\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188576522983617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08922264560109276\n",
      "    mean_inference_ms: 1.001345153417631\n",
      "    mean_raw_obs_processing_ms: 0.08430105119283288\n",
      "  time_since_restore: 2167.7698159217834\n",
      "  time_this_iter_s: 8.281183242797852\n",
      "  time_total_s: 2167.7698159217834\n",
      "  timers:\n",
      "    learn_throughput: 648.717\n",
      "    learn_time_ms: 6166.021\n",
      "    load_throughput: 11320658.57\n",
      "    load_time_ms: 0.353\n",
      "    sample_throughput: 448.972\n",
      "    sample_time_ms: 8909.244\n",
      "    update_time_ms: 1.454\n",
      "  timestamp: 1643385886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 252\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:50 (running for 00:36:22.18)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         2167.77</td><td style=\"text-align: right;\">1008000</td><td style=\"text-align: right;\">0.0160659</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1012000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-04-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.799011915922165\n",
      "  episode_reward_mean: -0.006830420196056366\n",
      "  episode_reward_min: -10.899794474244118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7175631815387358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016274729693748737\n",
      "          policy_loss: -0.09087688629865967\n",
      "          total_loss: 2.0328609218141214\n",
      "          vf_explained_var: 0.9192340271447295\n",
      "          vf_loss: 2.099020576901654\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1012000\n",
      "    num_agent_steps_trained: 1012000\n",
      "    num_steps_sampled: 1012000\n",
      "    num_steps_trained: 1012000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.816666666666666\n",
      "    ram_util_percent: 34.12500000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.101937843304909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08926585757377584\n",
      "    mean_inference_ms: 1.0018728296753958\n",
      "    mean_raw_obs_processing_ms: 0.08434353245046579\n",
      "  time_since_restore: 2176.1571094989777\n",
      "  time_this_iter_s: 8.387293577194214\n",
      "  time_total_s: 2176.1571094989777\n",
      "  timers:\n",
      "    learn_throughput: 649.023\n",
      "    learn_time_ms: 6163.109\n",
      "    load_throughput: 11337488.85\n",
      "    load_time_ms: 0.353\n",
      "    sample_throughput: 447.571\n",
      "    sample_time_ms: 8937.127\n",
      "    update_time_ms: 1.459\n",
      "  timestamp: 1643385894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1012000\n",
      "  training_iteration: 253\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:04:55 (running for 00:36:27.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         2176.16</td><td style=\"text-align: right;\">1012000</td><td style=\"text-align: right;\">-0.00683042</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:00 (running for 00:36:32.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         2176.16</td><td style=\"text-align: right;\">1012000</td><td style=\"text-align: right;\">-0.00683042</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-05-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.799011915922165\n",
      "  episode_reward_mean: -0.045038530826568605\n",
      "  episode_reward_min: -10.899794474244118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6326633512332873\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01650974671866569\n",
      "          policy_loss: -0.09816611809005099\n",
      "          total_loss: 1.4262617913069784\n",
      "          vf_explained_var: 0.9243643285125814\n",
      "          vf_loss: 1.4993537437851712\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 1016000\n",
      "    num_steps_trained: 1016000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.336363636363636\n",
      "    ram_util_percent: 34.17272727272728\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10197979665347352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08930299805788296\n",
      "    mean_inference_ms: 1.0023160257438366\n",
      "    mean_raw_obs_processing_ms: 0.08437866236516851\n",
      "  time_since_restore: 2184.3727169036865\n",
      "  time_this_iter_s: 8.215607404708862\n",
      "  time_total_s: 2184.3727169036865\n",
      "  timers:\n",
      "    learn_throughput: 649.324\n",
      "    learn_time_ms: 6160.252\n",
      "    load_throughput: 10349278.885\n",
      "    load_time_ms: 0.387\n",
      "    sample_throughput: 447.681\n",
      "    sample_time_ms: 8934.929\n",
      "    update_time_ms: 1.475\n",
      "  timestamp: 1643385903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1016000\n",
      "  training_iteration: 254\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:06 (running for 00:36:37.83)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         2184.37</td><td style=\"text-align: right;\">1016000</td><td style=\"text-align: right;\">-0.0450385</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:11 (running for 00:36:42.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         2184.37</td><td style=\"text-align: right;\">1016000</td><td style=\"text-align: right;\">-0.0450385</td><td style=\"text-align: right;\">              13.799</td><td style=\"text-align: right;\">            -10.8998</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1020000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.22845596075058\n",
      "  episode_reward_mean: -0.11905044998973607\n",
      "  episode_reward_min: -8.636599510908127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.647283377955037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01575491279795591\n",
      "          policy_loss: -0.10615180716399224\n",
      "          total_loss: 2.0717444394582443\n",
      "          vf_explained_var: 0.9145680298728328\n",
      "          vf_loss: 2.1539684697104398\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1020000\n",
      "    num_agent_steps_trained: 1020000\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.89375\n",
      "    ram_util_percent: 34.118750000000006\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10200592133838846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08932807170951765\n",
      "    mean_inference_ms: 1.0025720194326897\n",
      "    mean_raw_obs_processing_ms: 0.08439929226664876\n",
      "  time_since_restore: 2195.261862516403\n",
      "  time_this_iter_s: 10.889145612716675\n",
      "  time_total_s: 2195.261862516403\n",
      "  timers:\n",
      "    learn_throughput: 627.631\n",
      "    learn_time_ms: 6373.167\n",
      "    load_throughput: 10059489.147\n",
      "    load_time_ms: 0.398\n",
      "    sample_throughput: 445.708\n",
      "    sample_time_ms: 8974.491\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1643385914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 255\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:17 (running for 00:36:48.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         2195.26</td><td style=\"text-align: right;\">1020000</td><td style=\"text-align: right;\">-0.11905</td><td style=\"text-align: right;\">             9.22846</td><td style=\"text-align: right;\">             -8.6366</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:22 (running for 00:36:53.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         2195.26</td><td style=\"text-align: right;\">1020000</td><td style=\"text-align: right;\">-0.11905</td><td style=\"text-align: right;\">             9.22846</td><td style=\"text-align: right;\">             -8.6366</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1024000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-05-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.22845596075058\n",
      "  episode_reward_mean: 0.0013506174832582474\n",
      "  episode_reward_min: -8.636599510908127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5757592795997537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016042982940926015\n",
      "          policy_loss: -0.07900321529827692\n",
      "          total_loss: 1.0543314077570953\n",
      "          vf_explained_var: 0.9483178884111425\n",
      "          vf_loss: 1.1089693449517732\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1024000\n",
      "    num_agent_steps_trained: 1024000\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1024000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.483333333333334\n",
      "    ram_util_percent: 34.12500000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10200439910668535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08933098894413781\n",
      "    mean_inference_ms: 1.0025475191475601\n",
      "    mean_raw_obs_processing_ms: 0.08439634924847864\n",
      "  time_since_restore: 2203.912829875946\n",
      "  time_this_iter_s: 8.650967359542847\n",
      "  time_total_s: 2203.912829875946\n",
      "  timers:\n",
      "    learn_throughput: 626.215\n",
      "    learn_time_ms: 6387.581\n",
      "    load_throughput: 10218794.007\n",
      "    load_time_ms: 0.391\n",
      "    sample_throughput: 435.797\n",
      "    sample_time_ms: 9178.583\n",
      "    update_time_ms: 1.481\n",
      "  timestamp: 1643385922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 256\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:27 (running for 00:36:59.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         2203.91</td><td style=\"text-align: right;\">1024000</td><td style=\"text-align: right;\">0.00135062</td><td style=\"text-align: right;\">             9.22846</td><td style=\"text-align: right;\">             -8.6366</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1028000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-05-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.22845596075058\n",
      "  episode_reward_mean: 0.04941747099161148\n",
      "  episode_reward_min: -8.636599510908127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.568799566966231\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01635164168686261\n",
      "          policy_loss: -0.0897266641289236\n",
      "          total_loss: 1.8244180509990822\n",
      "          vf_explained_var: 0.9193036181952363\n",
      "          vf_loss: 1.88931066363409\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1028000\n",
      "    num_agent_steps_trained: 1028000\n",
      "    num_steps_sampled: 1028000\n",
      "    num_steps_trained: 1028000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.583333333333332\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10200140239651648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08933167734468213\n",
      "    mean_inference_ms: 1.002503592430004\n",
      "    mean_raw_obs_processing_ms: 0.0843921773656513\n",
      "  time_since_restore: 2212.1598613262177\n",
      "  time_this_iter_s: 8.247031450271606\n",
      "  time_total_s: 2212.1598613262177\n",
      "  timers:\n",
      "    learn_throughput: 628.287\n",
      "    learn_time_ms: 6366.517\n",
      "    load_throughput: 10313016.966\n",
      "    load_time_ms: 0.388\n",
      "    sample_throughput: 435.541\n",
      "    sample_time_ms: 9183.991\n",
      "    update_time_ms: 1.459\n",
      "  timestamp: 1643385930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1028000\n",
      "  training_iteration: 257\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:33 (running for 00:37:04.69)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         2212.16</td><td style=\"text-align: right;\">1028000</td><td style=\"text-align: right;\">0.0494175</td><td style=\"text-align: right;\">             9.22846</td><td style=\"text-align: right;\">             -8.6366</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:38 (running for 00:37:09.69)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         2212.16</td><td style=\"text-align: right;\">1028000</td><td style=\"text-align: right;\">0.0494175</td><td style=\"text-align: right;\">             9.22846</td><td style=\"text-align: right;\">             -8.6366</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-05-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.124058872461319\n",
      "  episode_reward_mean: 0.05008271485567093\n",
      "  episode_reward_min: -8.636599510908127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5424873680196782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01658677650185058\n",
      "          policy_loss: -0.07991292727960934\n",
      "          total_loss: 2.875101736674125\n",
      "          vf_explained_var: 0.8958508606239032\n",
      "          vf_loss: 2.929823521748986\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.23076923076923\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10199935700592494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08933985355189535\n",
      "    mean_inference_ms: 1.0025035580506672\n",
      "    mean_raw_obs_processing_ms: 0.08439061185285252\n",
      "  time_since_restore: 2220.9389510154724\n",
      "  time_this_iter_s: 8.77908968925476\n",
      "  time_total_s: 2220.9389510154724\n",
      "  timers:\n",
      "    learn_throughput: 628.841\n",
      "    learn_time_ms: 6360.906\n",
      "    load_throughput: 10313650.95\n",
      "    load_time_ms: 0.388\n",
      "    sample_throughput: 435.756\n",
      "    sample_time_ms: 9179.44\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1643385939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 258\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:43 (running for 00:37:15.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         2220.94</td><td style=\"text-align: right;\">1032000</td><td style=\"text-align: right;\">0.0500827</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">             -8.6366</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1036000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.124058872461319\n",
      "  episode_reward_mean: 0.056888414174318315\n",
      "  episode_reward_min: -7.46118351817131\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3913199263234293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017171238498232464\n",
      "          policy_loss: -0.09379650737354231\n",
      "          total_loss: 1.2740301441669124\n",
      "          vf_explained_var: 0.9417287986124715\n",
      "          vf_loss: 1.3417478373774918\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1036000\n",
      "    num_agent_steps_trained: 1036000\n",
      "    num_steps_sampled: 1036000\n",
      "    num_steps_trained: 1036000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.791666666666668\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10199843886355708\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08934964500631466\n",
      "    mean_inference_ms: 1.0025098585864356\n",
      "    mean_raw_obs_processing_ms: 0.08439031273805604\n",
      "  time_since_restore: 2229.3974809646606\n",
      "  time_this_iter_s: 8.458529949188232\n",
      "  time_total_s: 2229.3974809646606\n",
      "  timers:\n",
      "    learn_throughput: 630.777\n",
      "    learn_time_ms: 6341.387\n",
      "    load_throughput: 10279526.99\n",
      "    load_time_ms: 0.389\n",
      "    sample_throughput: 437.201\n",
      "    sample_time_ms: 9149.112\n",
      "    update_time_ms: 1.398\n",
      "  timestamp: 1643385948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1036000\n",
      "  training_iteration: 259\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:49 (running for 00:37:20.97)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">          2229.4</td><td style=\"text-align: right;\">1036000</td><td style=\"text-align: right;\">0.0568884</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">            -7.46118</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:54 (running for 00:37:25.97)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">          2229.4</td><td style=\"text-align: right;\">1036000</td><td style=\"text-align: right;\">0.0568884</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">            -7.46118</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-05-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.124058872461319\n",
      "  episode_reward_mean: 0.05280059155076742\n",
      "  episode_reward_min: -9.995597898960114\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4237698389637856\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015392149616689754\n",
      "          policy_loss: -0.10260907746951586\n",
      "          total_loss: 1.9274782429269004\n",
      "          vf_explained_var: 0.923273931395623\n",
      "          vf_loss: 2.0067105021608134\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1040000\n",
      "    num_agent_steps_trained: 1040000\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.58181818181818\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10198244208274396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08934655403635045\n",
      "    mean_inference_ms: 1.0023755168301245\n",
      "    mean_raw_obs_processing_ms: 0.08437829120384195\n",
      "  time_since_restore: 2237.5615770816803\n",
      "  time_this_iter_s: 8.164096117019653\n",
      "  time_total_s: 2237.5615770816803\n",
      "  timers:\n",
      "    learn_throughput: 654.643\n",
      "    learn_time_ms: 6110.2\n",
      "    load_throughput: 10384511.018\n",
      "    load_time_ms: 0.385\n",
      "    sample_throughput: 443.232\n",
      "    sample_time_ms: 9024.609\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1643385956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 260\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:05:59 (running for 00:37:31.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         2237.56</td><td style=\"text-align: right;\">1040000</td><td style=\"text-align: right;\">0.0528006</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:04 (running for 00:37:36.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         2237.56</td><td style=\"text-align: right;\">1040000</td><td style=\"text-align: right;\">0.0528006</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1044000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-06-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.124058872461319\n",
      "  episode_reward_mean: -0.11369348056614399\n",
      "  episode_reward_min: -9.995597898960114\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.337607208887736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014879048721924075\n",
      "          policy_loss: -0.10818996158558675\n",
      "          total_loss: 1.9244797396904199\n",
      "          vf_explained_var: 0.9191222142788672\n",
      "          vf_loss: 2.010072154006971\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1044000\n",
      "    num_agent_steps_trained: 1044000\n",
      "    num_steps_sampled: 1044000\n",
      "    num_steps_trained: 1044000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.200000000000003\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1019686264253076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08934524317888898\n",
      "    mean_inference_ms: 1.0022620958265132\n",
      "    mean_raw_obs_processing_ms: 0.08437313750992896\n",
      "  time_since_restore: 2245.930281162262\n",
      "  time_this_iter_s: 8.368704080581665\n",
      "  time_total_s: 2245.930281162262\n",
      "  timers:\n",
      "    learn_throughput: 659.262\n",
      "    learn_time_ms: 6067.386\n",
      "    load_throughput: 11876834.206\n",
      "    load_time_ms: 0.337\n",
      "    sample_throughput: 459.206\n",
      "    sample_time_ms: 8710.687\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1643385964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1044000\n",
      "  training_iteration: 261\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:09 (running for 00:37:41.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         2245.93</td><td style=\"text-align: right;\">1044000</td><td style=\"text-align: right;\">-0.113693</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-06-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.124058872461319\n",
      "  episode_reward_mean: -0.00750969722867012\n",
      "  episode_reward_min: -9.995597898960114\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4013573949055007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0169325732544352\n",
      "          policy_loss: -0.10105380259694592\n",
      "          total_loss: 2.7565083520867493\n",
      "          vf_explained_var: 0.9024661291030145\n",
      "          vf_loss: 2.831845805697864\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1048000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.275\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10195798381072972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08934543115288417\n",
      "    mean_inference_ms: 1.0021787879130761\n",
      "    mean_raw_obs_processing_ms: 0.08436993661807966\n",
      "  time_since_restore: 2254.2584545612335\n",
      "  time_this_iter_s: 8.328173398971558\n",
      "  time_total_s: 2254.2584545612335\n",
      "  timers:\n",
      "    learn_throughput: 659.043\n",
      "    learn_time_ms: 6069.407\n",
      "    load_throughput: 11891987.525\n",
      "    load_time_ms: 0.336\n",
      "    sample_throughput: 461.344\n",
      "    sample_time_ms: 8670.311\n",
      "    update_time_ms: 1.321\n",
      "  timestamp: 1643385973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 262\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:15 (running for 00:37:46.90)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         2254.26</td><td style=\"text-align: right;\">1048000</td><td style=\"text-align: right;\">-0.0075097</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:20 (running for 00:37:51.90)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         2254.26</td><td style=\"text-align: right;\">1048000</td><td style=\"text-align: right;\">-0.0075097</td><td style=\"text-align: right;\">             14.1241</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1052000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-06-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.5529546439647675\n",
      "  episode_reward_mean: 0.009486614167690277\n",
      "  episode_reward_min: -9.995597898960114\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3419327448773126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016502645464367104\n",
      "          policy_loss: -0.09302965151246197\n",
      "          total_loss: 2.537471238731517\n",
      "          vf_explained_var: 0.9005583224117115\n",
      "          vf_loss: 2.605437499236676\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1052000\n",
      "    num_agent_steps_trained: 1052000\n",
      "    num_steps_sampled: 1052000\n",
      "    num_steps_trained: 1052000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.661538461538463\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10194455399079977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08934028800459551\n",
      "    mean_inference_ms: 1.0020772047196345\n",
      "    mean_raw_obs_processing_ms: 0.08436528387372137\n",
      "  time_since_restore: 2262.986660718918\n",
      "  time_this_iter_s: 8.728206157684326\n",
      "  time_total_s: 2262.986660718918\n",
      "  timers:\n",
      "    learn_throughput: 656.137\n",
      "    learn_time_ms: 6096.289\n",
      "    load_throughput: 11890301.914\n",
      "    load_time_ms: 0.336\n",
      "    sample_throughput: 460.885\n",
      "    sample_time_ms: 8678.952\n",
      "    update_time_ms: 1.333\n",
      "  timestamp: 1643385981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1052000\n",
      "  training_iteration: 263\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:25 (running for 00:37:57.65)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         2262.99</td><td style=\"text-align: right;\">1052000</td><td style=\"text-align: right;\">0.00948661</td><td style=\"text-align: right;\">             7.55295</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1056000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-06-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.5529546439647675\n",
      "  episode_reward_mean: -0.01809519812464714\n",
      "  episode_reward_min: -9.995597898960114\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2540338912317828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01668662118099068\n",
      "          policy_loss: -0.0780129526170992\n",
      "          total_loss: 1.6362462822694621\n",
      "          vf_explained_var: 0.9270257721665085\n",
      "          vf_loss: 1.6889164261279568\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1056000\n",
      "    num_agent_steps_trained: 1056000\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.083333333333332\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10193117345265691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08933266777109033\n",
      "    mean_inference_ms: 1.001963653172858\n",
      "    mean_raw_obs_processing_ms: 0.08435978353723084\n",
      "  time_since_restore: 2271.6559405326843\n",
      "  time_this_iter_s: 8.66927981376648\n",
      "  time_total_s: 2271.6559405326843\n",
      "  timers:\n",
      "    learn_throughput: 651.07\n",
      "    learn_time_ms: 6143.73\n",
      "    load_throughput: 13235418.113\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 459.548\n",
      "    sample_time_ms: 8704.201\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1643385990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 264\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:31 (running for 00:38:03.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         2271.66</td><td style=\"text-align: right;\">1056000</td><td style=\"text-align: right;\">-0.0180952</td><td style=\"text-align: right;\">             7.55295</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:36 (running for 00:38:08.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         2271.66</td><td style=\"text-align: right;\">1056000</td><td style=\"text-align: right;\">-0.0180952</td><td style=\"text-align: right;\">             7.55295</td><td style=\"text-align: right;\">             -9.9956</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1060000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-06-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.5529546439647675\n",
      "  episode_reward_mean: 0.08577822402119636\n",
      "  episode_reward_min: -6.950744241476059\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3257153071382994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016662421860978417\n",
      "          policy_loss: -0.08218466865039\n",
      "          total_loss: 1.417220086107088\n",
      "          vf_explained_var: 0.9410688862364779\n",
      "          vf_loss: 1.4740987056605919\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1060000\n",
      "    num_agent_steps_trained: 1060000\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.750000000000004\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10192107914324135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08932792572829956\n",
      "    mean_inference_ms: 1.0018839685492482\n",
      "    mean_raw_obs_processing_ms: 0.08435690540097361\n",
      "  time_since_restore: 2280.07737660408\n",
      "  time_this_iter_s: 8.421436071395874\n",
      "  time_total_s: 2280.07737660408\n",
      "  timers:\n",
      "    learn_throughput: 674.089\n",
      "    learn_time_ms: 5933.935\n",
      "    load_throughput: 13678936.812\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 458.981\n",
      "    sample_time_ms: 8714.959\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1643385999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 265\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:42 (running for 00:38:13.79)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         2280.08</td><td style=\"text-align: right;\">1060000</td><td style=\"text-align: right;\">0.0857782</td><td style=\"text-align: right;\">             7.55295</td><td style=\"text-align: right;\">            -6.95074</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:47 (running for 00:38:18.80)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         2280.08</td><td style=\"text-align: right;\">1060000</td><td style=\"text-align: right;\">0.0857782</td><td style=\"text-align: right;\">             7.55295</td><td style=\"text-align: right;\">            -6.95074</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-06-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.671647489070892\n",
      "  episode_reward_mean: 0.09559748023748398\n",
      "  episode_reward_min: -6.950744241476059\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.266374675689205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016807881691925996\n",
      "          policy_loss: -0.09390330992599008\n",
      "          total_loss: 1.3138791625237753\n",
      "          vf_explained_var: 0.9368879982861139\n",
      "          vf_loss: 1.3822555041201012\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 1064000\n",
      "    num_steps_trained: 1064000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.566666666666666\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1019096792805274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08932115064219222\n",
      "    mean_inference_ms: 1.0017970531630018\n",
      "    mean_raw_obs_processing_ms: 0.08434820946040204\n",
      "  time_since_restore: 2288.6043705940247\n",
      "  time_this_iter_s: 8.526993989944458\n",
      "  time_total_s: 2288.6043705940247\n",
      "  timers:\n",
      "    learn_throughput: 675.838\n",
      "    learn_time_ms: 5918.579\n",
      "    load_throughput: 13653333.333\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 470.185\n",
      "    sample_time_ms: 8507.287\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1643386007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1064000\n",
      "  training_iteration: 266\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:52 (running for 00:38:24.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">          2288.6</td><td style=\"text-align: right;\">1064000</td><td style=\"text-align: right;\">0.0955975</td><td style=\"text-align: right;\">             6.67165</td><td style=\"text-align: right;\">            -6.95074</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1068000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.514397531747818\n",
      "  episode_reward_mean: -0.013297992795705794\n",
      "  episode_reward_min: -6.510771930217743\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.238489104470899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01687568425913234\n",
      "          policy_loss: -0.09606715691706506\n",
      "          total_loss: 1.5855801011801207\n",
      "          vf_explained_var: 0.922611794933196\n",
      "          vf_loss: 1.6560173130644265\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1068000\n",
      "    num_agent_steps_trained: 1068000\n",
      "    num_steps_sampled: 1068000\n",
      "    num_steps_trained: 1068000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.575\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1018945479165191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0893126728455358\n",
      "    mean_inference_ms: 1.0016760431608094\n",
      "    mean_raw_obs_processing_ms: 0.08433748223953254\n",
      "  time_since_restore: 2296.7616000175476\n",
      "  time_this_iter_s: 8.15722942352295\n",
      "  time_total_s: 2296.7616000175476\n",
      "  timers:\n",
      "    learn_throughput: 676.632\n",
      "    learn_time_ms: 5911.635\n",
      "    load_throughput: 13630039.808\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 471.156\n",
      "    sample_time_ms: 8489.752\n",
      "    update_time_ms: 1.406\n",
      "  timestamp: 1643386015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1068000\n",
      "  training_iteration: 267\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:06:57 (running for 00:38:29.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         2296.76</td><td style=\"text-align: right;\">1068000</td><td style=\"text-align: right;\">-0.013298</td><td style=\"text-align: right;\">              8.5144</td><td style=\"text-align: right;\">            -6.51077</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:02 (running for 00:38:34.52)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         2296.76</td><td style=\"text-align: right;\">1068000</td><td style=\"text-align: right;\">-0.013298</td><td style=\"text-align: right;\">              8.5144</td><td style=\"text-align: right;\">            -6.51077</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1072000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-07-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.845226347446442\n",
      "  episode_reward_mean: -0.010914555490016938\n",
      "  episode_reward_min: -7.2643373012542725\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2327306227017476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016201670484065126\n",
      "          policy_loss: -0.08744028061327914\n",
      "          total_loss: 2.705431218834854\n",
      "          vf_explained_var: 0.8950423010574874\n",
      "          vf_loss: 2.768265201591997\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1072000\n",
      "    num_agent_steps_trained: 1072000\n",
      "    num_steps_sampled: 1072000\n",
      "    num_steps_trained: 1072000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.291666666666668\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10187783672242996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08930756970280868\n",
      "    mean_inference_ms: 1.001533675038567\n",
      "    mean_raw_obs_processing_ms: 0.08432682202562095\n",
      "  time_since_restore: 2305.2253828048706\n",
      "  time_this_iter_s: 8.463782787322998\n",
      "  time_total_s: 2305.2253828048706\n",
      "  timers:\n",
      "    learn_throughput: 678.6\n",
      "    learn_time_ms: 5894.485\n",
      "    load_throughput: 13545305.991\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 472.361\n",
      "    sample_time_ms: 8468.101\n",
      "    update_time_ms: 1.455\n",
      "  timestamp: 1643386024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1072000\n",
      "  training_iteration: 268\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:08 (running for 00:38:40.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         2305.23</td><td style=\"text-align: right;\">1072000</td><td style=\"text-align: right;\">-0.0109146</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1076000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-07-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.845226347446442\n",
      "  episode_reward_mean: -0.02439266175031662\n",
      "  episode_reward_min: -7.2643373012542725\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.1277007236275622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01643404436366042\n",
      "          policy_loss: -0.09252733455830685\n",
      "          total_loss: 0.9982218513653304\n",
      "          vf_explained_var: 0.9376857339694936\n",
      "          vf_loss: 1.06578997362365\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1076000\n",
      "    num_agent_steps_trained: 1076000\n",
      "    num_steps_sampled: 1076000\n",
      "    num_steps_trained: 1076000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.258333333333336\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10186207436977869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08930458224013531\n",
      "    mean_inference_ms: 1.0014117987964724\n",
      "    mean_raw_obs_processing_ms: 0.08431883841874165\n",
      "  time_since_restore: 2313.765021800995\n",
      "  time_this_iter_s: 8.539638996124268\n",
      "  time_total_s: 2313.765021800995\n",
      "  timers:\n",
      "    learn_throughput: 677.915\n",
      "    learn_time_ms: 5900.446\n",
      "    load_throughput: 13798187.351\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 473.176\n",
      "    sample_time_ms: 8453.52\n",
      "    update_time_ms: 1.441\n",
      "  timestamp: 1643386032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1076000\n",
      "  training_iteration: 269\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:13 (running for 00:38:45.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         2313.77</td><td style=\"text-align: right;\">1076000</td><td style=\"text-align: right;\">-0.0243927</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:18 (running for 00:38:50.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         2313.77</td><td style=\"text-align: right;\">1076000</td><td style=\"text-align: right;\">-0.0243927</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-07-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.845226347446442\n",
      "  episode_reward_mean: -0.01997336611151695\n",
      "  episode_reward_min: -7.2643373012542725\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.114496719965371\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01593572586597799\n",
      "          policy_loss: -0.08317070503779236\n",
      "          total_loss: 1.6961163446071847\n",
      "          vf_explained_var: 0.9304288511635155\n",
      "          vf_loss: 1.755084688368664\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.49230769230769\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10184352357743882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08929911822809729\n",
      "    mean_inference_ms: 1.0012578199158413\n",
      "    mean_raw_obs_processing_ms: 0.08430840522477176\n",
      "  time_since_restore: 2322.235769033432\n",
      "  time_this_iter_s: 8.470747232437134\n",
      "  time_total_s: 2322.235769033432\n",
      "  timers:\n",
      "    learn_throughput: 674.381\n",
      "    learn_time_ms: 5931.362\n",
      "    load_throughput: 14078388.856\n",
      "    load_time_ms: 0.284\n",
      "    sample_throughput: 472.87\n",
      "    sample_time_ms: 8458.985\n",
      "    update_time_ms: 1.452\n",
      "  timestamp: 1643386041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 270\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:24 (running for 00:38:56.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         2322.24</td><td style=\"text-align: right;\">1080000</td><td style=\"text-align: right;\">-0.0199734</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:29 (running for 00:39:01.06)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         2322.24</td><td style=\"text-align: right;\">1080000</td><td style=\"text-align: right;\">-0.0199734</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1084000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-07-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.845226347446442\n",
      "  episode_reward_mean: 0.04662033945322037\n",
      "  episode_reward_min: -7.2643373012542725\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.9961674972247052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01713685009817033\n",
      "          policy_loss: -0.0791806943594448\n",
      "          total_loss: 1.8216674023337902\n",
      "          vf_explained_var: 0.9268705604537841\n",
      "          vf_loss: 1.8748215087839672\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1084000\n",
      "    num_agent_steps_trained: 1084000\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1084000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.299999999999997\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10182437406498543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08929558700790803\n",
      "    mean_inference_ms: 1.0011000639421943\n",
      "    mean_raw_obs_processing_ms: 0.08429815458166988\n",
      "  time_since_restore: 2330.889573574066\n",
      "  time_this_iter_s: 8.653804540634155\n",
      "  time_total_s: 2330.889573574066\n",
      "  timers:\n",
      "    learn_throughput: 670.719\n",
      "    learn_time_ms: 5963.751\n",
      "    load_throughput: 13819782.537\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 471.365\n",
      "    sample_time_ms: 8485.985\n",
      "    update_time_ms: 1.471\n",
      "  timestamp: 1643386050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 271\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:35 (running for 00:39:06.73)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         2330.89</td><td style=\"text-align: right;\">1084000</td><td style=\"text-align: right;\">0.0466203</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1088000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.845226347446442\n",
      "  episode_reward_mean: -0.071796954870224\n",
      "  episode_reward_min: -7.2643373012542725\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.9207790183764633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017140066351614917\n",
      "          policy_loss: -0.11290042181248947\n",
      "          total_loss: 1.6505504772965847\n",
      "          vf_explained_var: 0.9244171689274491\n",
      "          vf_loss: 1.737419428275798\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1088000\n",
      "    num_agent_steps_trained: 1088000\n",
      "    num_steps_sampled: 1088000\n",
      "    num_steps_trained: 1088000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.974999999999998\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10180959064718992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08929564969490661\n",
      "    mean_inference_ms: 1.0009848695258945\n",
      "    mean_raw_obs_processing_ms: 0.08429079543975329\n",
      "  time_since_restore: 2339.355945825577\n",
      "  time_this_iter_s: 8.46637225151062\n",
      "  time_total_s: 2339.355945825577\n",
      "  timers:\n",
      "    learn_throughput: 669.483\n",
      "    learn_time_ms: 5974.759\n",
      "    load_throughput: 13711356.653\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 469.43\n",
      "    sample_time_ms: 8520.963\n",
      "    update_time_ms: 1.493\n",
      "  timestamp: 1643386058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1088000\n",
      "  training_iteration: 272\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:40 (running for 00:39:12.22)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         2339.36</td><td style=\"text-align: right;\">1088000</td><td style=\"text-align: right;\">-0.071797</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:45 (running for 00:39:17.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         2339.36</td><td style=\"text-align: right;\">1088000</td><td style=\"text-align: right;\">-0.071797</td><td style=\"text-align: right;\">             11.8452</td><td style=\"text-align: right;\">            -7.26434</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1092000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-07-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.254099637269974\n",
      "  episode_reward_mean: -0.06548966735601425\n",
      "  episode_reward_min: -7.139305830001831\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.035785258713589\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01737281738612169\n",
      "          policy_loss: -0.12614232198966127\n",
      "          total_loss: 1.6505889899608108\n",
      "          vf_explained_var: 0.9183408443645764\n",
      "          vf_loss: 1.7503463389091594\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1092000\n",
      "    num_agent_steps_trained: 1092000\n",
      "    num_steps_sampled: 1092000\n",
      "    num_steps_trained: 1092000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.933333333333334\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10179344791730488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08929097119962334\n",
      "    mean_inference_ms: 1.0008550256063524\n",
      "    mean_raw_obs_processing_ms: 0.08428174557164843\n",
      "  time_since_restore: 2347.8742730617523\n",
      "  time_this_iter_s: 8.518327236175537\n",
      "  time_total_s: 2347.8742730617523\n",
      "  timers:\n",
      "    learn_throughput: 670.47\n",
      "    learn_time_ms: 5965.962\n",
      "    load_throughput: 13508225.443\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 469.499\n",
      "    sample_time_ms: 8519.721\n",
      "    update_time_ms: 1.5\n",
      "  timestamp: 1643386067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1092000\n",
      "  training_iteration: 273\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:51 (running for 00:39:22.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         2347.87</td><td style=\"text-align: right;\">1092000</td><td style=\"text-align: right;\">-0.0654897</td><td style=\"text-align: right;\">              8.2541</td><td style=\"text-align: right;\">            -7.13931</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1096000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-07-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.754832118749619\n",
      "  episode_reward_mean: 0.005389173626899719\n",
      "  episode_reward_min: -7.139305830001831\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.0421117262173722\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016661045761587178\n",
      "          policy_loss: -0.11472822850399841\n",
      "          total_loss: 1.7625862228353657\n",
      "          vf_explained_var: 0.9214941640694936\n",
      "          vf_loss: 1.852010498436228\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1096000\n",
      "    num_agent_steps_trained: 1096000\n",
      "    num_steps_sampled: 1096000\n",
      "    num_steps_trained: 1096000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.866666666666667\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10177645350929879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08928448795550882\n",
      "    mean_inference_ms: 1.0007125971199304\n",
      "    mean_raw_obs_processing_ms: 0.08427055476802234\n",
      "  time_since_restore: 2356.1568405628204\n",
      "  time_this_iter_s: 8.282567501068115\n",
      "  time_total_s: 2356.1568405628204\n",
      "  timers:\n",
      "    learn_throughput: 674.999\n",
      "    learn_time_ms: 5925.934\n",
      "    load_throughput: 13540933.01\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 469.909\n",
      "    sample_time_ms: 8512.282\n",
      "    update_time_ms: 1.465\n",
      "  timestamp: 1643386075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1096000\n",
      "  training_iteration: 274\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:07:56 (running for 00:39:28.06)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         2356.16</td><td style=\"text-align: right;\">1096000</td><td style=\"text-align: right;\">0.00538917</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -7.13931</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:01 (running for 00:39:33.07)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         2356.16</td><td style=\"text-align: right;\">1096000</td><td style=\"text-align: right;\">0.00538917</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -7.13931</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-08-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.754832118749619\n",
      "  episode_reward_mean: 0.05446293622255325\n",
      "  episode_reward_min: -7.621518716216087\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.0298417365679176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016112530804884978\n",
      "          policy_loss: -0.08787340009985592\n",
      "          total_loss: 2.4543459404322507\n",
      "          vf_explained_var: 0.9003418312918755\n",
      "          vf_loss: 2.517748435162088\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1100000\n",
      "    num_agent_steps_trained: 1100000\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.158333333333335\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10176008396263345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08927838123514607\n",
      "    mean_inference_ms: 1.0005786738370295\n",
      "    mean_raw_obs_processing_ms: 0.08425976854169105\n",
      "  time_since_restore: 2364.431282043457\n",
      "  time_this_iter_s: 8.274441480636597\n",
      "  time_total_s: 2364.431282043457\n",
      "  timers:\n",
      "    learn_throughput: 675.572\n",
      "    learn_time_ms: 5920.911\n",
      "    load_throughput: 13455141.551\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 472.698\n",
      "    sample_time_ms: 8462.063\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1643386083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 275\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:06 (running for 00:39:38.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         2364.43</td><td style=\"text-align: right;\">1100000</td><td style=\"text-align: right;\">0.0544629</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -7.62152</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:11 (running for 00:39:43.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         2364.43</td><td style=\"text-align: right;\">1100000</td><td style=\"text-align: right;\">0.0544629</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -7.62152</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.754832118749619\n",
      "  episode_reward_mean: -0.08513361513614655\n",
      "  episode_reward_min: -7.621518716216087\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.9468637137002842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016954519754334336\n",
      "          policy_loss: -0.1011485960364582\n",
      "          total_loss: 1.4632955139918473\n",
      "          vf_explained_var: 0.9389359973451142\n",
      "          vf_loss: 1.538694438194075\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1104000\n",
      "    num_agent_steps_trained: 1104000\n",
      "    num_steps_sampled: 1104000\n",
      "    num_steps_trained: 1104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.233333333333334\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10174619393616505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08927398571471472\n",
      "    mean_inference_ms: 1.0004306233090796\n",
      "    mean_raw_obs_processing_ms: 0.08424650380353044\n",
      "  time_since_restore: 2372.8390974998474\n",
      "  time_this_iter_s: 8.40781545639038\n",
      "  time_total_s: 2372.8390974998474\n",
      "  timers:\n",
      "    learn_throughput: 676.541\n",
      "    learn_time_ms: 5912.425\n",
      "    load_throughput: 13478923.435\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 473.182\n",
      "    sample_time_ms: 8453.407\n",
      "    update_time_ms: 1.471\n",
      "  timestamp: 1643386092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1104000\n",
      "  training_iteration: 276\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:17 (running for 00:39:48.80)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         2372.84</td><td style=\"text-align: right;\">1104000</td><td style=\"text-align: right;\">-0.0851336</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -7.62152</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-08-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.754832118749619\n",
      "  episode_reward_mean: 0.09033931851387024\n",
      "  episode_reward_min: -7.950432479381561\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.867474223977776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016159890294614353\n",
      "          policy_loss: -0.07799280363946191\n",
      "          total_loss: 2.260398321439542\n",
      "          vf_explained_var: 0.8931076879142433\n",
      "          vf_loss: 2.3138482938130056\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1108000\n",
      "    num_agent_steps_trained: 1108000\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.941666666666666\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10172896200829491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08926658558720262\n",
      "    mean_inference_ms: 1.0002507016395656\n",
      "    mean_raw_obs_processing_ms: 0.0842313124229734\n",
      "  time_since_restore: 2381.0671582221985\n",
      "  time_this_iter_s: 8.228060722351074\n",
      "  time_total_s: 2381.0671582221985\n",
      "  timers:\n",
      "    learn_throughput: 676.023\n",
      "    learn_time_ms: 5916.962\n",
      "    load_throughput: 13306802.03\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 473.481\n",
      "    sample_time_ms: 8448.066\n",
      "    update_time_ms: 1.46\n",
      "  timestamp: 1643386100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 277\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:22 (running for 00:39:54.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         2381.07</td><td style=\"text-align: right;\">1108000</td><td style=\"text-align: right;\">0.0903393</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -7.95043</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:27 (running for 00:39:59.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         2381.07</td><td style=\"text-align: right;\">1108000</td><td style=\"text-align: right;\">0.0903393</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -7.95043</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-08-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.754832118749619\n",
      "  episode_reward_mean: -0.07985167920589448\n",
      "  episode_reward_min: -10.356117308139801\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.976205656605382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01721031676680051\n",
      "          policy_loss: -0.11001097169975119\n",
      "          total_loss: 2.78969940669404\n",
      "          vf_explained_var: 0.8780887210240929\n",
      "          vf_loss: 2.8735722275190456\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1112000\n",
      "    num_agent_steps_trained: 1112000\n",
      "    num_steps_sampled: 1112000\n",
      "    num_steps_trained: 1112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.341666666666665\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10171588087205617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08925957306425987\n",
      "    mean_inference_ms: 1.0000774720477095\n",
      "    mean_raw_obs_processing_ms: 0.08421559060215777\n",
      "  time_since_restore: 2389.7112889289856\n",
      "  time_this_iter_s: 8.64413070678711\n",
      "  time_total_s: 2389.7112889289856\n",
      "  timers:\n",
      "    learn_throughput: 673.614\n",
      "    learn_time_ms: 5938.119\n",
      "    load_throughput: 13375760.185\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 473.371\n",
      "    sample_time_ms: 8450.027\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1643386109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1112000\n",
      "  training_iteration: 278\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:33 (running for 00:40:04.71)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         2389.71</td><td style=\"text-align: right;\">1112000</td><td style=\"text-align: right;\">-0.0798517</td><td style=\"text-align: right;\">             8.75483</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-08-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.303055167198181\n",
      "  episode_reward_mean: -0.02498523212969303\n",
      "  episode_reward_min: -10.356117308139801\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8073837034163935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0171650837220623\n",
      "          policy_loss: -0.10340053682285612\n",
      "          total_loss: 1.949977237288089\n",
      "          vf_explained_var: 0.9152500861434526\n",
      "          vf_loss: 2.027308279404076\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1116000\n",
      "    num_agent_steps_trained: 1116000\n",
      "    num_steps_sampled: 1116000\n",
      "    num_steps_trained: 1116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.85833333333333\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10170308394561908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08925234446601113\n",
      "    mean_inference_ms: 0.9999150951262336\n",
      "    mean_raw_obs_processing_ms: 0.08420255729307682\n",
      "  time_since_restore: 2397.9623947143555\n",
      "  time_this_iter_s: 8.251105785369873\n",
      "  time_total_s: 2397.9623947143555\n",
      "  timers:\n",
      "    learn_throughput: 676.742\n",
      "    learn_time_ms: 5910.676\n",
      "    load_throughput: 13389637.67\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 472.277\n",
      "    sample_time_ms: 8469.598\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1643386117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1116000\n",
      "  training_iteration: 279\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:38 (running for 00:40:09.98)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         2397.96</td><td style=\"text-align: right;\">1116000</td><td style=\"text-align: right;\">-0.0249852</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:43 (running for 00:40:14.99)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         2397.96</td><td style=\"text-align: right;\">1116000</td><td style=\"text-align: right;\">-0.0249852</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-08-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.303055167198181\n",
      "  episode_reward_mean: -0.11130264043807983\n",
      "  episode_reward_min: -10.356117308139801\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8360943898077935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016573939742571712\n",
      "          policy_loss: -0.11839884354142091\n",
      "          total_loss: 2.0548852614011937\n",
      "          vf_explained_var: 0.8981189196468682\n",
      "          vf_loss: 2.148112428508779\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.89090909090909\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10168868640535553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08924347489172846\n",
      "    mean_inference_ms: 0.9997329104314813\n",
      "    mean_raw_obs_processing_ms: 0.08418874184244779\n",
      "  time_since_restore: 2406.1155483722687\n",
      "  time_this_iter_s: 8.153153657913208\n",
      "  time_total_s: 2406.1155483722687\n",
      "  timers:\n",
      "    learn_throughput: 679.758\n",
      "    learn_time_ms: 5884.451\n",
      "    load_throughput: 13431443.439\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 474.118\n",
      "    sample_time_ms: 8436.719\n",
      "    update_time_ms: 1.374\n",
      "  timestamp: 1643386125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 280\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:48 (running for 00:40:20.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         2406.12</td><td style=\"text-align: right;\">1120000</td><td style=\"text-align: right;\">-0.111303</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:53 (running for 00:40:25.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         2406.12</td><td style=\"text-align: right;\">1120000</td><td style=\"text-align: right;\">-0.111303</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.303055167198181\n",
      "  episode_reward_mean: 0.062079755067825315\n",
      "  episode_reward_min: -10.356117308139801\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8901480203033776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017127134415362132\n",
      "          policy_loss: -0.08339300404292761\n",
      "          total_loss: 2.230748282699415\n",
      "          vf_explained_var: 0.9001780665689899\n",
      "          vf_loss: 2.2881294478732412\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1124000\n",
      "    num_agent_steps_trained: 1124000\n",
      "    num_steps_sampled: 1124000\n",
      "    num_steps_trained: 1124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.075\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10166627207284823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08922661494778569\n",
      "    mean_inference_ms: 0.9995046438516363\n",
      "    mean_raw_obs_processing_ms: 0.08417233686860079\n",
      "  time_since_restore: 2414.343182325363\n",
      "  time_this_iter_s: 8.227633953094482\n",
      "  time_total_s: 2414.343182325363\n",
      "  timers:\n",
      "    learn_throughput: 681.749\n",
      "    learn_time_ms: 5867.265\n",
      "    load_throughput: 13616764.873\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 477.051\n",
      "    sample_time_ms: 8384.844\n",
      "    update_time_ms: 1.37\n",
      "  timestamp: 1643386133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1124000\n",
      "  training_iteration: 281\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:08:58 (running for 00:40:30.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         2414.34</td><td style=\"text-align: right;\">1124000</td><td style=\"text-align: right;\">0.0620798</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.303055167198181\n",
      "  episode_reward_mean: -0.013323879837989806\n",
      "  episode_reward_min: -10.356117308139801\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8515379117381188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017290460906100185\n",
      "          policy_loss: -0.09246186439663932\n",
      "          total_loss: 1.4304609573205873\n",
      "          vf_explained_var: 0.9284372305357328\n",
      "          vf_loss: 1.4966629354383356\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1128000\n",
      "    num_agent_steps_trained: 1128000\n",
      "    num_steps_sampled: 1128000\n",
      "    num_steps_trained: 1128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.958333333333332\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1016442189912009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08921008744940692\n",
      "    mean_inference_ms: 0.9992801794241092\n",
      "    mean_raw_obs_processing_ms: 0.0841558994341718\n",
      "  time_since_restore: 2422.629245996475\n",
      "  time_this_iter_s: 8.28606367111206\n",
      "  time_total_s: 2422.629245996475\n",
      "  timers:\n",
      "    learn_throughput: 682.539\n",
      "    learn_time_ms: 5860.474\n",
      "    load_throughput: 13740553.645\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 478.674\n",
      "    sample_time_ms: 8356.419\n",
      "    update_time_ms: 1.373\n",
      "  timestamp: 1643386142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1128000\n",
      "  training_iteration: 282\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:04 (running for 00:40:35.71)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         2422.63</td><td style=\"text-align: right;\">1128000</td><td style=\"text-align: right;\">-0.0133239</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:09 (running for 00:40:40.72)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         2422.63</td><td style=\"text-align: right;\">1128000</td><td style=\"text-align: right;\">-0.0133239</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">            -10.3561</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-09-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.303055167198181\n",
      "  episode_reward_mean: 0.09599299728870392\n",
      "  episode_reward_min: -8.003295958042145\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.816347416882874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016988102741940007\n",
      "          policy_loss: -0.11370850657663678\n",
      "          total_loss: 1.6596195632298927\n",
      "          vf_explained_var: 0.934108397845299\n",
      "          vf_loss: 1.7475273948282966\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1132000\n",
      "    num_agent_steps_trained: 1132000\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.623076923076926\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10162263821614316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919841202040142\n",
      "    mean_inference_ms: 0.9990806247402985\n",
      "    mean_raw_obs_processing_ms: 0.08414222832203486\n",
      "  time_since_restore: 2431.3412249088287\n",
      "  time_this_iter_s: 8.711978912353516\n",
      "  time_total_s: 2431.3412249088287\n",
      "  timers:\n",
      "    learn_throughput: 681.821\n",
      "    learn_time_ms: 5866.64\n",
      "    load_throughput: 13810681.594\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 478.299\n",
      "    sample_time_ms: 8362.974\n",
      "    update_time_ms: 1.339\n",
      "  timestamp: 1643386150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 283\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:14 (running for 00:40:46.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         2431.34</td><td style=\"text-align: right;\">1132000</td><td style=\"text-align: right;\">0.095993</td><td style=\"text-align: right;\">             10.3031</td><td style=\"text-align: right;\">             -8.0033</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-09-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.774265110492706\n",
      "  episode_reward_mean: -0.01981692872941494\n",
      "  episode_reward_min: -8.93854433298111\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.9157398535359291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01678712344978877\n",
      "          policy_loss: -0.1105383944242031\n",
      "          total_loss: 1.8447064000912892\n",
      "          vf_explained_var: 0.9073555099707777\n",
      "          vf_loss: 1.9297493405640125\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1136000\n",
      "    num_agent_steps_trained: 1136000\n",
      "    num_steps_sampled: 1136000\n",
      "    num_steps_trained: 1136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.55\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10161185600038454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919547674503626\n",
      "    mean_inference_ms: 0.998963705751453\n",
      "    mean_raw_obs_processing_ms: 0.08413570962680227\n",
      "  time_since_restore: 2440.160768032074\n",
      "  time_this_iter_s: 8.81954312324524\n",
      "  time_total_s: 2440.160768032074\n",
      "  timers:\n",
      "    learn_throughput: 679.642\n",
      "    learn_time_ms: 5885.452\n",
      "    load_throughput: 13806135.616\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 475.966\n",
      "    sample_time_ms: 8403.965\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1643386159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1136000\n",
      "  training_iteration: 284\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:20 (running for 00:40:52.29)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         2440.16</td><td style=\"text-align: right;\">1136000</td><td style=\"text-align: right;\">-0.0198169</td><td style=\"text-align: right;\">             9.77427</td><td style=\"text-align: right;\">            -8.93854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:25 (running for 00:40:57.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         2440.16</td><td style=\"text-align: right;\">1136000</td><td style=\"text-align: right;\">-0.0198169</td><td style=\"text-align: right;\">             9.77427</td><td style=\"text-align: right;\">            -8.93854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-09-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.774265110492706\n",
      "  episode_reward_mean: 0.055312574803829194\n",
      "  episode_reward_min: -8.93854433298111\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8414423202955594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016071238624082038\n",
      "          policy_loss: -0.09306151908859911\n",
      "          total_loss: 2.2701389802569745\n",
      "          vf_explained_var: 0.8964072981829284\n",
      "          vf_loss: 2.3387922956017397\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_agent_steps_trained: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.991666666666664\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10160312518012912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919441604890978\n",
      "    mean_inference_ms: 0.9988670914092256\n",
      "    mean_raw_obs_processing_ms: 0.08413011668481665\n",
      "  time_since_restore: 2448.575833797455\n",
      "  time_this_iter_s: 8.41506576538086\n",
      "  time_total_s: 2448.575833797455\n",
      "  timers:\n",
      "    learn_throughput: 677.996\n",
      "    learn_time_ms: 5899.742\n",
      "    load_throughput: 13976354.548\n",
      "    load_time_ms: 0.286\n",
      "    sample_throughput: 474.895\n",
      "    sample_time_ms: 8422.911\n",
      "    update_time_ms: 1.326\n",
      "  timestamp: 1643386168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 285\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:31 (running for 00:41:02.73)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         2448.58</td><td style=\"text-align: right;\">1140000</td><td style=\"text-align: right;\">0.0553126</td><td style=\"text-align: right;\">             9.77427</td><td style=\"text-align: right;\">            -8.93854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:36 (running for 00:41:07.74)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         2448.58</td><td style=\"text-align: right;\">1140000</td><td style=\"text-align: right;\">0.0553126</td><td style=\"text-align: right;\">             9.77427</td><td style=\"text-align: right;\">            -8.93854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-09-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.790666103363037\n",
      "  episode_reward_mean: -0.06429580956697464\n",
      "  episode_reward_min: -8.93854433298111\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8166534567392\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017362090370209558\n",
      "          policy_loss: -0.11029296957096586\n",
      "          total_loss: 0.6759074977776837\n",
      "          vf_explained_var: 0.9627099795367128\n",
      "          vf_loss: 0.7598318021424034\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1144000\n",
      "    num_agent_steps_trained: 1144000\n",
      "    num_steps_sampled: 1144000\n",
      "    num_steps_trained: 1144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.466666666666665\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1015966283654301\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.089194888903046\n",
      "    mean_inference_ms: 0.9987880540869147\n",
      "    mean_raw_obs_processing_ms: 0.08412580674309902\n",
      "  time_since_restore: 2456.827568054199\n",
      "  time_this_iter_s: 8.251734256744385\n",
      "  time_total_s: 2456.827568054199\n",
      "  timers:\n",
      "    learn_throughput: 678.295\n",
      "    learn_time_ms: 5897.135\n",
      "    load_throughput: 14035987.618\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 474.801\n",
      "    sample_time_ms: 8424.578\n",
      "    update_time_ms: 1.305\n",
      "  timestamp: 1643386176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1144000\n",
      "  training_iteration: 286\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:41 (running for 00:41:13.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         2456.83</td><td style=\"text-align: right;\">1144000</td><td style=\"text-align: right;\">-0.0642958</td><td style=\"text-align: right;\">             8.79067</td><td style=\"text-align: right;\">            -8.93854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-09-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.860603630542755\n",
      "  episode_reward_mean: -0.003366048038005829\n",
      "  episode_reward_min: -8.93854433298111\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8424079829646696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017312195494656898\n",
      "          policy_loss: -0.10503510636207397\n",
      "          total_loss: 1.4249559517564272\n",
      "          vf_explained_var: 0.9292303587800713\n",
      "          vf_loss: 1.5036981593376846\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1148000\n",
      "    num_agent_steps_trained: 1148000\n",
      "    num_steps_sampled: 1148000\n",
      "    num_steps_trained: 1148000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.158333333333335\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159416318287363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919677506542549\n",
      "    mean_inference_ms: 0.9987371994281724\n",
      "    mean_raw_obs_processing_ms: 0.08412295261115417\n",
      "  time_since_restore: 2465.05686545372\n",
      "  time_this_iter_s: 8.229297399520874\n",
      "  time_total_s: 2465.05686545372\n",
      "  timers:\n",
      "    learn_throughput: 679.46\n",
      "    learn_time_ms: 5887.027\n",
      "    load_throughput: 14271194.284\n",
      "    load_time_ms: 0.28\n",
      "    sample_throughput: 474.386\n",
      "    sample_time_ms: 8431.955\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1643386184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1148000\n",
      "  training_iteration: 287\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:46 (running for 00:41:18.26)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         2465.06</td><td style=\"text-align: right;\">1148000</td><td style=\"text-align: right;\">-0.00336605</td><td style=\"text-align: right;\">              6.8606</td><td style=\"text-align: right;\">            -8.93854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:51 (running for 00:41:23.27)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         2465.06</td><td style=\"text-align: right;\">1148000</td><td style=\"text-align: right;\">-0.00336605</td><td style=\"text-align: right;\">              6.8606</td><td style=\"text-align: right;\">            -8.93854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-09-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.860603630542755\n",
      "  episode_reward_mean: -0.08487019628286362\n",
      "  episode_reward_min: -11.229356408119202\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8247781657403515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01441567853451265\n",
      "          policy_loss: -0.12439077559468006\n",
      "          total_loss: 1.5831058941293328\n",
      "          vf_explained_var: 0.9357644051633855\n",
      "          vf_loss: 1.6856028510758312\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1152000\n",
      "    num_agent_steps_trained: 1152000\n",
      "    num_steps_sampled: 1152000\n",
      "    num_steps_trained: 1152000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.216666666666665\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10158656982481987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919311028228162\n",
      "    mean_inference_ms: 0.9986467788631291\n",
      "    mean_raw_obs_processing_ms: 0.0841158979408627\n",
      "  time_since_restore: 2473.369946241379\n",
      "  time_this_iter_s: 8.313080787658691\n",
      "  time_total_s: 2473.369946241379\n",
      "  timers:\n",
      "    learn_throughput: 682.491\n",
      "    learn_time_ms: 5860.879\n",
      "    load_throughput: 14086663.308\n",
      "    load_time_ms: 0.284\n",
      "    sample_throughput: 475.348\n",
      "    sample_time_ms: 8414.883\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1643386192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1152000\n",
      "  training_iteration: 288\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:09:56 (running for 00:41:28.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         2473.37</td><td style=\"text-align: right;\">1152000</td><td style=\"text-align: right;\">-0.0848702</td><td style=\"text-align: right;\">              6.8606</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-10-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.971616595983505\n",
      "  episode_reward_mean: 0.04678739696741104\n",
      "  episode_reward_min: -11.229356408119202\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8015313026725606\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016167394142105877\n",
      "          policy_loss: -0.08965805115238312\n",
      "          total_loss: 1.3715913848826282\n",
      "          vf_explained_var: 0.928229425607189\n",
      "          vf_loss: 1.436695212462256\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1156000\n",
      "    num_agent_steps_trained: 1156000\n",
      "    num_steps_sampled: 1156000\n",
      "    num_steps_trained: 1156000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.13333333333333\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10157146014519038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08918169698426505\n",
      "    mean_inference_ms: 0.9984783071743101\n",
      "    mean_raw_obs_processing_ms: 0.08410059462012263\n",
      "  time_since_restore: 2482.152163505554\n",
      "  time_this_iter_s: 8.782217264175415\n",
      "  time_total_s: 2482.152163505554\n",
      "  timers:\n",
      "    learn_throughput: 676.706\n",
      "    learn_time_ms: 5910.988\n",
      "    load_throughput: 14117482.329\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 476.653\n",
      "    sample_time_ms: 8391.849\n",
      "    update_time_ms: 1.441\n",
      "  timestamp: 1643386201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1156000\n",
      "  training_iteration: 289\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:02 (running for 00:41:34.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         2482.15</td><td style=\"text-align: right;\">1156000</td><td style=\"text-align: right;\">0.0467874</td><td style=\"text-align: right;\">             7.97162</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:07 (running for 00:41:39.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         2482.15</td><td style=\"text-align: right;\">1156000</td><td style=\"text-align: right;\">0.0467874</td><td style=\"text-align: right;\">             7.97162</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-10-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.149964988231659\n",
      "  episode_reward_mean: -0.019331881105899812\n",
      "  episode_reward_min: -11.229356408119202\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6977195857673564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017782166504217083\n",
      "          policy_loss: -0.10426855949504722\n",
      "          total_loss: 1.9694932904064415\n",
      "          vf_explained_var: 0.911170708492238\n",
      "          vf_loss: 2.046755190762461\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1160000\n",
      "    num_agent_steps_trained: 1160000\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.299999999999997\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10156534033037895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0891792346267586\n",
      "    mean_inference_ms: 0.9984074687539741\n",
      "    mean_raw_obs_processing_ms: 0.08409305407710957\n",
      "  time_since_restore: 2490.8632414340973\n",
      "  time_this_iter_s: 8.71107792854309\n",
      "  time_total_s: 2490.8632414340973\n",
      "  timers:\n",
      "    learn_throughput: 675.242\n",
      "    learn_time_ms: 5923.799\n",
      "    load_throughput: 14135323.953\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 471.377\n",
      "    sample_time_ms: 8485.772\n",
      "    update_time_ms: 1.467\n",
      "  timestamp: 1643386210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 290\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:13 (running for 00:41:45.14)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         2490.86</td><td style=\"text-align: right;\">1160000</td><td style=\"text-align: right;\">-0.0193319</td><td style=\"text-align: right;\">             8.14996</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:18 (running for 00:41:50.15)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         2490.86</td><td style=\"text-align: right;\">1160000</td><td style=\"text-align: right;\">-0.0193319</td><td style=\"text-align: right;\">             8.14996</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-10-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.149964988231659\n",
      "  episode_reward_mean: -0.011052714586257935\n",
      "  episode_reward_min: -11.229356408119202\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7416167855262756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017439983125795318\n",
      "          policy_loss: -0.09966093229996141\n",
      "          total_loss: 2.3729571001436462\n",
      "          vf_explained_var: 0.9018966597254559\n",
      "          vf_loss: 2.446131050089995\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1164000\n",
      "    num_agent_steps_trained: 1164000\n",
      "    num_steps_sampled: 1164000\n",
      "    num_steps_trained: 1164000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.424999999999997\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1015636568968098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08918078459606281\n",
      "    mean_inference_ms: 0.9983824129638976\n",
      "    mean_raw_obs_processing_ms: 0.08408894390037026\n",
      "  time_since_restore: 2499.3672988414764\n",
      "  time_this_iter_s: 8.50405740737915\n",
      "  time_total_s: 2499.3672988414764\n",
      "  timers:\n",
      "    learn_throughput: 674.7\n",
      "    learn_time_ms: 5928.565\n",
      "    load_throughput: 14163964.542\n",
      "    load_time_ms: 0.282\n",
      "    sample_throughput: 469.393\n",
      "    sample_time_ms: 8521.64\n",
      "    update_time_ms: 1.434\n",
      "  timestamp: 1643386218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1164000\n",
      "  training_iteration: 291\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:24 (running for 00:41:55.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         2499.37</td><td style=\"text-align: right;\">1164000</td><td style=\"text-align: right;\">-0.0110527</td><td style=\"text-align: right;\">             8.14996</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.195808112621307\n",
      "  episode_reward_mean: -0.0395376518368721\n",
      "  episode_reward_min: -11.229356408119202\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.753567260952406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01689428613729052\n",
      "          policy_loss: -0.10270790241668702\n",
      "          total_loss: 2.2062462841666313\n",
      "          vf_explained_var: 0.9084043319507312\n",
      "          vf_loss: 2.2832959876906487\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1168000\n",
      "    num_agent_steps_trained: 1168000\n",
      "    num_steps_sampled: 1168000\n",
      "    num_steps_trained: 1168000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.450000000000003\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10155773832278371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08918079773762999\n",
      "    mean_inference_ms: 0.9983277105453198\n",
      "    mean_raw_obs_processing_ms: 0.08408302753276332\n",
      "  time_since_restore: 2507.6732034683228\n",
      "  time_this_iter_s: 8.305904626846313\n",
      "  time_total_s: 2507.6732034683228\n",
      "  timers:\n",
      "    learn_throughput: 674.247\n",
      "    learn_time_ms: 5932.545\n",
      "    load_throughput: 13936879.88\n",
      "    load_time_ms: 0.287\n",
      "    sample_throughput: 469.243\n",
      "    sample_time_ms: 8524.364\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1643386227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1168000\n",
      "  training_iteration: 292\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:29 (running for 00:42:01.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         2507.67</td><td style=\"text-align: right;\">1168000</td><td style=\"text-align: right;\">-0.0395377</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:34 (running for 00:42:06.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         2507.67</td><td style=\"text-align: right;\">1168000</td><td style=\"text-align: right;\">-0.0395377</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -11.2294</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-10-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.195808112621307\n",
      "  episode_reward_mean: 0.10753182023763656\n",
      "  episode_reward_min: -8.97789354622364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7589708828156994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017018924129036272\n",
      "          policy_loss: -0.0978341893493248\n",
      "          total_loss: 1.5673518566044187\n",
      "          vf_explained_var: 0.9292026341602366\n",
      "          vf_loss: 1.6393385497953301\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1172000\n",
      "    num_agent_steps_trained: 1172000\n",
      "    num_steps_sampled: 1172000\n",
      "    num_steps_trained: 1172000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.441666666666666\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10156699159971407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08918736610829928\n",
      "    mean_inference_ms: 0.9983680530009272\n",
      "    mean_raw_obs_processing_ms: 0.0840854701196142\n",
      "  time_since_restore: 2516.463649749756\n",
      "  time_this_iter_s: 8.790446281433105\n",
      "  time_total_s: 2516.463649749756\n",
      "  timers:\n",
      "    learn_throughput: 675.677\n",
      "    learn_time_ms: 5919.988\n",
      "    load_throughput: 13668906.632\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 467.921\n",
      "    sample_time_ms: 8548.456\n",
      "    update_time_ms: 1.453\n",
      "  timestamp: 1643386236\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1172000\n",
      "  training_iteration: 293\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:40 (running for 00:42:11.81)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         2516.46</td><td style=\"text-align: right;\">1172000</td><td style=\"text-align: right;\">0.107532</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -8.97789</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.195808112621307\n",
      "  episode_reward_mean: -0.04843995146453381\n",
      "  episode_reward_min: -8.97789354622364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6773851767663033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01725041413697353\n",
      "          policy_loss: -0.10733206563088442\n",
      "          total_loss: 2.3726017618126245\n",
      "          vf_explained_var: 0.8952126522858938\n",
      "          vf_loss: 2.453734768406358\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1176000\n",
      "    num_agent_steps_trained: 1176000\n",
      "    num_steps_sampled: 1176000\n",
      "    num_steps_trained: 1176000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.775000000000002\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1015822266303532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08919639052573704\n",
      "    mean_inference_ms: 0.9984286251222027\n",
      "    mean_raw_obs_processing_ms: 0.08408802579421196\n",
      "  time_since_restore: 2524.8389995098114\n",
      "  time_this_iter_s: 8.375349760055542\n",
      "  time_total_s: 2524.8389995098114\n",
      "  timers:\n",
      "    learn_throughput: 678.282\n",
      "    learn_time_ms: 5897.248\n",
      "    load_throughput: 13641122.042\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 469.811\n",
      "    sample_time_ms: 8514.058\n",
      "    update_time_ms: 1.479\n",
      "  timestamp: 1643386244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1176000\n",
      "  training_iteration: 294\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:45 (running for 00:42:17.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         2524.84</td><td style=\"text-align: right;\">1176000</td><td style=\"text-align: right;\">-0.04844</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -8.97789</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:50 (running for 00:42:22.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         2524.84</td><td style=\"text-align: right;\">1176000</td><td style=\"text-align: right;\">-0.04844</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -8.97789</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-10-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.195808112621307\n",
      "  episode_reward_mean: -0.011775140166282654\n",
      "  episode_reward_min: -7.337023854255676\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6400740770883457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01828040547834281\n",
      "          policy_loss: -0.1023089472665101\n",
      "          total_loss: 1.7051814376829713\n",
      "          vf_explained_var: 0.928648728132248\n",
      "          vf_loss: 1.779727023295177\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1180000\n",
      "    num_agent_steps_trained: 1180000\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.125\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159057357266828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08920314309434751\n",
      "    mean_inference_ms: 0.998423894387198\n",
      "    mean_raw_obs_processing_ms: 0.08408456428172303\n",
      "  time_since_restore: 2533.2732927799225\n",
      "  time_this_iter_s: 8.434293270111084\n",
      "  time_total_s: 2533.2732927799225\n",
      "  timers:\n",
      "    learn_throughput: 679.54\n",
      "    learn_time_ms: 5886.335\n",
      "    load_throughput: 12637252.184\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 470.363\n",
      "    sample_time_ms: 8504.069\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1643386252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 295\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:10:55 (running for 00:42:27.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         2533.27</td><td style=\"text-align: right;\">1180000</td><td style=\"text-align: right;\">-0.0117751</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -7.33702</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:01 (running for 00:42:32.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         2533.27</td><td style=\"text-align: right;\">1180000</td><td style=\"text-align: right;\">-0.0117751</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -7.33702</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.195808112621307\n",
      "  episode_reward_mean: 0.040821868479251865\n",
      "  episode_reward_min: -7.420561134815216\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.603722865991695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017641911020842102\n",
      "          policy_loss: -0.1151190371417831\n",
      "          total_loss: 1.9987586924661032\n",
      "          vf_explained_var: 0.8995959268462274\n",
      "          vf_loss: 2.0870840881460455\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 1184000\n",
      "    num_steps_trained: 1184000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.507692307692306\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159770070425214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0892088745959366\n",
      "    mean_inference_ms: 0.9984242140364603\n",
      "    mean_raw_obs_processing_ms: 0.08408047218232303\n",
      "  time_since_restore: 2542.1339304447174\n",
      "  time_this_iter_s: 8.860637664794922\n",
      "  time_total_s: 2542.1339304447174\n",
      "  timers:\n",
      "    learn_throughput: 674.487\n",
      "    learn_time_ms: 5930.435\n",
      "    load_throughput: 12619192.178\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 470.037\n",
      "    sample_time_ms: 8509.971\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1643386261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1184000\n",
      "  training_iteration: 296\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:06 (running for 00:42:38.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         2542.13</td><td style=\"text-align: right;\">1184000</td><td style=\"text-align: right;\">0.0408219</td><td style=\"text-align: right;\">             10.1958</td><td style=\"text-align: right;\">            -7.42056</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-11-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.104264795780182\n",
      "  episode_reward_mean: 0.004537646770477295\n",
      "  episode_reward_min: -7.420561134815216\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5708490911350454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0174755468471075\n",
      "          policy_loss: -0.12475927203533149\n",
      "          total_loss: 1.1686633485351858\n",
      "          vf_explained_var: 0.9443597091782477\n",
      "          vf_loss: 1.2668816387252781\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1188000\n",
      "    num_agent_steps_trained: 1188000\n",
      "    num_steps_sampled: 1188000\n",
      "    num_steps_trained: 1188000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.025\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10161294715646438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0892215983554704\n",
      "    mean_inference_ms: 0.9985116411110946\n",
      "    mean_raw_obs_processing_ms: 0.08408255715319886\n",
      "  time_since_restore: 2550.800880432129\n",
      "  time_this_iter_s: 8.666949987411499\n",
      "  time_total_s: 2550.800880432129\n",
      "  timers:\n",
      "    learn_throughput: 671.97\n",
      "    learn_time_ms: 5952.643\n",
      "    load_throughput: 12438623.962\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 466.438\n",
      "    sample_time_ms: 8575.626\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643386270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1188000\n",
      "  training_iteration: 297\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:12 (running for 00:42:44.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">          2550.8</td><td style=\"text-align: right;\">1188000</td><td style=\"text-align: right;\">0.00453765</td><td style=\"text-align: right;\">             8.10426</td><td style=\"text-align: right;\">            -7.42056</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:17 (running for 00:42:49.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">          2550.8</td><td style=\"text-align: right;\">1188000</td><td style=\"text-align: right;\">0.00453765</td><td style=\"text-align: right;\">             8.10426</td><td style=\"text-align: right;\">            -7.42056</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-11-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.104264795780182\n",
      "  episode_reward_mean: -0.039376763105392454\n",
      "  episode_reward_min: -8.988054037094116\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5509550265086594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0172252707306045\n",
      "          policy_loss: -0.11107617023561189\n",
      "          total_loss: 2.550191515619286\n",
      "          vf_explained_var: 0.8991720659758455\n",
      "          vf_loss: 2.6351068145885903\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1192000\n",
      "    num_agent_steps_trained: 1192000\n",
      "    num_steps_sampled: 1192000\n",
      "    num_steps_trained: 1192000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.75384615384615\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10162076463546553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923295665341828\n",
      "    mean_inference_ms: 0.9985915994791799\n",
      "    mean_raw_obs_processing_ms: 0.08408642408904013\n",
      "  time_since_restore: 2559.586085796356\n",
      "  time_this_iter_s: 8.785205364227295\n",
      "  time_total_s: 2559.586085796356\n",
      "  timers:\n",
      "    learn_throughput: 670.444\n",
      "    learn_time_ms: 5966.199\n",
      "    load_throughput: 12562497.941\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 463.422\n",
      "    sample_time_ms: 8631.446\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1643386279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1192000\n",
      "  training_iteration: 298\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:23 (running for 00:42:55.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         2559.59</td><td style=\"text-align: right;\">1192000</td><td style=\"text-align: right;\">-0.0393768</td><td style=\"text-align: right;\">             8.10426</td><td style=\"text-align: right;\">            -8.98805</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:28 (running for 00:43:00.06)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         2559.59</td><td style=\"text-align: right;\">1192000</td><td style=\"text-align: right;\">-0.0393768</td><td style=\"text-align: right;\">             8.10426</td><td style=\"text-align: right;\">            -8.98805</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-11-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.538603901863098\n",
      "  episode_reward_mean: 0.002216169461607933\n",
      "  episode_reward_min: -8.988054037094116\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7036335740038144\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01782507600019706\n",
      "          policy_loss: -0.10958059200596425\n",
      "          total_loss: 1.5989014030043636\n",
      "          vf_explained_var: 0.9339574092177935\n",
      "          vf_loss: 1.6814101599397198\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1196000\n",
      "    num_agent_steps_trained: 1196000\n",
      "    num_steps_sampled: 1196000\n",
      "    num_steps_trained: 1196000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.0125\n",
      "    ram_util_percent: 34.1\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10162269381577652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0892454338194206\n",
      "    mean_inference_ms: 0.9986829861703619\n",
      "    mean_raw_obs_processing_ms: 0.08409319119931151\n",
      "  time_since_restore: 2570.6098001003265\n",
      "  time_this_iter_s: 11.023714303970337\n",
      "  time_total_s: 2570.6098001003265\n",
      "  timers:\n",
      "    learn_throughput: 647.19\n",
      "    learn_time_ms: 6180.563\n",
      "    load_throughput: 12496995.158\n",
      "    load_time_ms: 0.32\n",
      "    sample_throughput: 462.174\n",
      "    sample_time_ms: 8654.752\n",
      "    update_time_ms: 1.427\n",
      "  timestamp: 1643386290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1196000\n",
      "  training_iteration: 299\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:33 (running for 00:43:05.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         2570.61</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\">0.00221617</td><td style=\"text-align: right;\">              7.5386</td><td style=\"text-align: right;\">            -8.98805</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:38 (running for 00:43:10.15)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         2570.61</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\">0.00221617</td><td style=\"text-align: right;\">              7.5386</td><td style=\"text-align: right;\">            -8.98805</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:43 (running for 00:43:15.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         2570.61</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\">0.00221617</td><td style=\"text-align: right;\">              7.5386</td><td style=\"text-align: right;\">            -8.98805</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-11-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.538603901863098\n",
      "  episode_reward_mean: 0.06039901286363602\n",
      "  episode_reward_min: -8.988054037094116\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7402413984780671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01720633401750864\n",
      "          policy_loss: -0.08966896693392466\n",
      "          total_loss: 1.3561637451406567\n",
      "          vf_explained_var: 0.9384809244063592\n",
      "          vf_loss: 1.419700595808606\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.368181818181814\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10174877297060515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08936152737161268\n",
      "    mean_inference_ms: 1.0000610714889322\n",
      "    mean_raw_obs_processing_ms: 0.08421222814302969\n",
      "  time_since_restore: 2586.325477361679\n",
      "  time_this_iter_s: 15.715677261352539\n",
      "  time_total_s: 2586.325477361679\n",
      "  timers:\n",
      "    learn_throughput: 624.96\n",
      "    learn_time_ms: 6400.411\n",
      "    load_throughput: 11108532.08\n",
      "    load_time_ms: 0.36\n",
      "    sample_throughput: 427.762\n",
      "    sample_time_ms: 9350.984\n",
      "    update_time_ms: 1.428\n",
      "  timestamp: 1643386306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 300\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:49 (running for 00:43:20.85)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         2586.33</td><td style=\"text-align: right;\">1200000</td><td style=\"text-align: right;\">0.060399</td><td style=\"text-align: right;\">              7.5386</td><td style=\"text-align: right;\">            -8.98805</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:54 (running for 00:43:25.86)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         2586.33</td><td style=\"text-align: right;\">1200000</td><td style=\"text-align: right;\">0.060399</td><td style=\"text-align: right;\">              7.5386</td><td style=\"text-align: right;\">            -8.98805</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-11-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.384083062410355\n",
      "  episode_reward_mean: -0.04271977126598358\n",
      "  episode_reward_min: -9.473950415849686\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6798910728064917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017801039109226074\n",
      "          policy_loss: -0.08799408273671264\n",
      "          total_loss: 2.7007841851867695\n",
      "          vf_explained_var: 0.8851926720270547\n",
      "          vf_loss: 2.761742940849514\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1204000\n",
      "    num_agent_steps_trained: 1204000\n",
      "    num_steps_sampled: 1204000\n",
      "    num_steps_trained: 1204000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.625\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188078094560568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08948017535116916\n",
      "    mean_inference_ms: 1.001457126780533\n",
      "    mean_raw_obs_processing_ms: 0.08433538900432584\n",
      "  time_since_restore: 2594.7801566123962\n",
      "  time_this_iter_s: 8.454679250717163\n",
      "  time_total_s: 2594.7801566123962\n",
      "  timers:\n",
      "    learn_throughput: 626.566\n",
      "    learn_time_ms: 6384.0\n",
      "    load_throughput: 10986324.406\n",
      "    load_time_ms: 0.364\n",
      "    sample_throughput: 417.434\n",
      "    sample_time_ms: 9582.343\n",
      "    update_time_ms: 1.448\n",
      "  timestamp: 1643386314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1204000\n",
      "  training_iteration: 301\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:11:59 (running for 00:43:31.33)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         2594.78</td><td style=\"text-align: right;\">1204000</td><td style=\"text-align: right;\">-0.0427198</td><td style=\"text-align: right;\">             7.38408</td><td style=\"text-align: right;\">            -9.47395</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.384083062410355\n",
      "  episode_reward_mean: -0.02789902538061142\n",
      "  episode_reward_min: -11.81888684630394\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7774729862008043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016354048207355715\n",
      "          policy_loss: -0.1255328370979236\n",
      "          total_loss: 1.971100026918375\n",
      "          vf_explained_var: 0.915267793273413\n",
      "          vf_loss: 2.0717951622541233\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1208000\n",
      "    num_agent_steps_trained: 1208000\n",
      "    num_steps_sampled: 1208000\n",
      "    num_steps_trained: 1208000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.623076923076926\n",
      "    ram_util_percent: 34.0923076923077\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10201186549169787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08959699760323579\n",
      "    mean_inference_ms: 1.0028093691858317\n",
      "    mean_raw_obs_processing_ms: 0.0844567573954263\n",
      "  time_since_restore: 2603.3400118350983\n",
      "  time_this_iter_s: 8.559855222702026\n",
      "  time_total_s: 2603.3400118350983\n",
      "  timers:\n",
      "    learn_throughput: 626.182\n",
      "    learn_time_ms: 6387.919\n",
      "    load_throughput: 11110003.311\n",
      "    load_time_ms: 0.36\n",
      "    sample_throughput: 417.201\n",
      "    sample_time_ms: 9587.709\n",
      "    update_time_ms: 1.451\n",
      "  timestamp: 1643386323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1208000\n",
      "  training_iteration: 302\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:05 (running for 00:43:36.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         2603.34</td><td style=\"text-align: right;\">1208000</td><td style=\"text-align: right;\">-0.027899</td><td style=\"text-align: right;\">             7.38408</td><td style=\"text-align: right;\">            -11.8189</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:10 (running for 00:43:41.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         2603.34</td><td style=\"text-align: right;\">1208000</td><td style=\"text-align: right;\">-0.027899</td><td style=\"text-align: right;\">             7.38408</td><td style=\"text-align: right;\">            -11.8189</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-12-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.384083062410355\n",
      "  episode_reward_mean: -0.09098041743040085\n",
      "  episode_reward_min: -11.81888684630394\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7522330791719498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016222893848098732\n",
      "          policy_loss: -0.12565381716215804\n",
      "          total_loss: 1.7138167247544194\n",
      "          vf_explained_var: 0.9336224220773225\n",
      "          vf_loss: 1.814832026006714\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1212000\n",
      "    num_agent_steps_trained: 1212000\n",
      "    num_steps_sampled: 1212000\n",
      "    num_steps_trained: 1212000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.649999999999995\n",
      "    ram_util_percent: 34.05833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10214309928775332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08971838577462676\n",
      "    mean_inference_ms: 1.0041534163702786\n",
      "    mean_raw_obs_processing_ms: 0.08457589736722523\n",
      "  time_since_restore: 2612.1460959911346\n",
      "  time_this_iter_s: 8.806084156036377\n",
      "  time_total_s: 2612.1460959911346\n",
      "  timers:\n",
      "    learn_throughput: 625.684\n",
      "    learn_time_ms: 6392.999\n",
      "    load_throughput: 11324479.244\n",
      "    load_time_ms: 0.353\n",
      "    sample_throughput: 417.178\n",
      "    sample_time_ms: 9588.243\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1643386332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1212000\n",
      "  training_iteration: 303\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:16 (running for 00:43:47.74)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         2612.15</td><td style=\"text-align: right;\">1212000</td><td style=\"text-align: right;\">-0.0909804</td><td style=\"text-align: right;\">             7.38408</td><td style=\"text-align: right;\">            -11.8189</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-12-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.897625654935837\n",
      "  episode_reward_mean: 0.10632511496543884\n",
      "  episode_reward_min: -11.81888684630394\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7684234890886532\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018037804478144705\n",
      "          policy_loss: -0.08930801376700401\n",
      "          total_loss: 3.2282727407912413\n",
      "          vf_explained_var: 0.8931133027999631\n",
      "          vf_loss: 3.2901858292119477\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1216000\n",
      "    num_agent_steps_trained: 1216000\n",
      "    num_steps_sampled: 1216000\n",
      "    num_steps_trained: 1216000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.133333333333336\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10227188955742723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983552614815428\n",
      "    mean_inference_ms: 1.0054518296913466\n",
      "    mean_raw_obs_processing_ms: 0.08469075474811431\n",
      "  time_since_restore: 2620.4861011505127\n",
      "  time_this_iter_s: 8.340005159378052\n",
      "  time_total_s: 2620.4861011505127\n",
      "  timers:\n",
      "    learn_throughput: 624.995\n",
      "    learn_time_ms: 6400.05\n",
      "    load_throughput: 11369758.742\n",
      "    load_time_ms: 0.352\n",
      "    sample_throughput: 417.421\n",
      "    sample_time_ms: 9582.645\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1643386340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1216000\n",
      "  training_iteration: 304\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:21 (running for 00:43:53.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         2620.49</td><td style=\"text-align: right;\">1216000</td><td style=\"text-align: right;\">0.106325</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -11.8189</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:26 (running for 00:43:58.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         2620.49</td><td style=\"text-align: right;\">1216000</td><td style=\"text-align: right;\">0.106325</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -11.8189</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-12-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.897625654935837\n",
      "  episode_reward_mean: -0.054631869792938235\n",
      "  episode_reward_min: -11.81888684630394\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7260918971030943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01790542611927507\n",
      "          policy_loss: -0.09499729233643701\n",
      "          total_loss: 2.800050966868988\n",
      "          vf_explained_var: 0.8896271177517471\n",
      "          vf_loss: 2.8678543868885247\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1220000\n",
      "    num_agent_steps_trained: 1220000\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.533333333333335\n",
      "    ram_util_percent: 34.050000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10227515384161481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.089843290865968\n",
      "    mean_inference_ms: 1.0054375002700149\n",
      "    mean_raw_obs_processing_ms: 0.08469124768963798\n",
      "  time_since_restore: 2628.8633975982666\n",
      "  time_this_iter_s: 8.377296447753906\n",
      "  time_total_s: 2628.8633975982666\n",
      "  timers:\n",
      "    learn_throughput: 624.543\n",
      "    learn_time_ms: 6404.679\n",
      "    load_throughput: 12203386.674\n",
      "    load_time_ms: 0.328\n",
      "    sample_throughput: 417.575\n",
      "    sample_time_ms: 9579.128\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643386348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 305\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:31 (running for 00:44:03.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         2628.86</td><td style=\"text-align: right;\">1220000</td><td style=\"text-align: right;\">-0.0546319</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -11.8189</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:36 (running for 00:44:08.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         2628.86</td><td style=\"text-align: right;\">1220000</td><td style=\"text-align: right;\">-0.0546319</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -11.8189</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-12-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.897625654935837\n",
      "  episode_reward_mean: 0.026196921467781065\n",
      "  episode_reward_min: -12.59706699848175\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.769083797675307\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017232804047132674\n",
      "          policy_loss: -0.10896398601052101\n",
      "          total_loss: 2.3712242252373645\n",
      "          vf_explained_var: 0.8993364400761101\n",
      "          vf_loss: 2.4540158982719142\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1224000\n",
      "    num_agent_steps_trained: 1224000\n",
      "    num_steps_sampled: 1224000\n",
      "    num_steps_trained: 1224000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.308333333333334\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10226838309523632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.089844891481509\n",
      "    mean_inference_ms: 1.0053541637199543\n",
      "    mean_raw_obs_processing_ms: 0.08468427196288139\n",
      "  time_since_restore: 2637.185353755951\n",
      "  time_this_iter_s: 8.321956157684326\n",
      "  time_total_s: 2637.185353755951\n",
      "  timers:\n",
      "    learn_throughput: 628.07\n",
      "    learn_time_ms: 6368.717\n",
      "    load_throughput: 12024954.128\n",
      "    load_time_ms: 0.333\n",
      "    sample_throughput: 418.169\n",
      "    sample_time_ms: 9565.521\n",
      "    update_time_ms: 1.406\n",
      "  timestamp: 1643386357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1224000\n",
      "  training_iteration: 306\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:42 (running for 00:44:13.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         2637.19</td><td style=\"text-align: right;\">1224000</td><td style=\"text-align: right;\">0.0261969</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-12-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.897625654935837\n",
      "  episode_reward_mean: 0.09096600323915481\n",
      "  episode_reward_min: -12.59706699848175\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8439774981109045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01672968329218449\n",
      "          policy_loss: -0.0743322854721418\n",
      "          total_loss: 1.8166083897201104\n",
      "          vf_explained_var: 0.9311695575073201\n",
      "          vf_loss: 1.8655324684836532\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1228000\n",
      "    num_agent_steps_trained: 1228000\n",
      "    num_steps_sampled: 1228000\n",
      "    num_steps_trained: 1228000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.675\n",
      "    ram_util_percent: 34.050000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10225791434694056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984445805169561\n",
      "    mean_inference_ms: 1.0052559751157792\n",
      "    mean_raw_obs_processing_ms: 0.08467808335294866\n",
      "  time_since_restore: 2645.620362997055\n",
      "  time_this_iter_s: 8.435009241104126\n",
      "  time_total_s: 2645.620362997055\n",
      "  timers:\n",
      "    learn_throughput: 628.455\n",
      "    learn_time_ms: 6364.814\n",
      "    load_throughput: 12169749.021\n",
      "    load_time_ms: 0.329\n",
      "    sample_throughput: 420.61\n",
      "    sample_time_ms: 9509.987\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1643386365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1228000\n",
      "  training_iteration: 307\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:47 (running for 00:44:19.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         2645.62</td><td style=\"text-align: right;\">1228000</td><td style=\"text-align: right;\">0.090966</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:52 (running for 00:44:24.31)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         2645.62</td><td style=\"text-align: right;\">1228000</td><td style=\"text-align: right;\">0.090966</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-12-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.897625654935837\n",
      "  episode_reward_mean: 0.10256587862968444\n",
      "  episode_reward_min: -12.59706699848175\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7637651803672956\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017461283027848957\n",
      "          policy_loss: -0.09268868609621961\n",
      "          total_loss: 1.280474053031104\n",
      "          vf_explained_var: 0.9283400966916033\n",
      "          vf_loss: 1.346643424570881\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1232000\n",
      "    num_agent_steps_trained: 1232000\n",
      "    num_steps_sampled: 1232000\n",
      "    num_steps_trained: 1232000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.883333333333336\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10224202856375424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983536595430223\n",
      "    mean_inference_ms: 1.005083900290936\n",
      "    mean_raw_obs_processing_ms: 0.08466602988857218\n",
      "  time_since_restore: 2654.294426679611\n",
      "  time_this_iter_s: 8.674063682556152\n",
      "  time_total_s: 2654.294426679611\n",
      "  timers:\n",
      "    learn_throughput: 626.711\n",
      "    learn_time_ms: 6382.524\n",
      "    load_throughput: 12188315.292\n",
      "    load_time_ms: 0.328\n",
      "    sample_throughput: 422.057\n",
      "    sample_time_ms: 9477.386\n",
      "    update_time_ms: 1.458\n",
      "  timestamp: 1643386374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1232000\n",
      "  training_iteration: 308\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:12:58 (running for 00:44:30.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         2654.29</td><td style=\"text-align: right;\">1232000</td><td style=\"text-align: right;\">0.102566</td><td style=\"text-align: right;\">             8.89763</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.490485668182373\n",
      "  episode_reward_mean: -0.12541815131902695\n",
      "  episode_reward_min: -12.59706699848175\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6552480156703662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017654021920434707\n",
      "          policy_loss: -0.12079036221388847\n",
      "          total_loss: 1.1385467757236822\n",
      "          vf_explained_var: 0.9484432616541463\n",
      "          vf_loss: 1.2325250971341326\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1236000\n",
      "    num_agent_steps_trained: 1236000\n",
      "    num_steps_sampled: 1236000\n",
      "    num_steps_trained: 1236000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.823076923076922\n",
      "    ram_util_percent: 34.01538461538461\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10222809884387456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983020541526834\n",
      "    mean_inference_ms: 1.004942043000315\n",
      "    mean_raw_obs_processing_ms: 0.08466091394780692\n",
      "  time_since_restore: 2663.0252673625946\n",
      "  time_this_iter_s: 8.730840682983398\n",
      "  time_total_s: 2663.0252673625946\n",
      "  timers:\n",
      "    learn_throughput: 649.783\n",
      "    learn_time_ms: 6155.897\n",
      "    load_throughput: 12036169.022\n",
      "    load_time_ms: 0.332\n",
      "    sample_throughput: 421.389\n",
      "    sample_time_ms: 9492.412\n",
      "    update_time_ms: 1.4\n",
      "  timestamp: 1643386383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1236000\n",
      "  training_iteration: 309\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:04 (running for 00:44:35.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         2663.03</td><td style=\"text-align: right;\">1236000</td><td style=\"text-align: right;\">-0.125418</td><td style=\"text-align: right;\">             8.49049</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:09 (running for 00:44:40.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         2663.03</td><td style=\"text-align: right;\">1236000</td><td style=\"text-align: right;\">-0.125418</td><td style=\"text-align: right;\">             8.49049</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-13-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.490485668182373\n",
      "  episode_reward_mean: 0.024322756826877595\n",
      "  episode_reward_min: -12.59706699848175\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6160247595079484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01735952414843989\n",
      "          policy_loss: -0.09134542074264779\n",
      "          total_loss: 0.7509576066296988\n",
      "          vf_explained_var: 0.9602269825114998\n",
      "          vf_loss: 0.8159382465906361\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1240000\n",
      "    num_agent_steps_trained: 1240000\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.808333333333334\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10221607147420345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08982644243966079\n",
      "    mean_inference_ms: 1.0048224266964407\n",
      "    mean_raw_obs_processing_ms: 0.08465814923120712\n",
      "  time_since_restore: 2671.3234038352966\n",
      "  time_this_iter_s: 8.298136472702026\n",
      "  time_total_s: 2671.3234038352966\n",
      "  timers:\n",
      "    learn_throughput: 675.667\n",
      "    learn_time_ms: 5920.076\n",
      "    load_throughput: 13598002.918\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 456.724\n",
      "    sample_time_ms: 8758.028\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1643386391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 310\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:14 (running for 00:44:46.07)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         2671.32</td><td style=\"text-align: right;\">1240000</td><td style=\"text-align: right;\">0.0243228</td><td style=\"text-align: right;\">             8.49049</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:19 (running for 00:44:51.08)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         2671.32</td><td style=\"text-align: right;\">1240000</td><td style=\"text-align: right;\">0.0243228</td><td style=\"text-align: right;\">             8.49049</td><td style=\"text-align: right;\">            -12.5971</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-13-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.756495416164398\n",
      "  episode_reward_mean: 0.01220656454563141\n",
      "  episode_reward_min: -9.600199222564697\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6104761850449347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017501805935063054\n",
      "          policy_loss: -0.07252527810533041\n",
      "          total_loss: 1.4389925381183744\n",
      "          vf_explained_var: 0.9324892570895533\n",
      "          vf_loss: 1.4849369404376835\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1244000\n",
      "    num_agent_steps_trained: 1244000\n",
      "    num_steps_sampled: 1244000\n",
      "    num_steps_trained: 1244000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.208333333333332\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10221386979842027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983185289005352\n",
      "    mean_inference_ms: 1.0047777917734806\n",
      "    mean_raw_obs_processing_ms: 0.08466268682889472\n",
      "  time_since_restore: 2679.8728036880493\n",
      "  time_this_iter_s: 8.549399852752686\n",
      "  time_total_s: 2679.8728036880493\n",
      "  timers:\n",
      "    learn_throughput: 675.115\n",
      "    learn_time_ms: 5924.92\n",
      "    load_throughput: 13652222.313\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 469.105\n",
      "    sample_time_ms: 8526.877\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1643386399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1244000\n",
      "  training_iteration: 311\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:24 (running for 00:44:56.65)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         2679.87</td><td style=\"text-align: right;\">1244000</td><td style=\"text-align: right;\">0.0122066</td><td style=\"text-align: right;\">              7.7565</td><td style=\"text-align: right;\">             -9.6002</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.756495416164398\n",
      "  episode_reward_mean: -0.10659291684627532\n",
      "  episode_reward_min: -7.123957097530365\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5911204107346073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017159076183797695\n",
      "          policy_loss: -0.12335171336858904\n",
      "          total_loss: 1.5991944401076825\n",
      "          vf_explained_var: 0.91831336630288\n",
      "          vf_loss: 1.6964857952248666\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1248000\n",
      "    num_agent_steps_trained: 1248000\n",
      "    num_steps_sampled: 1248000\n",
      "    num_steps_trained: 1248000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.15833333333333\n",
      "    ram_util_percent: 34.00833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10221138342977212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983649343455846\n",
      "    mean_inference_ms: 1.0047305336737253\n",
      "    mean_raw_obs_processing_ms: 0.08466353229633299\n",
      "  time_since_restore: 2688.1260821819305\n",
      "  time_this_iter_s: 8.253278493881226\n",
      "  time_total_s: 2688.1260821819305\n",
      "  timers:\n",
      "    learn_throughput: 677.471\n",
      "    learn_time_ms: 5904.31\n",
      "    load_throughput: 13596900.883\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 469.408\n",
      "    sample_time_ms: 8521.378\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1643386408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1248000\n",
      "  training_iteration: 312\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:30 (running for 00:45:01.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         2688.13</td><td style=\"text-align: right;\">1248000</td><td style=\"text-align: right;\">-0.106593</td><td style=\"text-align: right;\">              7.7565</td><td style=\"text-align: right;\">            -7.12396</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:35 (running for 00:45:06.93)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         2688.13</td><td style=\"text-align: right;\">1248000</td><td style=\"text-align: right;\">-0.106593</td><td style=\"text-align: right;\">              7.7565</td><td style=\"text-align: right;\">            -7.12396</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-13-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.530701279640198\n",
      "  episode_reward_mean: -0.005588092207908631\n",
      "  episode_reward_min: -6.424078434705734\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5157447573959186\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01866066022369361\n",
      "          policy_loss: -0.11972326757486469\n",
      "          total_loss: 2.4348575829868757\n",
      "          vf_explained_var: 0.9038409711212241\n",
      "          vf_loss: 2.52623996530329\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1252000\n",
      "    num_agent_steps_trained: 1252000\n",
      "    num_steps_sampled: 1252000\n",
      "    num_steps_trained: 1252000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.850000000000005\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10220998505454534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984215828121293\n",
      "    mean_inference_ms: 1.0047104196701375\n",
      "    mean_raw_obs_processing_ms: 0.08466492707241305\n",
      "  time_since_restore: 2696.8343205451965\n",
      "  time_this_iter_s: 8.708238363265991\n",
      "  time_total_s: 2696.8343205451965\n",
      "  timers:\n",
      "    learn_throughput: 676.36\n",
      "    learn_time_ms: 5914.007\n",
      "    load_throughput: 13620081.182\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 471.641\n",
      "    sample_time_ms: 8481.027\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1643386416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1252000\n",
      "  training_iteration: 313\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:40 (running for 00:45:12.66)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         2696.83</td><td style=\"text-align: right;\">1252000</td><td style=\"text-align: right;\">-0.00558809</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -6.42408</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.530701279640198\n",
      "  episode_reward_mean: 0.02904009371995926\n",
      "  episode_reward_min: -9.989164978265762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.4927243544209388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0176222310492264\n",
      "          policy_loss: -0.11362490301090543\n",
      "          total_loss: 1.6568027154542506\n",
      "          vf_explained_var: 0.9288288306805396\n",
      "          vf_loss: 1.7436638500301107\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1256000\n",
      "    num_agent_steps_trained: 1256000\n",
      "    num_steps_sampled: 1256000\n",
      "    num_steps_trained: 1256000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.36666666666667\n",
      "    ram_util_percent: 34.050000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10220523128791933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984307455792326\n",
      "    mean_inference_ms: 1.0046525233423709\n",
      "    mean_raw_obs_processing_ms: 0.084658975740175\n",
      "  time_since_restore: 2705.0969150066376\n",
      "  time_this_iter_s: 8.26259446144104\n",
      "  time_total_s: 2705.0969150066376\n",
      "  timers:\n",
      "    learn_throughput: 676.707\n",
      "    learn_time_ms: 5910.977\n",
      "    load_throughput: 13654444.535\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 471.321\n",
      "    sample_time_ms: 8486.791\n",
      "    update_time_ms: 1.535\n",
      "  timestamp: 1643386425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1256000\n",
      "  training_iteration: 314\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:46 (running for 00:45:17.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">          2705.1</td><td style=\"text-align: right;\">1256000</td><td style=\"text-align: right;\">0.0290401</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -9.98916</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:51 (running for 00:45:22.95)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">          2705.1</td><td style=\"text-align: right;\">1256000</td><td style=\"text-align: right;\">0.0290401</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -9.98916</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.530701279640198\n",
      "  episode_reward_mean: -0.02170928508043289\n",
      "  episode_reward_min: -9.989164978265762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5145909578569474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017932925742090763\n",
      "          policy_loss: -0.11259964908853734\n",
      "          total_loss: 2.0573659283840047\n",
      "          vf_explained_var: 0.8955379621316027\n",
      "          vf_loss: 2.142729941659397\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1260000\n",
      "    num_agent_steps_trained: 1260000\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.325\n",
      "    ram_util_percent: 34.050000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10220046750996485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984670854910143\n",
      "    mean_inference_ms: 1.0045688713427965\n",
      "    mean_raw_obs_processing_ms: 0.08465071564346388\n",
      "  time_since_restore: 2713.253179550171\n",
      "  time_this_iter_s: 8.156264543533325\n",
      "  time_total_s: 2713.253179550171\n",
      "  timers:\n",
      "    learn_throughput: 679.252\n",
      "    learn_time_ms: 5888.833\n",
      "    load_throughput: 13519110.395\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 471.474\n",
      "    sample_time_ms: 8484.027\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1643386433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 315\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:13:56 (running for 00:45:28.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         2713.25</td><td style=\"text-align: right;\">1260000</td><td style=\"text-align: right;\">-0.0217093</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -9.98916</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:01 (running for 00:45:33.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         2713.25</td><td style=\"text-align: right;\">1260000</td><td style=\"text-align: right;\">-0.0217093</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -9.98916</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-14-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.530701279640198\n",
      "  episode_reward_mean: 0.006830958724021912\n",
      "  episode_reward_min: -9.989164978265762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3796677833603275\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01689554022860539\n",
      "          policy_loss: -0.09622447564797376\n",
      "          total_loss: 2.322990813389701\n",
      "          vf_explained_var: 0.8954043046761585\n",
      "          vf_loss: 2.393555192409023\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1264000\n",
      "    num_agent_steps_trained: 1264000\n",
      "    num_steps_sampled: 1264000\n",
      "    num_steps_trained: 1264000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.49166666666667\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10218524702711296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984072979161467\n",
      "    mean_inference_ms: 1.0044015814020575\n",
      "    mean_raw_obs_processing_ms: 0.08463444787999429\n",
      "  time_since_restore: 2721.5421228408813\n",
      "  time_this_iter_s: 8.28894329071045\n",
      "  time_total_s: 2721.5421228408813\n",
      "  timers:\n",
      "    learn_throughput: 679.156\n",
      "    learn_time_ms: 5889.659\n",
      "    load_throughput: 13699041.398\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 472.945\n",
      "    sample_time_ms: 8457.647\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1643386441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1264000\n",
      "  training_iteration: 316\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:06 (running for 00:45:38.43)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         2721.54</td><td style=\"text-align: right;\">1264000</td><td style=\"text-align: right;\">0.00683096</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -9.98916</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-14-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.530701279640198\n",
      "  episode_reward_mean: 0.02371251419186592\n",
      "  episode_reward_min: -9.989164978265762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3134421988200116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017829637000305834\n",
      "          policy_loss: -0.10237460670021352\n",
      "          total_loss: 1.7154170562973827\n",
      "          vf_explained_var: 0.9301955477524829\n",
      "          vf_loss: 1.790712894883848\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1268000\n",
      "    num_agent_steps_trained: 1268000\n",
      "    num_steps_sampled: 1268000\n",
      "    num_steps_trained: 1268000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.553846153846152\n",
      "    ram_util_percent: 34.03846153846154\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10217854455659685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984077594323132\n",
      "    mean_inference_ms: 1.0043247274793115\n",
      "    mean_raw_obs_processing_ms: 0.08462617527154087\n",
      "  time_since_restore: 2730.853818178177\n",
      "  time_this_iter_s: 9.311695337295532\n",
      "  time_total_s: 2730.853818178177\n",
      "  timers:\n",
      "    learn_throughput: 672.976\n",
      "    learn_time_ms: 5943.752\n",
      "    load_throughput: 12593616.574\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 471.015\n",
      "    sample_time_ms: 8492.299\n",
      "    update_time_ms: 1.496\n",
      "  timestamp: 1643386451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1268000\n",
      "  training_iteration: 317\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:12 (running for 00:45:43.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         2730.85</td><td style=\"text-align: right;\">1268000</td><td style=\"text-align: right;\">0.0237125</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -9.98916</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:17 (running for 00:45:48.77)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         2730.85</td><td style=\"text-align: right;\">1268000</td><td style=\"text-align: right;\">0.0237125</td><td style=\"text-align: right;\">              8.5307</td><td style=\"text-align: right;\">            -9.98916</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-14-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.385435938835144\n",
      "  episode_reward_mean: 0.017932704240083693\n",
      "  episode_reward_min: -11.12443146109581\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3252888871777442\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01747191144281586\n",
      "          policy_loss: -0.09640587753446031\n",
      "          total_loss: 2.0131373333019926\n",
      "          vf_explained_var: 0.9185432200790733\n",
      "          vf_loss: 2.0830077521663197\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1272000\n",
      "    num_agent_steps_trained: 1272000\n",
      "    num_steps_sampled: 1272000\n",
      "    num_steps_trained: 1272000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.599999999999998\n",
      "    ram_util_percent: 34.050000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10217372949263803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984012364398505\n",
      "    mean_inference_ms: 1.004252367531941\n",
      "    mean_raw_obs_processing_ms: 0.08461885608631771\n",
      "  time_since_restore: 2739.5947971343994\n",
      "  time_this_iter_s: 8.740978956222534\n",
      "  time_total_s: 2739.5947971343994\n",
      "  timers:\n",
      "    learn_throughput: 673.55\n",
      "    learn_time_ms: 5938.682\n",
      "    load_throughput: 12580395.921\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 467.396\n",
      "    sample_time_ms: 8558.047\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1643386459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1272000\n",
      "  training_iteration: 318\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:22 (running for 00:45:54.53)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         2739.59</td><td style=\"text-align: right;\">1272000</td><td style=\"text-align: right;\">0.0179327</td><td style=\"text-align: right;\">             8.38544</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:27 (running for 00:45:59.53)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         2739.59</td><td style=\"text-align: right;\">1272000</td><td style=\"text-align: right;\">0.0179327</td><td style=\"text-align: right;\">             8.38544</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-14-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.344217091798782\n",
      "  episode_reward_mean: 0.06942672967910767\n",
      "  episode_reward_min: -11.12443146109581\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1863466844763808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018701806068903565\n",
      "          policy_loss: -0.09191023066780099\n",
      "          total_loss: 2.307971517316326\n",
      "          vf_explained_var: 0.9021320508372399\n",
      "          vf_loss: 2.371478371618576\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1276000\n",
      "    num_agent_steps_trained: 1276000\n",
      "    num_steps_sampled: 1276000\n",
      "    num_steps_trained: 1276000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.646153846153844\n",
      "    ram_util_percent: 34.00769230769231\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10217438478293567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984464094634652\n",
      "    mean_inference_ms: 1.0042133204787267\n",
      "    mean_raw_obs_processing_ms: 0.08461947688791177\n",
      "  time_since_restore: 2748.1457414627075\n",
      "  time_this_iter_s: 8.550944328308105\n",
      "  time_total_s: 2748.1457414627075\n",
      "  timers:\n",
      "    learn_throughput: 675.475\n",
      "    learn_time_ms: 5921.757\n",
      "    load_throughput: 12673527.723\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 467.728\n",
      "    sample_time_ms: 8551.986\n",
      "    update_time_ms: 1.519\n",
      "  timestamp: 1643386468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1276000\n",
      "  training_iteration: 319\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:33 (running for 00:46:05.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         2748.15</td><td style=\"text-align: right;\">1276000</td><td style=\"text-align: right;\">0.0694267</td><td style=\"text-align: right;\">             8.34422</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-14-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.001142144203186\n",
      "  episode_reward_mean: -0.06076501697301864\n",
      "  episode_reward_min: -11.12443146109581\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2196604594107596\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01729180015710072\n",
      "          policy_loss: -0.11951865659806357\n",
      "          total_loss: 2.8263131618810196\n",
      "          vf_explained_var: 0.9068361622031017\n",
      "          vf_loss: 2.9195699209487564\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1280000\n",
      "    num_agent_steps_trained: 1280000\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.075000000000003\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021739426757117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0898452110246772\n",
      "    mean_inference_ms: 1.0041886481658557\n",
      "    mean_raw_obs_processing_ms: 0.0846215542347667\n",
      "  time_since_restore: 2756.6048698425293\n",
      "  time_this_iter_s: 8.459128379821777\n",
      "  time_total_s: 2756.6048698425293\n",
      "  timers:\n",
      "    learn_throughput: 673.094\n",
      "    learn_time_ms: 5942.71\n",
      "    load_throughput: 12729298.938\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 468.932\n",
      "    sample_time_ms: 8530.022\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1643386476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 320\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:38 (running for 00:46:10.58)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">          2756.6</td><td style=\"text-align: right;\">1280000</td><td style=\"text-align: right;\">-0.060765</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:43 (running for 00:46:15.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">          2756.6</td><td style=\"text-align: right;\">1280000</td><td style=\"text-align: right;\">-0.060765</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-14-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.001142144203186\n",
      "  episode_reward_mean: -0.0004080027341842651\n",
      "  episode_reward_min: -11.12443146109581\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.143458471182854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01834600868979862\n",
      "          policy_loss: -0.10542180535373508\n",
      "          total_loss: 1.5774684753464234\n",
      "          vf_explained_var: 0.9156260090489541\n",
      "          vf_loss: 1.6550272816211307\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1284000\n",
      "    num_agent_steps_trained: 1284000\n",
      "    num_steps_sampled: 1284000\n",
      "    num_steps_trained: 1284000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.599999999999994\n",
      "    ram_util_percent: 34.016666666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021816912940491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985137597401813\n",
      "    mean_inference_ms: 1.0042440635950103\n",
      "    mean_raw_obs_processing_ms: 0.08463267119365912\n",
      "  time_since_restore: 2765.245491504669\n",
      "  time_this_iter_s: 8.640621662139893\n",
      "  time_total_s: 2765.245491504669\n",
      "  timers:\n",
      "    learn_throughput: 671.54\n",
      "    learn_time_ms: 5956.455\n",
      "    load_throughput: 12769992.388\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 468.047\n",
      "    sample_time_ms: 8546.155\n",
      "    update_time_ms: 1.496\n",
      "  timestamp: 1643386485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1284000\n",
      "  training_iteration: 321\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:49 (running for 00:46:21.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         2765.25</td><td style=\"text-align: right;\">1284000</td><td style=\"text-align: right;\">-0.000408003</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.001142144203186\n",
      "  episode_reward_mean: 0.08311086609959602\n",
      "  episode_reward_min: -11.12443146109581\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1129642773059107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01765411365877514\n",
      "          policy_loss: -0.08427718372875324\n",
      "          total_loss: 1.436521344499973\n",
      "          vf_explained_var: 0.9362189937663334\n",
      "          vf_loss: 1.4939863548083332\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1288000\n",
      "    num_agent_steps_trained: 1288000\n",
      "    num_steps_sampled: 1288000\n",
      "    num_steps_trained: 1288000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.658333333333335\n",
      "    ram_util_percent: 34.00833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021846474057979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985473166970075\n",
      "    mean_inference_ms: 1.0042433537377882\n",
      "    mean_raw_obs_processing_ms: 0.08464291592651861\n",
      "  time_since_restore: 2773.678005218506\n",
      "  time_this_iter_s: 8.43251371383667\n",
      "  time_total_s: 2773.678005218506\n",
      "  timers:\n",
      "    learn_throughput: 671.598\n",
      "    learn_time_ms: 5955.94\n",
      "    load_throughput: 12764163.116\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 466.298\n",
      "    sample_time_ms: 8578.211\n",
      "    update_time_ms: 1.504\n",
      "  timestamp: 1643386494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1288000\n",
      "  training_iteration: 322\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:14:55 (running for 00:46:26.70)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         2773.68</td><td style=\"text-align: right;\">1288000</td><td style=\"text-align: right;\">0.0831109</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:00 (running for 00:46:31.70)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         2773.68</td><td style=\"text-align: right;\">1288000</td><td style=\"text-align: right;\">0.0831109</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -11.1244</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-15-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.001142144203186\n",
      "  episode_reward_mean: 0.04581363454461098\n",
      "  episode_reward_min: -9.359244346618652\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.121047999910129\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018609044878990105\n",
      "          policy_loss: -0.08492685589302201\n",
      "          total_loss: 1.8298860562374435\n",
      "          vf_explained_var: 0.9188791597402224\n",
      "          vf_loss: 1.8865504247086342\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1292000\n",
      "    num_agent_steps_trained: 1292000\n",
      "    num_steps_sampled: 1292000\n",
      "    num_steps_trained: 1292000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.05\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10218318293585589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985406092398097\n",
      "    mean_inference_ms: 1.0041946259039056\n",
      "    mean_raw_obs_processing_ms: 0.08465052023676066\n",
      "  time_since_restore: 2781.9835579395294\n",
      "  time_this_iter_s: 8.30555272102356\n",
      "  time_total_s: 2781.9835579395294\n",
      "  timers:\n",
      "    learn_throughput: 674.169\n",
      "    learn_time_ms: 5933.234\n",
      "    load_throughput: 12779719.683\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 467.258\n",
      "    sample_time_ms: 8560.589\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1643386502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1292000\n",
      "  training_iteration: 323\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:05 (running for 00:46:37.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         2781.98</td><td style=\"text-align: right;\">1292000</td><td style=\"text-align: right;\">0.0458136</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -9.35924</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:10 (running for 00:46:42.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         2781.98</td><td style=\"text-align: right;\">1292000</td><td style=\"text-align: right;\">0.0458136</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -9.35924</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.001142144203186\n",
      "  episode_reward_mean: -0.039081494808197025\n",
      "  episode_reward_min: -9.359244346618652\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1802519100327646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016741642010335595\n",
      "          policy_loss: -0.10674609864551214\n",
      "          total_loss: 2.286851869152999\n",
      "          vf_explained_var: 0.9065750054774746\n",
      "          vf_loss: 2.3681716035490714\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1296000\n",
      "    num_agent_steps_trained: 1296000\n",
      "    num_steps_sampled: 1296000\n",
      "    num_steps_trained: 1296000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.775000000000002\n",
      "    ram_util_percent: 34.04166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10217994558168365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985396548195805\n",
      "    mean_inference_ms: 1.0041656924938973\n",
      "    mean_raw_obs_processing_ms: 0.08465312359740587\n",
      "  time_since_restore: 2790.2985429763794\n",
      "  time_this_iter_s: 8.314985036849976\n",
      "  time_total_s: 2790.2985429763794\n",
      "  timers:\n",
      "    learn_throughput: 675.876\n",
      "    learn_time_ms: 5918.246\n",
      "    load_throughput: 12757369.021\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 467.438\n",
      "    sample_time_ms: 8557.289\n",
      "    update_time_ms: 1.339\n",
      "  timestamp: 1643386510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1296000\n",
      "  training_iteration: 324\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:15 (running for 00:46:47.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">          2790.3</td><td style=\"text-align: right;\">1296000</td><td style=\"text-align: right;\">-0.0390815</td><td style=\"text-align: right;\">             9.00114</td><td style=\"text-align: right;\">            -9.35924</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-15-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.271583378314972\n",
      "  episode_reward_mean: 0.10089047729969025\n",
      "  episode_reward_min: -12.335390567779541\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2051173434462599\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018803795012981406\n",
      "          policy_loss: -0.09154334579424192\n",
      "          total_loss: 2.6560287656986046\n",
      "          vf_explained_var: 0.8922147077257915\n",
      "          vf_loss: 2.7190138360345237\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1300000\n",
      "    num_agent_steps_trained: 1300000\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.90833333333333\n",
      "    ram_util_percent: 34.01666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10217476426569519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985511160743322\n",
      "    mean_inference_ms: 1.004126342298896\n",
      "    mean_raw_obs_processing_ms: 0.08465673024652812\n",
      "  time_since_restore: 2798.513838291168\n",
      "  time_this_iter_s: 8.215295314788818\n",
      "  time_total_s: 2798.513838291168\n",
      "  timers:\n",
      "    learn_throughput: 675.277\n",
      "    learn_time_ms: 5923.499\n",
      "    load_throughput: 12864976.612\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 468.23\n",
      "    sample_time_ms: 8542.806\n",
      "    update_time_ms: 1.343\n",
      "  timestamp: 1643386518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 325\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:20 (running for 00:46:52.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         2798.51</td><td style=\"text-align: right;\">1300000</td><td style=\"text-align: right;\"> 0.10089</td><td style=\"text-align: right;\">             9.27158</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:25 (running for 00:46:57.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         2798.51</td><td style=\"text-align: right;\">1300000</td><td style=\"text-align: right;\"> 0.10089</td><td style=\"text-align: right;\">             9.27158</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-15-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.271583378314972\n",
      "  episode_reward_mean: -0.05117608994245529\n",
      "  episode_reward_min: -12.335390567779541\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.176230497962685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017841822063546658\n",
      "          policy_loss: -0.12801293077458056\n",
      "          total_loss: 0.651128995091459\n",
      "          vf_explained_var: 0.9531143890273186\n",
      "          vf_loss: 0.7520446546415809\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1304000\n",
      "    num_agent_steps_trained: 1304000\n",
      "    num_steps_sampled: 1304000\n",
      "    num_steps_trained: 1304000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.327272727272728\n",
      "    ram_util_percent: 34.06363636363637\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10216823704455011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985420612078719\n",
      "    mean_inference_ms: 1.0040511259085134\n",
      "    mean_raw_obs_processing_ms: 0.08465462579722292\n",
      "  time_since_restore: 2806.7491676807404\n",
      "  time_this_iter_s: 8.235329389572144\n",
      "  time_total_s: 2806.7491676807404\n",
      "  timers:\n",
      "    learn_throughput: 678.044\n",
      "    learn_time_ms: 5899.318\n",
      "    load_throughput: 12880780.038\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 466.922\n",
      "    sample_time_ms: 8566.734\n",
      "    update_time_ms: 1.434\n",
      "  timestamp: 1643386527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1304000\n",
      "  training_iteration: 326\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:31 (running for 00:47:02.86)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         2806.75</td><td style=\"text-align: right;\">1304000</td><td style=\"text-align: right;\">-0.0511761</td><td style=\"text-align: right;\">             9.27158</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-15-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.271583378314972\n",
      "  episode_reward_mean: -0.058729995787143704\n",
      "  episode_reward_min: -12.335390567779541\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.136301302268941\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017927323575589724\n",
      "          policy_loss: -0.0820592856194864\n",
      "          total_loss: 3.1203802893619224\n",
      "          vf_explained_var: 0.8853948094511545\n",
      "          vf_loss: 3.1752124696168846\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1308000\n",
      "    num_agent_steps_trained: 1308000\n",
      "    num_steps_sampled: 1308000\n",
      "    num_steps_trained: 1308000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.799999999999997\n",
      "    ram_util_percent: 34.03076923076923\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021593956555689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985264278347586\n",
      "    mean_inference_ms: 1.0039496733997593\n",
      "    mean_raw_obs_processing_ms: 0.08464798443591365\n",
      "  time_since_restore: 2815.2537035942078\n",
      "  time_this_iter_s: 8.504535913467407\n",
      "  time_total_s: 2815.2537035942078\n",
      "  timers:\n",
      "    learn_throughput: 683.697\n",
      "    learn_time_ms: 5850.545\n",
      "    load_throughput: 13967046.287\n",
      "    load_time_ms: 0.286\n",
      "    sample_throughput: 469.961\n",
      "    sample_time_ms: 8511.338\n",
      "    update_time_ms: 1.41\n",
      "  timestamp: 1643386535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1308000\n",
      "  training_iteration: 327\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:36 (running for 00:47:08.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         2815.25</td><td style=\"text-align: right;\">1308000</td><td style=\"text-align: right;\">-0.05873</td><td style=\"text-align: right;\">             9.27158</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:41 (running for 00:47:13.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         2815.25</td><td style=\"text-align: right;\">1308000</td><td style=\"text-align: right;\">-0.05873</td><td style=\"text-align: right;\">             9.27158</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-15-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.561481952667236\n",
      "  episode_reward_mean: -0.07665443122386932\n",
      "  episode_reward_min: -12.335390567779541\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0787388653524461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016820714023636475\n",
      "          policy_loss: -0.09081702364348276\n",
      "          total_loss: 2.194153800231945\n",
      "          vf_explained_var: 0.9124022130684186\n",
      "          vf_loss: 2.2594243682520365\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1312000\n",
      "    num_agent_steps_trained: 1312000\n",
      "    num_steps_sampled: 1312000\n",
      "    num_steps_trained: 1312000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.900000000000002\n",
      "    ram_util_percent: 34.0\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10214807573569282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985017778303261\n",
      "    mean_inference_ms: 1.00383147401849\n",
      "    mean_raw_obs_processing_ms: 0.08463923747402809\n",
      "  time_since_restore: 2823.595030784607\n",
      "  time_this_iter_s: 8.34132719039917\n",
      "  time_total_s: 2823.595030784607\n",
      "  timers:\n",
      "    learn_throughput: 685.206\n",
      "    learn_time_ms: 5837.659\n",
      "    load_throughput: 13647780.037\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 474.197\n",
      "    sample_time_ms: 8435.32\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1643386544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1312000\n",
      "  training_iteration: 328\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:47 (running for 00:47:18.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">          2823.6</td><td style=\"text-align: right;\">1312000</td><td style=\"text-align: right;\">-0.0766544</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:52 (running for 00:47:23.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">          2823.6</td><td style=\"text-align: right;\">1312000</td><td style=\"text-align: right;\">-0.0766544</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-15-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.561481952667236\n",
      "  episode_reward_mean: -0.03908167332410813\n",
      "  episode_reward_min: -12.335390567779541\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0496335437861821\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018784416755080552\n",
      "          policy_loss: -0.1247183043632396\n",
      "          total_loss: 2.86247442914874\n",
      "          vf_explained_var: 0.8955066728335555\n",
      "          vf_loss: 2.9586638931064835\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1316000\n",
      "    num_agent_steps_trained: 1316000\n",
      "    num_steps_sampled: 1316000\n",
      "    num_steps_trained: 1316000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.907692307692304\n",
      "    ram_util_percent: 34.02307692307692\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10214254198253123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984563848859956\n",
      "    mean_inference_ms: 1.0037045751467029\n",
      "    mean_raw_obs_processing_ms: 0.0846315484921054\n",
      "  time_since_restore: 2832.184230566025\n",
      "  time_this_iter_s: 8.589199781417847\n",
      "  time_total_s: 2832.184230566025\n",
      "  timers:\n",
      "    learn_throughput: 685.2\n",
      "    learn_time_ms: 5837.711\n",
      "    load_throughput: 13741679.089\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 474.714\n",
      "    sample_time_ms: 8426.126\n",
      "    update_time_ms: 1.348\n",
      "  timestamp: 1643386552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1316000\n",
      "  training_iteration: 329\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:15:57 (running for 00:47:29.37)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         2832.18</td><td style=\"text-align: right;\">1316000</td><td style=\"text-align: right;\">-0.0390817</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.3354</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.561481952667236\n",
      "  episode_reward_mean: -0.023725886046886444\n",
      "  episode_reward_min: -12.177893817424774\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.068118110202974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018211684557197395\n",
      "          policy_loss: -0.09959828814777034\n",
      "          total_loss: 2.289632550973986\n",
      "          vf_explained_var: 0.9108065398790504\n",
      "          vf_loss: 2.3615718403330415\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_agent_steps_trained: 1320000\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.833333333333332\n",
      "    ram_util_percent: 34.00833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10214262459410692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984031988821246\n",
      "    mean_inference_ms: 1.003603257106853\n",
      "    mean_raw_obs_processing_ms: 0.08462306363572139\n",
      "  time_since_restore: 2840.7074959278107\n",
      "  time_this_iter_s: 8.523265361785889\n",
      "  time_total_s: 2840.7074959278107\n",
      "  timers:\n",
      "    learn_throughput: 685.19\n",
      "    learn_time_ms: 5837.795\n",
      "    load_throughput: 13657779.225\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 474.348\n",
      "    sample_time_ms: 8432.627\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1643386561\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 330\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:03 (running for 00:47:34.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         2840.71</td><td style=\"text-align: right;\">1320000</td><td style=\"text-align: right;\">-0.0237259</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:08 (running for 00:47:39.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         2840.71</td><td style=\"text-align: right;\">1320000</td><td style=\"text-align: right;\">-0.0237259</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.561481952667236\n",
      "  episode_reward_mean: -0.006935078203678131\n",
      "  episode_reward_min: -12.177893817424774\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9794243288296525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0188617492676099\n",
      "          policy_loss: -0.11214658349391914\n",
      "          total_loss: 1.335754456875285\n",
      "          vf_explained_var: 0.9285384567834998\n",
      "          vf_loss: 1.4192547718203197\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1324000\n",
      "    num_agent_steps_trained: 1324000\n",
      "    num_steps_sampled: 1324000\n",
      "    num_steps_trained: 1324000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.46666666666667\n",
      "    ram_util_percent: 34.00833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021431556121577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983955268331421\n",
      "    mean_inference_ms: 1.0035250969746954\n",
      "    mean_raw_obs_processing_ms: 0.08461633709053498\n",
      "  time_since_restore: 2849.3064107894897\n",
      "  time_this_iter_s: 8.598914861679077\n",
      "  time_total_s: 2849.3064107894897\n",
      "  timers:\n",
      "    learn_throughput: 685.084\n",
      "    learn_time_ms: 5838.698\n",
      "    load_throughput: 13638904.154\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 474.623\n",
      "    sample_time_ms: 8427.74\n",
      "    update_time_ms: 1.354\n",
      "  timestamp: 1643386569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1324000\n",
      "  training_iteration: 331\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:13 (running for 00:47:45.54)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         2849.31</td><td style=\"text-align: right;\">1324000</td><td style=\"text-align: right;\">-0.00693508</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.561481952667236\n",
      "  episode_reward_mean: -0.06914607644081115\n",
      "  episode_reward_min: -12.177893817424774\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9885233217029161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01636899919396324\n",
      "          policy_loss: -0.12797202480379843\n",
      "          total_loss: 1.4105315322108987\n",
      "          vf_explained_var: 0.9276969577676506\n",
      "          vf_loss: 1.5136431492905142\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1328000\n",
      "    num_agent_steps_trained: 1328000\n",
      "    num_steps_sampled: 1328000\n",
      "    num_steps_trained: 1328000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.791666666666668\n",
      "    ram_util_percent: 34.04166666666668\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10214269511533523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983692132653755\n",
      "    mean_inference_ms: 1.0034582129366845\n",
      "    mean_raw_obs_processing_ms: 0.08461040362946147\n",
      "  time_since_restore: 2857.83464384079\n",
      "  time_this_iter_s: 8.528233051300049\n",
      "  time_total_s: 2857.83464384079\n",
      "  timers:\n",
      "    learn_throughput: 682.614\n",
      "    learn_time_ms: 5859.827\n",
      "    load_throughput: 13047061.202\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 475.206\n",
      "    sample_time_ms: 8417.409\n",
      "    update_time_ms: 1.364\n",
      "  timestamp: 1643386578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1328000\n",
      "  training_iteration: 332\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:19 (running for 00:47:51.08)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         2857.83</td><td style=\"text-align: right;\">1328000</td><td style=\"text-align: right;\">-0.0691461</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:24 (running for 00:47:56.09)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         2857.83</td><td style=\"text-align: right;\">1328000</td><td style=\"text-align: right;\">-0.0691461</td><td style=\"text-align: right;\">             9.56148</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.151018887758255\n",
      "  episode_reward_mean: 0.06082335472106934\n",
      "  episode_reward_min: -12.177893817424774\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9833934994154079\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019121387424712286\n",
      "          policy_loss: -0.07684955127857705\n",
      "          total_loss: 1.8817567601209126\n",
      "          vf_explained_var: 0.9208234301177404\n",
      "          vf_loss: 1.9295657088679652\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1332000\n",
      "    num_agent_steps_trained: 1332000\n",
      "    num_steps_sampled: 1332000\n",
      "    num_steps_trained: 1332000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.391666666666666\n",
      "    ram_util_percent: 34.05833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10214326466957054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08983478233588504\n",
      "    mean_inference_ms: 1.00340917323381\n",
      "    mean_raw_obs_processing_ms: 0.08460853286685129\n",
      "  time_since_restore: 2866.0721123218536\n",
      "  time_this_iter_s: 8.237468481063843\n",
      "  time_total_s: 2866.0721123218536\n",
      "  timers:\n",
      "    learn_throughput: 683.352\n",
      "    learn_time_ms: 5853.5\n",
      "    load_throughput: 12744770.586\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 474.052\n",
      "    sample_time_ms: 8437.901\n",
      "    update_time_ms: 1.391\n",
      "  timestamp: 1643386586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1332000\n",
      "  training_iteration: 333\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:29 (running for 00:48:01.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         2866.07</td><td style=\"text-align: right;\">1332000</td><td style=\"text-align: right;\">0.0608234</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:34 (running for 00:48:06.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         2866.07</td><td style=\"text-align: right;\">1332000</td><td style=\"text-align: right;\">0.0608234</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.151018887758255\n",
      "  episode_reward_mean: 0.09270310521125794\n",
      "  episode_reward_min: -12.177893817424774\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9655783023885501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018153580877948626\n",
      "          policy_loss: -0.0994402092711259\n",
      "          total_loss: 2.1793969562682776\n",
      "          vf_explained_var: 0.9015039880429545\n",
      "          vf_loss: 2.2512664172758337\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1336000\n",
      "    num_agent_steps_trained: 1336000\n",
      "    num_steps_sampled: 1336000\n",
      "    num_steps_trained: 1336000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.35\n",
      "    ram_util_percent: 34.03333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10213450913130526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08982840196866496\n",
      "    mean_inference_ms: 1.0033135699119804\n",
      "    mean_raw_obs_processing_ms: 0.08460208744245054\n",
      "  time_since_restore: 2874.3204765319824\n",
      "  time_this_iter_s: 8.248364210128784\n",
      "  time_total_s: 2874.3204765319824\n",
      "  timers:\n",
      "    learn_throughput: 681.458\n",
      "    learn_time_ms: 5869.766\n",
      "    load_throughput: 12643918.909\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 475.696\n",
      "    sample_time_ms: 8408.727\n",
      "    update_time_ms: 1.398\n",
      "  timestamp: 1643386594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1336000\n",
      "  training_iteration: 334\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:39 (running for 00:48:11.62)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         2874.32</td><td style=\"text-align: right;\">1336000</td><td style=\"text-align: right;\">0.0927031</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -12.1779</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.151018887758255\n",
      "  episode_reward_mean: 0.057297190725803374\n",
      "  episode_reward_min: -10.844403088092804\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9272560553525084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01750841655913165\n",
      "          policy_loss: -0.08809112106570073\n",
      "          total_loss: 2.459186844757047\n",
      "          vf_explained_var: 0.8873772860214274\n",
      "          vf_loss: 2.520687050336311\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1340000\n",
      "    num_agent_steps_trained: 1340000\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.091666666666665\n",
      "    ram_util_percent: 34.025000000000006\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1021204991157213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.089820228276796\n",
      "    mean_inference_ms: 1.0031878701831234\n",
      "    mean_raw_obs_processing_ms: 0.08459426982307182\n",
      "  time_since_restore: 2882.511565208435\n",
      "  time_this_iter_s: 8.191088676452637\n",
      "  time_total_s: 2882.511565208435\n",
      "  timers:\n",
      "    learn_throughput: 681.22\n",
      "    learn_time_ms: 5871.822\n",
      "    load_throughput: 12706161.769\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 475.018\n",
      "    sample_time_ms: 8420.726\n",
      "    update_time_ms: 1.417\n",
      "  timestamp: 1643386603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 335\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:45 (running for 00:48:16.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         2882.51</td><td style=\"text-align: right;\">1340000</td><td style=\"text-align: right;\">0.0572972</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -10.8444</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:50 (running for 00:48:21.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         2882.51</td><td style=\"text-align: right;\">1340000</td><td style=\"text-align: right;\">0.0572972</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -10.8444</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.151018887758255\n",
      "  episode_reward_mean: -0.012173397541046143\n",
      "  episode_reward_min: -10.844403088092804\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8813945105639837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018674334842726677\n",
      "          policy_loss: -0.1012022450706491\n",
      "          total_loss: 2.888267180321097\n",
      "          vf_explained_var: 0.8917584662155439\n",
      "          vf_loss: 2.9611077800553334\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1344000\n",
      "    num_agent_steps_trained: 1344000\n",
      "    num_steps_sampled: 1344000\n",
      "    num_steps_trained: 1344000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.566666666666666\n",
      "    ram_util_percent: 34.050000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10210623066131333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08980913829287389\n",
      "    mean_inference_ms: 1.0030441713103775\n",
      "    mean_raw_obs_processing_ms: 0.08458480354861393\n",
      "  time_since_restore: 2890.914954662323\n",
      "  time_this_iter_s: 8.40338945388794\n",
      "  time_total_s: 2890.914954662323\n",
      "  timers:\n",
      "    learn_throughput: 679.387\n",
      "    learn_time_ms: 5887.66\n",
      "    load_throughput: 12512840.095\n",
      "    load_time_ms: 0.32\n",
      "    sample_throughput: 474.803\n",
      "    sample_time_ms: 8424.552\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1643386611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1344000\n",
      "  training_iteration: 336\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:16:55 (running for 00:48:27.27)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         2890.91</td><td style=\"text-align: right;\">1344000</td><td style=\"text-align: right;\">-0.0121734</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -10.8444</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.151018887758255\n",
      "  episode_reward_mean: 0.10178578913211822\n",
      "  episode_reward_min: -10.844403088092804\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8671109650724678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01910991920465084\n",
      "          policy_loss: -0.12083845392698221\n",
      "          total_loss: 1.6684235929145967\n",
      "          vf_explained_var: 0.92338962766432\n",
      "          vf_loss: 1.7602388633755586\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1348000\n",
      "    num_agent_steps_trained: 1348000\n",
      "    num_steps_sampled: 1348000\n",
      "    num_steps_trained: 1348000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.275000000000002\n",
      "    ram_util_percent: 34.03333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10209325806420072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08979934060572253\n",
      "    mean_inference_ms: 1.002895640965156\n",
      "    mean_raw_obs_processing_ms: 0.08457468380832169\n",
      "  time_since_restore: 2899.2304680347443\n",
      "  time_this_iter_s: 8.315513372421265\n",
      "  time_total_s: 2899.2304680347443\n",
      "  timers:\n",
      "    learn_throughput: 681.75\n",
      "    learn_time_ms: 5867.251\n",
      "    load_throughput: 12433093.227\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 473.847\n",
      "    sample_time_ms: 8441.543\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1643386619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1348000\n",
      "  training_iteration: 337\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:00 (running for 00:48:32.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         2899.23</td><td style=\"text-align: right;\">1348000</td><td style=\"text-align: right;\">0.101786</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -10.8444</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:05 (running for 00:48:37.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         2899.23</td><td style=\"text-align: right;\">1348000</td><td style=\"text-align: right;\">0.101786</td><td style=\"text-align: right;\">             9.15102</td><td style=\"text-align: right;\">            -10.8444</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.119357943534851\n",
      "  episode_reward_mean: 0.02889423370361328\n",
      "  episode_reward_min: -10.844403088092804\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8344370487556663\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018173983049366214\n",
      "          policy_loss: -0.09089833684666182\n",
      "          total_loss: 1.6448905886644096\n",
      "          vf_explained_var: 0.913235237713783\n",
      "          vf_loss: 1.7081871945451024\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1352000\n",
      "    num_agent_steps_trained: 1352000\n",
      "    num_steps_sampled: 1352000\n",
      "    num_steps_trained: 1352000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.60833333333333\n",
      "    ram_util_percent: 34.01666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10208866082824648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0897936645718909\n",
      "    mean_inference_ms: 1.0027818701665907\n",
      "    mean_raw_obs_processing_ms: 0.08456618421874387\n",
      "  time_since_restore: 2907.962747812271\n",
      "  time_this_iter_s: 8.732279777526855\n",
      "  time_total_s: 2907.962747812271\n",
      "  timers:\n",
      "    learn_throughput: 680.107\n",
      "    learn_time_ms: 5881.431\n",
      "    load_throughput: 12651546.641\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 473.594\n",
      "    sample_time_ms: 8446.05\n",
      "    update_time_ms: 1.371\n",
      "  timestamp: 1643386628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1352000\n",
      "  training_iteration: 338\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:11 (running for 00:48:43.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         2907.96</td><td style=\"text-align: right;\">1352000</td><td style=\"text-align: right;\">0.0288942</td><td style=\"text-align: right;\">             9.11936</td><td style=\"text-align: right;\">            -10.8444</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:16 (running for 00:48:48.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         2907.96</td><td style=\"text-align: right;\">1352000</td><td style=\"text-align: right;\">0.0288942</td><td style=\"text-align: right;\">             9.11936</td><td style=\"text-align: right;\">            -10.8444</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-17-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.772300712764263\n",
      "  episode_reward_mean: -0.1506097474694252\n",
      "  episode_reward_min: -9.863758832216263\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8305268184151701\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017082863025117023\n",
      "          policy_loss: -0.12268100468221531\n",
      "          total_loss: 1.36540613994816\n",
      "          vf_explained_var: 0.9403556110397462\n",
      "          vf_loss: 1.4621425410872826\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1356000\n",
      "    num_agent_steps_trained: 1356000\n",
      "    num_steps_sampled: 1356000\n",
      "    num_steps_trained: 1356000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.325\n",
      "    ram_util_percent: 34.06666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10209033470467105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08979073143122619\n",
      "    mean_inference_ms: 1.0027156300496194\n",
      "    mean_raw_obs_processing_ms: 0.08456350614744093\n",
      "  time_since_restore: 2916.478000640869\n",
      "  time_this_iter_s: 8.515252828598022\n",
      "  time_total_s: 2916.478000640869\n",
      "  timers:\n",
      "    learn_throughput: 680.769\n",
      "    learn_time_ms: 5875.705\n",
      "    load_throughput: 12604023.74\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 472.897\n",
      "    sample_time_ms: 8458.508\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1643386637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1356000\n",
      "  training_iteration: 339\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:22 (running for 00:48:53.90)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         2916.48</td><td style=\"text-align: right;\">1356000</td><td style=\"text-align: right;\">-0.15061</td><td style=\"text-align: right;\">              8.7723</td><td style=\"text-align: right;\">            -9.86376</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.012998342514038\n",
      "  episode_reward_mean: -0.09659628480672837\n",
      "  episode_reward_min: -9.863758832216263\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7068967335006242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019330077297531167\n",
      "          policy_loss: -0.10640290844184096\n",
      "          total_loss: 2.508005496100222\n",
      "          vf_explained_var: 0.8947017259495232\n",
      "          vf_loss: 2.5850508364538354\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1360000\n",
      "    num_agent_steps_trained: 1360000\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.083333333333332\n",
      "    ram_util_percent: 34.025000000000006\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10209417676068476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08979164051281624\n",
      "    mean_inference_ms: 1.002678778050802\n",
      "    mean_raw_obs_processing_ms: 0.084562193309926\n",
      "  time_since_restore: 2924.834680557251\n",
      "  time_this_iter_s: 8.356679916381836\n",
      "  time_total_s: 2924.834680557251\n",
      "  timers:\n",
      "    learn_throughput: 682.606\n",
      "    learn_time_ms: 5859.896\n",
      "    load_throughput: 12322597.136\n",
      "    load_time_ms: 0.325\n",
      "    sample_throughput: 473.254\n",
      "    sample_time_ms: 8452.13\n",
      "    update_time_ms: 1.389\n",
      "  timestamp: 1643386645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 340\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:27 (running for 00:48:59.28)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         2924.83</td><td style=\"text-align: right;\">1360000</td><td style=\"text-align: right;\">-0.0965963</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -9.86376</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:32 (running for 00:49:04.28)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         2924.83</td><td style=\"text-align: right;\">1360000</td><td style=\"text-align: right;\">-0.0965963</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -9.86376</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-17-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.012998342514038\n",
      "  episode_reward_mean: 0.01625036135315895\n",
      "  episode_reward_min: -8.38625381886959\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7195291545762811\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01966036489904449\n",
      "          policy_loss: -0.11958119269730824\n",
      "          total_loss: 2.708237733902229\n",
      "          vf_explained_var: 0.9031342432704023\n",
      "          vf_loss: 2.7979597498332303\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1364000\n",
      "    num_agent_steps_trained: 1364000\n",
      "    num_steps_sampled: 1364000\n",
      "    num_steps_trained: 1364000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.75384615384615\n",
      "    ram_util_percent: 34.06923076923078\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10209872849760501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08979280546895142\n",
      "    mean_inference_ms: 1.0026696969047075\n",
      "    mean_raw_obs_processing_ms: 0.08456428905712145\n",
      "  time_since_restore: 2933.4419977664948\n",
      "  time_this_iter_s: 8.607317209243774\n",
      "  time_total_s: 2933.4419977664948\n",
      "  timers:\n",
      "    learn_throughput: 682.835\n",
      "    learn_time_ms: 5857.927\n",
      "    load_throughput: 12342541.014\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 473.985\n",
      "    sample_time_ms: 8439.085\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1643386654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1364000\n",
      "  training_iteration: 341\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:38 (running for 00:49:09.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         2933.44</td><td style=\"text-align: right;\">1364000</td><td style=\"text-align: right;\">0.0162504</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -8.38625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-17-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.012998342514038\n",
      "  episode_reward_mean: 0.03427809238433838\n",
      "  episode_reward_min: -8.38625381886959\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6776436068357959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01693991046997976\n",
      "          policy_loss: -0.09471645669596049\n",
      "          total_loss: 2.0224330302598257\n",
      "          vf_explained_var: 0.9280715475159307\n",
      "          vf_loss: 2.091421999770307\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1368000\n",
      "    num_agent_steps_trained: 1368000\n",
      "    num_steps_sampled: 1368000\n",
      "    num_steps_trained: 1368000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.900000000000002\n",
      "    ram_util_percent: 34.025\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10210128516050901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08979107579531487\n",
      "    mean_inference_ms: 1.0026469307220298\n",
      "    mean_raw_obs_processing_ms: 0.08456550659354438\n",
      "  time_since_restore: 2941.8735525608063\n",
      "  time_this_iter_s: 8.431554794311523\n",
      "  time_total_s: 2941.8735525608063\n",
      "  timers:\n",
      "    learn_throughput: 683.172\n",
      "    learn_time_ms: 5855.038\n",
      "    load_throughput: 12812903.62\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 474.477\n",
      "    sample_time_ms: 8430.331\n",
      "    update_time_ms: 1.379\n",
      "  timestamp: 1643386662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1368000\n",
      "  training_iteration: 342\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:43 (running for 00:49:15.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         2941.87</td><td style=\"text-align: right;\">1368000</td><td style=\"text-align: right;\">0.0342781</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -8.38625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:48 (running for 00:49:20.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         2941.87</td><td style=\"text-align: right;\">1368000</td><td style=\"text-align: right;\">0.0342781</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -8.38625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.012998342514038\n",
      "  episode_reward_mean: -0.04994417130947113\n",
      "  episode_reward_min: -8.38625381886959\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6241460354017314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018884485046085094\n",
      "          policy_loss: -0.09003689738981907\n",
      "          total_loss: 1.7193951387877164\n",
      "          vf_explained_var: 0.9165553881916949\n",
      "          vf_loss: 1.780751212934653\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1372000\n",
      "    num_agent_steps_trained: 1372000\n",
      "    num_steps_sampled: 1372000\n",
      "    num_steps_trained: 1372000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.541666666666668\n",
      "    ram_util_percent: 34.00833333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10209570677587472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08978613397154928\n",
      "    mean_inference_ms: 1.0025848650132907\n",
      "    mean_raw_obs_processing_ms: 0.08456223044757291\n",
      "  time_since_restore: 2950.251412153244\n",
      "  time_this_iter_s: 8.377859592437744\n",
      "  time_total_s: 2950.251412153244\n",
      "  timers:\n",
      "    learn_throughput: 681.277\n",
      "    learn_time_ms: 5871.327\n",
      "    load_throughput: 13112322.001\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 474.771\n",
      "    sample_time_ms: 8425.11\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1643386671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1372000\n",
      "  training_iteration: 343\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:54 (running for 00:49:25.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         2950.25</td><td style=\"text-align: right;\">1372000</td><td style=\"text-align: right;\">-0.0499442</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -8.38625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:17:59 (running for 00:49:30.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         2950.25</td><td style=\"text-align: right;\">1372000</td><td style=\"text-align: right;\">-0.0499442</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -8.38625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-17-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.012998342514038\n",
      "  episode_reward_mean: 0.14949622333049775\n",
      "  episode_reward_min: -8.38625381886959\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6286985363050174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018731482569705405\n",
      "          policy_loss: -0.07613850674061205\n",
      "          total_loss: 2.5628731038288706\n",
      "          vf_explained_var: 0.8886060791630899\n",
      "          vf_loss: 2.610563183447687\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1376000\n",
      "    num_agent_steps_trained: 1376000\n",
      "    num_steps_sampled: 1376000\n",
      "    num_steps_trained: 1376000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.28333333333333\n",
      "    ram_util_percent: 34.08333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.102083100192032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08977803143487031\n",
      "    mean_inference_ms: 1.0024678236277622\n",
      "    mean_raw_obs_processing_ms: 0.08455277670784117\n",
      "  time_since_restore: 2958.547473669052\n",
      "  time_this_iter_s: 8.296061515808105\n",
      "  time_total_s: 2958.547473669052\n",
      "  timers:\n",
      "    learn_throughput: 680.191\n",
      "    learn_time_ms: 5880.702\n",
      "    load_throughput: 13111297.28\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 474.112\n",
      "    sample_time_ms: 8436.822\n",
      "    update_time_ms: 1.389\n",
      "  timestamp: 1643386679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1376000\n",
      "  training_iteration: 344\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:04 (running for 00:49:36.08)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         2958.55</td><td style=\"text-align: right;\">1376000</td><td style=\"text-align: right;\">0.149496</td><td style=\"text-align: right;\">              11.013</td><td style=\"text-align: right;\">            -8.38625</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.740904092788696\n",
      "  episode_reward_mean: -0.0246596759557724\n",
      "  episode_reward_min: -11.01562151312828\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5905963460964861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017731927210711727\n",
      "          policy_loss: -0.12817674895538197\n",
      "          total_loss: 2.3428836317707895\n",
      "          vf_explained_var: 0.9088987308804707\n",
      "          vf_loss: 2.4441300175363017\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1380000\n",
      "    num_agent_steps_trained: 1380000\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.033333333333335\n",
      "    ram_util_percent: 34.025\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10207332912148427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08976964474181337\n",
      "    mean_inference_ms: 1.00237356534531\n",
      "    mean_raw_obs_processing_ms: 0.08454911418219986\n",
      "  time_since_restore: 2967.150515317917\n",
      "  time_this_iter_s: 8.603041648864746\n",
      "  time_total_s: 2967.150515317917\n",
      "  timers:\n",
      "    learn_throughput: 678.144\n",
      "    learn_time_ms: 5898.452\n",
      "    load_throughput: 13055183.254\n",
      "    load_time_ms: 0.306\n",
      "    sample_throughput: 472.282\n",
      "    sample_time_ms: 8469.517\n",
      "    update_time_ms: 1.376\n",
      "  timestamp: 1643386688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 345\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:10 (running for 00:49:41.71)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         2967.15</td><td style=\"text-align: right;\">1380000</td><td style=\"text-align: right;\">-0.0246597</td><td style=\"text-align: right;\">              9.7409</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:15 (running for 00:49:46.71)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         2967.15</td><td style=\"text-align: right;\">1380000</td><td style=\"text-align: right;\">-0.0246597</td><td style=\"text-align: right;\">              9.7409</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-18-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.314237147569656\n",
      "  episode_reward_mean: 0.0090123550593853\n",
      "  episode_reward_min: -11.01562151312828\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5876819547466052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019303676452218546\n",
      "          policy_loss: -0.1207876463686066\n",
      "          total_loss: 1.673956930897479\n",
      "          vf_explained_var: 0.9246323620119402\n",
      "          vf_loss: 1.7654271290587482\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1384000\n",
      "    num_agent_steps_trained: 1384000\n",
      "    num_steps_sampled: 1384000\n",
      "    num_steps_trained: 1384000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.4\n",
      "    ram_util_percent: 34.03333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10206161018260641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08976054385542208\n",
      "    mean_inference_ms: 1.0022813524057563\n",
      "    mean_raw_obs_processing_ms: 0.08454400887044339\n",
      "  time_since_restore: 2975.762081861496\n",
      "  time_this_iter_s: 8.611566543579102\n",
      "  time_total_s: 2975.762081861496\n",
      "  timers:\n",
      "    learn_throughput: 677.009\n",
      "    learn_time_ms: 5908.344\n",
      "    load_throughput: 13264718.533\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 470.694\n",
      "    sample_time_ms: 8498.087\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1643386696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1384000\n",
      "  training_iteration: 346\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:20 (running for 00:49:52.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         2975.76</td><td style=\"text-align: right;\">1384000</td><td style=\"text-align: right;\">0.00901236</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.314237147569656\n",
      "  episode_reward_mean: -0.0656743137538433\n",
      "  episode_reward_min: -11.01562151312828\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5709267159463257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019658076330352155\n",
      "          policy_loss: -0.11446916627427263\n",
      "          total_loss: 1.5581312605705593\n",
      "          vf_explained_var: 0.9315244394604878\n",
      "          vf_loss: 1.642744722569822\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1388000\n",
      "    num_agent_steps_trained: 1388000\n",
      "    num_steps_sampled: 1388000\n",
      "    num_steps_trained: 1388000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.858333333333334\n",
      "    ram_util_percent: 34.091666666666676\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10205361423851272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08975376517597958\n",
      "    mean_inference_ms: 1.0022163401489979\n",
      "    mean_raw_obs_processing_ms: 0.08453895565192564\n",
      "  time_since_restore: 2984.0246860980988\n",
      "  time_this_iter_s: 8.262604236602783\n",
      "  time_total_s: 2984.0246860980988\n",
      "  timers:\n",
      "    learn_throughput: 678.094\n",
      "    learn_time_ms: 5898.89\n",
      "    load_throughput: 13426069.142\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 469.914\n",
      "    sample_time_ms: 8512.188\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1643386704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1388000\n",
      "  training_iteration: 347\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:25 (running for 00:49:57.63)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         2984.02</td><td style=\"text-align: right;\">1388000</td><td style=\"text-align: right;\">-0.0656743</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:30 (running for 00:50:02.64)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         2984.02</td><td style=\"text-align: right;\">1388000</td><td style=\"text-align: right;\">-0.0656743</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.314237147569656\n",
      "  episode_reward_mean: -0.06661287069320679\n",
      "  episode_reward_min: -11.01562151312828\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5929775973481517\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019147466899645672\n",
      "          policy_loss: -0.1160625023356769\n",
      "          total_loss: 1.8677200343882945\n",
      "          vf_explained_var: 0.9119894136023777\n",
      "          vf_loss: 1.9547023240997585\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1392000\n",
      "    num_agent_steps_trained: 1392000\n",
      "    num_steps_sampled: 1392000\n",
      "    num_steps_trained: 1392000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.625\n",
      "    ram_util_percent: 34.050000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10204506972165298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974804991462486\n",
      "    mean_inference_ms: 1.0021612517872927\n",
      "    mean_raw_obs_processing_ms: 0.08453347161025311\n",
      "  time_since_restore: 2992.491199731827\n",
      "  time_this_iter_s: 8.466513633728027\n",
      "  time_total_s: 2992.491199731827\n",
      "  timers:\n",
      "    learn_throughput: 679.31\n",
      "    learn_time_ms: 5888.325\n",
      "    load_throughput: 13403543.98\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 471.318\n",
      "    sample_time_ms: 8486.844\n",
      "    update_time_ms: 1.379\n",
      "  timestamp: 1643386713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1392000\n",
      "  training_iteration: 348\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:36 (running for 00:50:08.12)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         2992.49</td><td style=\"text-align: right;\">1392000</td><td style=\"text-align: right;\">-0.0666129</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:41 (running for 00:50:13.13)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         2992.49</td><td style=\"text-align: right;\">1392000</td><td style=\"text-align: right;\">-0.0666129</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.314237147569656\n",
      "  episode_reward_mean: -0.063102695196867\n",
      "  episode_reward_min: -11.01562151312828\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6255841655436383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018373180251264512\n",
      "          policy_loss: -0.11284491216453413\n",
      "          total_loss: 1.1746066192735827\n",
      "          vf_explained_var: 0.9438430242640998\n",
      "          vf_loss: 1.2595472559012393\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1396000\n",
      "    num_agent_steps_trained: 1396000\n",
      "    num_steps_sampled: 1396000\n",
      "    num_steps_trained: 1396000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.941666666666666\n",
      "    ram_util_percent: 34.06666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10203865129235389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.089745671564157\n",
      "    mean_inference_ms: 1.0021368891871938\n",
      "    mean_raw_obs_processing_ms: 0.08453288427490817\n",
      "  time_since_restore: 3000.7216839790344\n",
      "  time_this_iter_s: 8.230484247207642\n",
      "  time_total_s: 3000.7216839790344\n",
      "  timers:\n",
      "    learn_throughput: 681.397\n",
      "    learn_time_ms: 5870.296\n",
      "    load_throughput: 13253192.195\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 472.483\n",
      "    sample_time_ms: 8465.911\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1643386721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1396000\n",
      "  training_iteration: 349\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:46 (running for 00:50:18.37)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         3000.72</td><td style=\"text-align: right;\">1396000</td><td style=\"text-align: right;\">-0.0631027</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -11.0156</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-18-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 11.314237147569656\n",
      "  episode_reward_mean: 0.0009776398539543152\n",
      "  episode_reward_min: -8.012372866272926\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6446997300271065\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017995183220320954\n",
      "          policy_loss: -0.126932261652884\n",
      "          total_loss: 1.6280060068659123\n",
      "          vf_explained_var: 0.9303458616938642\n",
      "          vf_loss: 1.7276080837813756\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_agent_steps_trained: 1400000\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.581818181818182\n",
      "    ram_util_percent: 34.02727272727273\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10203041599520744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974421065758019\n",
      "    mean_inference_ms: 1.0020786742103582\n",
      "    mean_raw_obs_processing_ms: 0.08452882012860759\n",
      "  time_since_restore: 3008.8691742420197\n",
      "  time_this_iter_s: 8.14749026298523\n",
      "  time_total_s: 3008.8691742420197\n",
      "  timers:\n",
      "    learn_throughput: 683.563\n",
      "    learn_time_ms: 5851.688\n",
      "    load_throughput: 13645559.984\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 473.642\n",
      "    sample_time_ms: 8445.195\n",
      "    update_time_ms: 1.408\n",
      "  timestamp: 1643386729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 350\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:51 (running for 00:50:23.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         3008.87</td><td style=\"text-align: right;\">1400000</td><td style=\"text-align: right;\">0.00097764</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -8.01237</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:18:56 (running for 00:50:28.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         3008.87</td><td style=\"text-align: right;\">1400000</td><td style=\"text-align: right;\">0.00097764</td><td style=\"text-align: right;\">             11.3142</td><td style=\"text-align: right;\">            -8.01237</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.182330876588821\n",
      "  episode_reward_mean: -0.07342967092990875\n",
      "  episode_reward_min: -7.251883491873741\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6490991334120433\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018601828257013845\n",
      "          policy_loss: -0.11259290917616298\n",
      "          total_loss: 2.248264254574659\n",
      "          vf_explained_var: 0.9073145415834202\n",
      "          vf_loss: 2.3326056436585483\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1404000\n",
      "    num_agent_steps_trained: 1404000\n",
      "    num_steps_sampled: 1404000\n",
      "    num_steps_trained: 1404000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.153846153846153\n",
      "    ram_util_percent: 34.0846153846154\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10202211885251032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974552733650666\n",
      "    mean_inference_ms: 1.0020012207801576\n",
      "    mean_raw_obs_processing_ms: 0.0845253144825312\n",
      "  time_since_restore: 3017.3417909145355\n",
      "  time_this_iter_s: 8.47261667251587\n",
      "  time_total_s: 3017.3417909145355\n",
      "  timers:\n",
      "    learn_throughput: 684.598\n",
      "    learn_time_ms: 5842.848\n",
      "    load_throughput: 13731556.72\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 474.938\n",
      "    sample_time_ms: 8422.157\n",
      "    update_time_ms: 1.434\n",
      "  timestamp: 1643386738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1404000\n",
      "  training_iteration: 351\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:02 (running for 00:50:34.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         3017.34</td><td style=\"text-align: right;\">1404000</td><td style=\"text-align: right;\">-0.0734297</td><td style=\"text-align: right;\">             8.18233</td><td style=\"text-align: right;\">            -7.25188</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-19-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.845998287200928\n",
      "  episode_reward_mean: -0.00970756247639656\n",
      "  episode_reward_min: -7.251883491873741\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.566626177936472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019304298734173355\n",
      "          policy_loss: -0.10597917777110612\n",
      "          total_loss: 1.1949539777685096\n",
      "          vf_explained_var: 0.9336029589176178\n",
      "          vf_loss: 1.2716147566434517\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1408000\n",
      "    num_agent_steps_trained: 1408000\n",
      "    num_steps_sampled: 1408000\n",
      "    num_steps_trained: 1408000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.272727272727273\n",
      "    ram_util_percent: 34.08181818181819\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10201073416850398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974722487825111\n",
      "    mean_inference_ms: 1.001916825975022\n",
      "    mean_raw_obs_processing_ms: 0.08452430248525783\n",
      "  time_since_restore: 3025.6025772094727\n",
      "  time_this_iter_s: 8.260786294937134\n",
      "  time_total_s: 3025.6025772094727\n",
      "  timers:\n",
      "    learn_throughput: 687.549\n",
      "    learn_time_ms: 5817.763\n",
      "    load_throughput: 13782318.245\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 474.98\n",
      "    sample_time_ms: 8421.408\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1643386746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1408000\n",
      "  training_iteration: 352\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:07 (running for 00:50:39.32)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">          3025.6</td><td style=\"text-align: right;\">1408000</td><td style=\"text-align: right;\">-0.00970756</td><td style=\"text-align: right;\">               7.846</td><td style=\"text-align: right;\">            -7.25188</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:12 (running for 00:50:44.33)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">          3025.6</td><td style=\"text-align: right;\">1408000</td><td style=\"text-align: right;\">-0.00970756</td><td style=\"text-align: right;\">               7.846</td><td style=\"text-align: right;\">            -7.25188</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-19-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.845998287200928\n",
      "  episode_reward_mean: 0.024510230422019958\n",
      "  episode_reward_min: -7.251883491873741\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5361561271291907\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018905796753278404\n",
      "          policy_loss: -0.12002500895691175\n",
      "          total_loss: 1.20984998098875\n",
      "          vf_explained_var: 0.9217498981183575\n",
      "          vf_loss: 1.3011618190955732\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1412000\n",
      "    num_agent_steps_trained: 1412000\n",
      "    num_steps_sampled: 1412000\n",
      "    num_steps_trained: 1412000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.383333333333336\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10200122114204824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974849906406707\n",
      "    mean_inference_ms: 1.0018425935898767\n",
      "    mean_raw_obs_processing_ms: 0.08452566220704127\n",
      "  time_since_restore: 3034.009953022003\n",
      "  time_this_iter_s: 8.407375812530518\n",
      "  time_total_s: 3034.009953022003\n",
      "  timers:\n",
      "    learn_throughput: 688.115\n",
      "    learn_time_ms: 5812.981\n",
      "    load_throughput: 13765356.088\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 475.953\n",
      "    sample_time_ms: 8404.185\n",
      "    update_time_ms: 1.418\n",
      "  timestamp: 1643386755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1412000\n",
      "  training_iteration: 353\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:18 (running for 00:50:49.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         3034.01</td><td style=\"text-align: right;\">1412000</td><td style=\"text-align: right;\">0.0245102</td><td style=\"text-align: right;\">               7.846</td><td style=\"text-align: right;\">            -7.25188</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:23 (running for 00:50:54.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         3034.01</td><td style=\"text-align: right;\">1412000</td><td style=\"text-align: right;\">0.0245102</td><td style=\"text-align: right;\">               7.846</td><td style=\"text-align: right;\">            -7.25188</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-19-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.845998287200928\n",
      "  episode_reward_mean: -0.035968528538942335\n",
      "  episode_reward_min: -9.514074072241783\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5046856689356989\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017932740617757546\n",
      "          policy_loss: -0.119836167558547\n",
      "          total_loss: 1.6093272946189128\n",
      "          vf_explained_var: 0.918485552200707\n",
      "          vf_loss: 1.7019281156601445\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1416000\n",
      "    num_agent_steps_trained: 1416000\n",
      "    num_steps_sampled: 1416000\n",
      "    num_steps_trained: 1416000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.684615384615388\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10199358102698863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0897501540264172\n",
      "    mean_inference_ms: 1.0017903272942326\n",
      "    mean_raw_obs_processing_ms: 0.08452650695351872\n",
      "  time_since_restore: 3042.575982093811\n",
      "  time_this_iter_s: 8.566029071807861\n",
      "  time_total_s: 3042.575982093811\n",
      "  timers:\n",
      "    learn_throughput: 687.805\n",
      "    learn_time_ms: 5815.605\n",
      "    load_throughput: 13582590.674\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 474.85\n",
      "    sample_time_ms: 8423.718\n",
      "    update_time_ms: 1.376\n",
      "  timestamp: 1643386763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1416000\n",
      "  training_iteration: 354\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:28 (running for 00:51:00.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         3042.58</td><td style=\"text-align: right;\">1416000</td><td style=\"text-align: right;\">-0.0359685</td><td style=\"text-align: right;\">               7.846</td><td style=\"text-align: right;\">            -9.51407</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.886431396007538\n",
      "  episode_reward_mean: 0.08102459967136383\n",
      "  episode_reward_min: -9.514074072241783\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5112003154511894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018473661683628777\n",
      "          policy_loss: -0.08557648133306253\n",
      "          total_loss: 2.806827438274844\n",
      "          vf_explained_var: 0.9029801712882134\n",
      "          vf_loss: 2.8643470662976465\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1420000\n",
      "    num_agent_steps_trained: 1420000\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.658333333333335\n",
      "    ram_util_percent: 34.075\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10198723172788465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974837332029204\n",
      "    mean_inference_ms: 1.0017427738191762\n",
      "    mean_raw_obs_processing_ms: 0.08452494113668393\n",
      "  time_since_restore: 3051.3095738887787\n",
      "  time_this_iter_s: 8.733591794967651\n",
      "  time_total_s: 3051.3095738887787\n",
      "  timers:\n",
      "    learn_throughput: 684.75\n",
      "    learn_time_ms: 5841.549\n",
      "    load_throughput: 13582590.674\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 475.435\n",
      "    sample_time_ms: 8413.347\n",
      "    update_time_ms: 1.389\n",
      "  timestamp: 1643386772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 355\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:34 (running for 00:51:06.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         3051.31</td><td style=\"text-align: right;\">1420000</td><td style=\"text-align: right;\">0.0810246</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.51407</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:39 (running for 00:51:11.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         3051.31</td><td style=\"text-align: right;\">1420000</td><td style=\"text-align: right;\">0.0810246</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.51407</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-19-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.886431396007538\n",
      "  episode_reward_mean: 0.024444809406995772\n",
      "  episode_reward_min: -9.93224549293518\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.4785813939026607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0198527871032749\n",
      "          policy_loss: -0.10367169579420919\n",
      "          total_loss: 3.280130166821282\n",
      "          vf_explained_var: 0.8713625706011249\n",
      "          vf_loss: 3.3536504274013863\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1424000\n",
      "    num_agent_steps_trained: 1424000\n",
      "    num_steps_sampled: 1424000\n",
      "    num_steps_trained: 1424000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.684615384615388\n",
      "    ram_util_percent: 34.03076923076923\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10198147056329425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974472776847829\n",
      "    mean_inference_ms: 1.00170651710757\n",
      "    mean_raw_obs_processing_ms: 0.08452270696804103\n",
      "  time_since_restore: 3059.938963651657\n",
      "  time_this_iter_s: 8.629389762878418\n",
      "  time_total_s: 3059.938963651657\n",
      "  timers:\n",
      "    learn_throughput: 684.21\n",
      "    learn_time_ms: 5846.162\n",
      "    load_throughput: 13487592.25\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 474.141\n",
      "    sample_time_ms: 8436.302\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1643386781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1424000\n",
      "  training_iteration: 356\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:45 (running for 00:51:16.75)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         3059.94</td><td style=\"text-align: right;\">1424000</td><td style=\"text-align: right;\">0.0244448</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-19-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.886431396007538\n",
      "  episode_reward_mean: 0.056926047801971434\n",
      "  episode_reward_min: -9.93224549293518\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.43440353680722493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019964837935949734\n",
      "          policy_loss: -0.10174117282693905\n",
      "          total_loss: 1.9676530761724358\n",
      "          vf_explained_var: 0.9122079132705606\n",
      "          vf_loss: 2.0390726540357837\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1428000\n",
      "    num_agent_steps_trained: 1428000\n",
      "    num_steps_sampled: 1428000\n",
      "    num_steps_trained: 1428000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.94166666666667\n",
      "    ram_util_percent: 34.05833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10197910510677129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974165991715825\n",
      "    mean_inference_ms: 1.0016875713996314\n",
      "    mean_raw_obs_processing_ms: 0.08451904812503357\n",
      "  time_since_restore: 3068.386962413788\n",
      "  time_this_iter_s: 8.447998762130737\n",
      "  time_total_s: 3068.386962413788\n",
      "  timers:\n",
      "    learn_throughput: 682.569\n",
      "    learn_time_ms: 5860.214\n",
      "    load_throughput: 13413188.359\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 473.646\n",
      "    sample_time_ms: 8445.124\n",
      "    update_time_ms: 1.406\n",
      "  timestamp: 1643386789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1428000\n",
      "  training_iteration: 357\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:50 (running for 00:51:22.22)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         3068.39</td><td style=\"text-align: right;\">1428000</td><td style=\"text-align: right;\">0.056926</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:19:55 (running for 00:51:27.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         3068.39</td><td style=\"text-align: right;\">1428000</td><td style=\"text-align: right;\">0.056926</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-19-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.886431396007538\n",
      "  episode_reward_mean: 0.04031318843364715\n",
      "  episode_reward_min: -9.93224549293518\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.35936458150866213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017956537908096\n",
      "          policy_loss: -0.10370132957174573\n",
      "          total_loss: 1.8171776820507441\n",
      "          vf_explained_var: 0.9063664909332029\n",
      "          vf_loss: 1.8936075183893404\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1432000\n",
      "    num_agent_steps_trained: 1432000\n",
      "    num_steps_sampled: 1432000\n",
      "    num_steps_trained: 1432000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.233333333333338\n",
      "    ram_util_percent: 34.025000000000006\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1019763819757313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08973953987744579\n",
      "    mean_inference_ms: 1.0016713368380934\n",
      "    mean_raw_obs_processing_ms: 0.0845136472371429\n",
      "  time_since_restore: 3076.7340893745422\n",
      "  time_this_iter_s: 8.347126960754395\n",
      "  time_total_s: 3076.7340893745422\n",
      "  timers:\n",
      "    learn_throughput: 684.547\n",
      "    learn_time_ms: 5843.277\n",
      "    load_throughput: 13341722.465\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 472.588\n",
      "    sample_time_ms: 8464.026\n",
      "    update_time_ms: 1.45\n",
      "  timestamp: 1643386797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1432000\n",
      "  training_iteration: 358\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:00 (running for 00:51:32.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         3076.73</td><td style=\"text-align: right;\">1432000</td><td style=\"text-align: right;\">0.0403132</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:05 (running for 00:51:37.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         3076.73</td><td style=\"text-align: right;\">1432000</td><td style=\"text-align: right;\">0.0403132</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-20-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.886431396007538\n",
      "  episode_reward_mean: 0.07010271936655045\n",
      "  episode_reward_min: -9.93224549293518\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.29977094372434\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0189415933205057\n",
      "          policy_loss: -0.11133619835600257\n",
      "          total_loss: 2.141750885178435\n",
      "          vf_explained_var: 0.8984472653558178\n",
      "          vf_loss: 2.224319537496695\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1436000\n",
      "    num_agent_steps_trained: 1436000\n",
      "    num_steps_sampled: 1436000\n",
      "    num_steps_trained: 1436000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.85\n",
      "    ram_util_percent: 34.025\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10197289571729208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974011052546146\n",
      "    mean_inference_ms: 1.001636396314993\n",
      "    mean_raw_obs_processing_ms: 0.08450750109491421\n",
      "  time_since_restore: 3085.1985907554626\n",
      "  time_this_iter_s: 8.46450138092041\n",
      "  time_total_s: 3085.1985907554626\n",
      "  timers:\n",
      "    learn_throughput: 682.096\n",
      "    learn_time_ms: 5864.274\n",
      "    load_throughput: 13292042.466\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 473.399\n",
      "    sample_time_ms: 8449.532\n",
      "    update_time_ms: 1.452\n",
      "  timestamp: 1643386806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1436000\n",
      "  training_iteration: 359\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:11 (running for 00:51:43.08)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">          3085.2</td><td style=\"text-align: right;\">1436000</td><td style=\"text-align: right;\">0.0701027</td><td style=\"text-align: right;\">             9.88643</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-20-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.987802237272263\n",
      "  episode_reward_mean: 0.03883078217506409\n",
      "  episode_reward_min: -9.93224549293518\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.30312659535807385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01808707043441067\n",
      "          policy_loss: -0.06800683011095571\n",
      "          total_loss: 1.6231036972760233\n",
      "          vf_explained_var: 0.9441880811286228\n",
      "          vf_loss: 1.6636407972904304\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_agent_steps_trained: 1440000\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.77272727272727\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10196841545758228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974335013822847\n",
      "    mean_inference_ms: 1.0016298557703411\n",
      "    mean_raw_obs_processing_ms: 0.084502598465766\n",
      "  time_since_restore: 3093.443834066391\n",
      "  time_this_iter_s: 8.245243310928345\n",
      "  time_total_s: 3093.443834066391\n",
      "  timers:\n",
      "    learn_throughput: 682.161\n",
      "    learn_time_ms: 5863.721\n",
      "    load_throughput: 13140050.125\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 471.645\n",
      "    sample_time_ms: 8480.951\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1643386814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 360\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:16 (running for 00:51:48.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         3093.44</td><td style=\"text-align: right;\">1440000</td><td style=\"text-align: right;\">0.0388308</td><td style=\"text-align: right;\">              8.9878</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:21 (running for 00:51:53.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         3093.44</td><td style=\"text-align: right;\">1440000</td><td style=\"text-align: right;\">0.0388308</td><td style=\"text-align: right;\">              8.9878</td><td style=\"text-align: right;\">            -9.93225</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.987802237272263\n",
      "  episode_reward_mean: 0.07617798164486884\n",
      "  episode_reward_min: -7.3959455490112305\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.28470837896269174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019786851025364188\n",
      "          policy_loss: -0.10715183918814986\n",
      "          total_loss: 1.7504593353337978\n",
      "          vf_explained_var: 0.9234584375094342\n",
      "          vf_loss: 1.827559883572081\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1444000\n",
      "    num_agent_steps_trained: 1444000\n",
      "    num_steps_sampled: 1444000\n",
      "    num_steps_trained: 1444000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.825\n",
      "    ram_util_percent: 34.07500000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10196321816916132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974441843585486\n",
      "    mean_inference_ms: 1.001597567709932\n",
      "    mean_raw_obs_processing_ms: 0.08449616031491602\n",
      "  time_since_restore: 3101.838226079941\n",
      "  time_this_iter_s: 8.394392013549805\n",
      "  time_total_s: 3101.838226079941\n",
      "  timers:\n",
      "    learn_throughput: 682.24\n",
      "    learn_time_ms: 5863.036\n",
      "    load_throughput: 13146227.864\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 472.079\n",
      "    sample_time_ms: 8473.155\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1643386823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1444000\n",
      "  training_iteration: 361\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:27 (running for 00:51:58.77)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         3101.84</td><td style=\"text-align: right;\">1444000</td><td style=\"text-align: right;\">0.076178</td><td style=\"text-align: right;\">              8.9878</td><td style=\"text-align: right;\">            -7.39595</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-20-31\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.483012035489082\n",
      "  episode_reward_mean: -0.02872856080532074\n",
      "  episode_reward_min: -9.231847137212753\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.23675973983781953\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01836715084159633\n",
      "          policy_loss: -0.1047759180452915\n",
      "          total_loss: 1.6912232887286502\n",
      "          vf_explained_var: 0.9182186789410088\n",
      "          vf_loss: 1.7681040945633124\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1448000\n",
      "    num_agent_steps_trained: 1448000\n",
      "    num_steps_sampled: 1448000\n",
      "    num_steps_trained: 1448000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.508333333333333\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10195520754440004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0897451168190172\n",
      "    mean_inference_ms: 1.0015485998692324\n",
      "    mean_raw_obs_processing_ms: 0.08448871316596147\n",
      "  time_since_restore: 3110.1547808647156\n",
      "  time_this_iter_s: 8.31655478477478\n",
      "  time_total_s: 3110.1547808647156\n",
      "  timers:\n",
      "    learn_throughput: 681.414\n",
      "    learn_time_ms: 5870.147\n",
      "    load_throughput: 13229156.284\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 472.207\n",
      "    sample_time_ms: 8470.859\n",
      "    update_time_ms: 1.389\n",
      "  timestamp: 1643386831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1448000\n",
      "  training_iteration: 362\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:32 (running for 00:52:04.10)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         3110.15</td><td style=\"text-align: right;\">1448000</td><td style=\"text-align: right;\">-0.0287286</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:37 (running for 00:52:09.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         3110.15</td><td style=\"text-align: right;\">1448000</td><td style=\"text-align: right;\">-0.0287286</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-20-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.483012035489082\n",
      "  episode_reward_mean: -0.03269794195890427\n",
      "  episode_reward_min: -9.231847137212753\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.17392562336779088\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01884810212947763\n",
      "          policy_loss: -0.12314209643405892\n",
      "          total_loss: 0.8387763353224884\n",
      "          vf_explained_var: 0.9523965489479803\n",
      "          vf_loss: 0.933292880409988\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1452000\n",
      "    num_agent_steps_trained: 1452000\n",
      "    num_steps_sampled: 1452000\n",
      "    num_steps_trained: 1452000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.075000000000003\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10195140093468615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974600572331451\n",
      "    mean_inference_ms: 1.0015080834117043\n",
      "    mean_raw_obs_processing_ms: 0.08448435733137505\n",
      "  time_since_restore: 3118.654802083969\n",
      "  time_this_iter_s: 8.50002121925354\n",
      "  time_total_s: 3118.654802083969\n",
      "  timers:\n",
      "    learn_throughput: 681.052\n",
      "    learn_time_ms: 5873.27\n",
      "    load_throughput: 13243776.445\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 471.478\n",
      "    sample_time_ms: 8483.966\n",
      "    update_time_ms: 1.424\n",
      "  timestamp: 1643386839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1452000\n",
      "  training_iteration: 363\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:42 (running for 00:52:14.63)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         3118.65</td><td style=\"text-align: right;\">1452000</td><td style=\"text-align: right;\">-0.0326979</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:47 (running for 00:52:19.64)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         3118.65</td><td style=\"text-align: right;\">1452000</td><td style=\"text-align: right;\">-0.0326979</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.483012035489082\n",
      "  episode_reward_mean: 0.06965221524238586\n",
      "  episode_reward_min: -9.231847137212753\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.13575544716810348\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01907356455747022\n",
      "          policy_loss: -0.07051972506908319\n",
      "          total_loss: 1.4608724199325567\n",
      "          vf_explained_var: 0.9378381900248989\n",
      "          vf_loss: 1.5024241648253895\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1456000\n",
      "    num_agent_steps_trained: 1456000\n",
      "    num_steps_sampled: 1456000\n",
      "    num_steps_trained: 1456000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.125\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10194634407916615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08974167023807167\n",
      "    mean_inference_ms: 1.0014586575137816\n",
      "    mean_raw_obs_processing_ms: 0.08447747231388238\n",
      "  time_since_restore: 3126.8883843421936\n",
      "  time_this_iter_s: 8.233582258224487\n",
      "  time_total_s: 3126.8883843421936\n",
      "  timers:\n",
      "    learn_throughput: 683.227\n",
      "    learn_time_ms: 5854.566\n",
      "    load_throughput: 13477840.617\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 472.095\n",
      "    sample_time_ms: 8472.865\n",
      "    update_time_ms: 1.471\n",
      "  timestamp: 1643386848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1456000\n",
      "  training_iteration: 364\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:53 (running for 00:52:24.88)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         3126.89</td><td style=\"text-align: right;\">1456000</td><td style=\"text-align: right;\">0.0696522</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-20-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.483012035489082\n",
      "  episode_reward_mean: -0.035813146233558656\n",
      "  episode_reward_min: -9.231847137212753\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.1257315231076572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01936455098862186\n",
      "          policy_loss: -0.08527825893208345\n",
      "          total_loss: 1.182496763010239\n",
      "          vf_explained_var: 0.9342590851809389\n",
      "          vf_loss: 1.2383651079309563\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1460000\n",
      "    num_agent_steps_trained: 1460000\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.95\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10193960460448691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0897344109697011\n",
      "    mean_inference_ms: 1.0013699085465138\n",
      "    mean_raw_obs_processing_ms: 0.08446877173499238\n",
      "  time_since_restore: 3134.9923479557037\n",
      "  time_this_iter_s: 8.103963613510132\n",
      "  time_total_s: 3134.9923479557037\n",
      "  timers:\n",
      "    learn_throughput: 689.851\n",
      "    learn_time_ms: 5798.352\n",
      "    load_throughput: 13477840.617\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 473.509\n",
      "    sample_time_ms: 8447.565\n",
      "    update_time_ms: 1.446\n",
      "  timestamp: 1643386856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 365\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:20:58 (running for 00:52:30.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         3134.99</td><td style=\"text-align: right;\">1460000</td><td style=\"text-align: right;\">-0.0358131</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:03 (running for 00:52:35.01)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         3134.99</td><td style=\"text-align: right;\">1460000</td><td style=\"text-align: right;\">-0.0358131</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-21-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.483012035489082\n",
      "  episode_reward_mean: -0.033386554718017575\n",
      "  episode_reward_min: -9.231847137212753\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.0776519313454628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019853746816417175\n",
      "          policy_loss: -0.10056401406745276\n",
      "          total_loss: 1.6159147666141351\n",
      "          vf_explained_var: 0.9258937950416278\n",
      "          vf_loss: 1.686325892125086\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1464000\n",
      "    num_agent_steps_trained: 1464000\n",
      "    num_steps_sampled: 1464000\n",
      "    num_steps_trained: 1464000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.15\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10193161608978425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08972763181673188\n",
      "    mean_inference_ms: 1.0012734266380745\n",
      "    mean_raw_obs_processing_ms: 0.08445905490036953\n",
      "  time_since_restore: 3143.4138462543488\n",
      "  time_this_iter_s: 8.42149829864502\n",
      "  time_total_s: 3143.4138462543488\n",
      "  timers:\n",
      "    learn_throughput: 690.506\n",
      "    learn_time_ms: 5792.856\n",
      "    load_throughput: 13406757.232\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 477.567\n",
      "    sample_time_ms: 8375.787\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1643386864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1464000\n",
      "  training_iteration: 366\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:08 (running for 00:52:40.46)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         3143.41</td><td style=\"text-align: right;\">1464000</td><td style=\"text-align: right;\">-0.0333866</td><td style=\"text-align: right;\">              10.483</td><td style=\"text-align: right;\">            -9.23185</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-21-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.291530027985573\n",
      "  episode_reward_mean: -0.04612898841500282\n",
      "  episode_reward_min: -6.661539360880852\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.07193537808914659\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019568039107786244\n",
      "          policy_loss: -0.11037576824266424\n",
      "          total_loss: 2.6734389895966317\n",
      "          vf_explained_var: 0.884663257098967\n",
      "          vf_loss: 2.7540958150580366\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1468000\n",
      "    num_agent_steps_trained: 1468000\n",
      "    num_steps_sampled: 1468000\n",
      "    num_steps_trained: 1468000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.883333333333336\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10192472628414874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08971961341912998\n",
      "    mean_inference_ms: 1.0011763432392482\n",
      "    mean_raw_obs_processing_ms: 0.08444988214639715\n",
      "  time_since_restore: 3151.807077407837\n",
      "  time_this_iter_s: 8.39323115348816\n",
      "  time_total_s: 3151.807077407837\n",
      "  timers:\n",
      "    learn_throughput: 690.385\n",
      "    learn_time_ms: 5793.869\n",
      "    load_throughput: 13321594.41\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 478.244\n",
      "    sample_time_ms: 8363.938\n",
      "    update_time_ms: 1.503\n",
      "  timestamp: 1643386873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1468000\n",
      "  training_iteration: 367\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:14 (running for 00:52:45.87)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         3151.81</td><td style=\"text-align: right;\">1468000</td><td style=\"text-align: right;\">-0.046129</td><td style=\"text-align: right;\">             9.29153</td><td style=\"text-align: right;\">            -6.66154</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:19 (running for 00:52:50.88)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         3151.81</td><td style=\"text-align: right;\">1468000</td><td style=\"text-align: right;\">-0.046129</td><td style=\"text-align: right;\">             9.29153</td><td style=\"text-align: right;\">            -6.66154</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.291530027985573\n",
      "  episode_reward_mean: 0.07244972378015518\n",
      "  episode_reward_min: -7.3885416984558105\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.02310910503910754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019126964755788158\n",
      "          policy_loss: -0.07739286227752605\n",
      "          total_loss: 1.7176544270498217\n",
      "          vf_explained_var: 0.9234437734209081\n",
      "          vf_loss: 1.76599821089817\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1472000\n",
      "    num_agent_steps_trained: 1472000\n",
      "    num_steps_sampled: 1472000\n",
      "    num_steps_trained: 1472000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.366666666666664\n",
      "    ram_util_percent: 34.083333333333336\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10191487688004151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08971237580263686\n",
      "    mean_inference_ms: 1.0010594873188632\n",
      "    mean_raw_obs_processing_ms: 0.0844396844470975\n",
      "  time_since_restore: 3160.2563047409058\n",
      "  time_this_iter_s: 8.449227333068848\n",
      "  time_total_s: 3160.2563047409058\n",
      "  timers:\n",
      "    learn_throughput: 688.768\n",
      "    learn_time_ms: 5807.475\n",
      "    load_throughput: 13452983.722\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 478.378\n",
      "    sample_time_ms: 8361.593\n",
      "    update_time_ms: 1.441\n",
      "  timestamp: 1643386881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1472000\n",
      "  training_iteration: 368\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:24 (running for 00:52:56.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         3160.26</td><td style=\"text-align: right;\">1472000</td><td style=\"text-align: right;\">0.0724497</td><td style=\"text-align: right;\">             9.29153</td><td style=\"text-align: right;\">            -7.38854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:29 (running for 00:53:01.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         3160.26</td><td style=\"text-align: right;\">1472000</td><td style=\"text-align: right;\">0.0724497</td><td style=\"text-align: right;\">             9.29153</td><td style=\"text-align: right;\">            -7.38854</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.70237272977829\n",
      "  episode_reward_mean: -0.0999940812587738\n",
      "  episode_reward_min: -8.440461277961731\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.0035282601502233295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01965355261793038\n",
      "          policy_loss: -0.09666326836011903\n",
      "          total_loss: 1.757282142808175\n",
      "          vf_explained_var: 0.9236227824482867\n",
      "          vf_loss: 1.824096576701249\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1476000\n",
      "    num_agent_steps_trained: 1476000\n",
      "    num_steps_sampled: 1476000\n",
      "    num_steps_trained: 1476000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.833333333333332\n",
      "    ram_util_percent: 34.04166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10190569726237088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08970621731805885\n",
      "    mean_inference_ms: 1.0009535249719237\n",
      "    mean_raw_obs_processing_ms: 0.08443322281814812\n",
      "  time_since_restore: 3168.583975791931\n",
      "  time_this_iter_s: 8.32767105102539\n",
      "  time_total_s: 3168.583975791931\n",
      "  timers:\n",
      "    learn_throughput: 690.273\n",
      "    learn_time_ms: 5794.808\n",
      "    load_throughput: 13773266.563\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 477.689\n",
      "    sample_time_ms: 8373.649\n",
      "    update_time_ms: 1.488\n",
      "  timestamp: 1643386889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1476000\n",
      "  training_iteration: 369\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:35 (running for 00:53:06.70)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         3168.58</td><td style=\"text-align: right;\">1476000</td><td style=\"text-align: right;\">-0.0999941</td><td style=\"text-align: right;\">             10.7024</td><td style=\"text-align: right;\">            -8.44046</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-21-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.70237272977829\n",
      "  episode_reward_mean: -0.04706840991973877\n",
      "  episode_reward_min: -8.440461277961731\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.029934853509629284\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019287458051439263\n",
      "          policy_loss: -0.11435830285772682\n",
      "          total_loss: 2.63992786413304\n",
      "          vf_explained_var: 0.8833155149413693\n",
      "          vf_loss: 2.724993332867981\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1480000\n",
      "    num_agent_steps_trained: 1480000\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.100000000000005\n",
      "    ram_util_percent: 34.091666666666676\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1019066190516919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08970769190698964\n",
      "    mean_inference_ms: 1.0009358985880519\n",
      "    mean_raw_obs_processing_ms: 0.08443776629528552\n",
      "  time_since_restore: 3176.906837463379\n",
      "  time_this_iter_s: 8.322861671447754\n",
      "  time_total_s: 3176.906837463379\n",
      "  timers:\n",
      "    learn_throughput: 692.719\n",
      "    learn_time_ms: 5774.349\n",
      "    load_throughput: 14054801.039\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 476.747\n",
      "    sample_time_ms: 8390.195\n",
      "    update_time_ms: 1.504\n",
      "  timestamp: 1643386898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 370\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:40 (running for 00:53:12.04)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         3176.91</td><td style=\"text-align: right;\">1480000</td><td style=\"text-align: right;\">-0.0470684</td><td style=\"text-align: right;\">             10.7024</td><td style=\"text-align: right;\">            -8.44046</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:45 (running for 00:53:17.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         3176.91</td><td style=\"text-align: right;\">1480000</td><td style=\"text-align: right;\">-0.0470684</td><td style=\"text-align: right;\">             10.7024</td><td style=\"text-align: right;\">            -8.44046</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.70237272977829\n",
      "  episode_reward_mean: 0.01933948144316673\n",
      "  episode_reward_min: -10.4600171148777\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.02401848002987844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019781256396129487\n",
      "          policy_loss: -0.09650516072588582\n",
      "          total_loss: 2.1025307190699642\n",
      "          vf_explained_var: 0.9164289546269242\n",
      "          vf_loss: 2.168993097727978\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1484000\n",
      "    num_agent_steps_trained: 1484000\n",
      "    num_steps_sampled: 1484000\n",
      "    num_steps_trained: 1484000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.0\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10190087726764457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08970313116090319\n",
      "    mean_inference_ms: 1.000858536535107\n",
      "    mean_raw_obs_processing_ms: 0.08443802116200391\n",
      "  time_since_restore: 3184.7362158298492\n",
      "  time_this_iter_s: 7.829378366470337\n",
      "  time_total_s: 3184.7362158298492\n",
      "  timers:\n",
      "    learn_throughput: 695.304\n",
      "    learn_time_ms: 5752.88\n",
      "    load_throughput: 12964389.151\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 479.921\n",
      "    sample_time_ms: 8334.709\n",
      "    update_time_ms: 1.508\n",
      "  timestamp: 1643386906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1484000\n",
      "  training_iteration: 371\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:51 (running for 00:53:22.90)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         3184.74</td><td style=\"text-align: right;\">1484000</td><td style=\"text-align: right;\">0.0193395</td><td style=\"text-align: right;\">             10.7024</td><td style=\"text-align: right;\">              -10.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-21-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.70237272977829\n",
      "  episode_reward_mean: 0.09784728422760963\n",
      "  episode_reward_min: -10.4600171148777\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.061968508755327556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01911820064377804\n",
      "          policy_loss: -0.09587129782284459\n",
      "          total_loss: 2.2603543074766512\n",
      "          vf_explained_var: 0.906074006711283\n",
      "          vf_loss: 2.327189835898017\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1488000\n",
      "    num_agent_steps_trained: 1488000\n",
      "    num_steps_sampled: 1488000\n",
      "    num_steps_trained: 1488000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.16363636363636\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10188824881104372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08969379692265896\n",
      "    mean_inference_ms: 1.0007315580649478\n",
      "    mean_raw_obs_processing_ms: 0.0844347106023267\n",
      "  time_since_restore: 3192.700094938278\n",
      "  time_this_iter_s: 7.963879108428955\n",
      "  time_total_s: 3192.700094938278\n",
      "  timers:\n",
      "    learn_throughput: 696.573\n",
      "    learn_time_ms: 5742.403\n",
      "    load_throughput: 12629641.674\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 482.602\n",
      "    sample_time_ms: 8288.397\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1643386914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1488000\n",
      "  training_iteration: 372\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:21:57 (running for 00:53:28.88)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">          3192.7</td><td style=\"text-align: right;\">1488000</td><td style=\"text-align: right;\">0.0978473</td><td style=\"text-align: right;\">             10.7024</td><td style=\"text-align: right;\">              -10.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:02 (running for 00:53:33.89)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">          3192.7</td><td style=\"text-align: right;\">1488000</td><td style=\"text-align: right;\">0.0978473</td><td style=\"text-align: right;\">             10.7024</td><td style=\"text-align: right;\">              -10.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1492000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.70237272977829\n",
      "  episode_reward_mean: -0.0715268811583519\n",
      "  episode_reward_min: -10.4600171148777\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.13844738070682813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019731977043714714\n",
      "          policy_loss: -0.1035542272021293\n",
      "          total_loss: 1.4711402644843905\n",
      "          vf_explained_var: 0.9252959011703409\n",
      "          vf_loss: 1.5447265470620766\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1492000\n",
      "    num_agent_steps_trained: 1492000\n",
      "    num_steps_sampled: 1492000\n",
      "    num_steps_trained: 1492000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.599999999999998\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10187067504776014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08967879340192862\n",
      "    mean_inference_ms: 1.0005676213728885\n",
      "    mean_raw_obs_processing_ms: 0.08442621578627603\n",
      "  time_since_restore: 3200.709300518036\n",
      "  time_this_iter_s: 8.00920557975769\n",
      "  time_total_s: 3200.709300518036\n",
      "  timers:\n",
      "    learn_throughput: 698.964\n",
      "    learn_time_ms: 5722.752\n",
      "    load_throughput: 12583226.581\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 484.931\n",
      "    sample_time_ms: 8248.588\n",
      "    update_time_ms: 1.505\n",
      "  timestamp: 1643386922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1492000\n",
      "  training_iteration: 373\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:07 (running for 00:53:38.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         3200.71</td><td style=\"text-align: right;\">1492000</td><td style=\"text-align: right;\">-0.0715269</td><td style=\"text-align: right;\">             10.7024</td><td style=\"text-align: right;\">              -10.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.549516350030899\n",
      "  episode_reward_mean: 0.07851463288068772\n",
      "  episode_reward_min: -10.4600171148777\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.07022518411038384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01909588099356064\n",
      "          policy_loss: -0.07365321911400764\n",
      "          total_loss: 1.3359259851236818\n",
      "          vf_explained_var: 0.9456206735744271\n",
      "          vf_loss: 1.3805773318014158\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1496000\n",
      "    num_agent_steps_trained: 1496000\n",
      "    num_steps_sampled: 1496000\n",
      "    num_steps_trained: 1496000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.666666666666664\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10185197021965728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.089667820178877\n",
      "    mean_inference_ms: 1.0004069857237206\n",
      "    mean_raw_obs_processing_ms: 0.08441479586734366\n",
      "  time_since_restore: 3208.885054588318\n",
      "  time_this_iter_s: 8.175754070281982\n",
      "  time_total_s: 3208.885054588318\n",
      "  timers:\n",
      "    learn_throughput: 700.52\n",
      "    learn_time_ms: 5710.046\n",
      "    load_throughput: 12459870.776\n",
      "    load_time_ms: 0.321\n",
      "    sample_throughput: 485.707\n",
      "    sample_time_ms: 8235.425\n",
      "    update_time_ms: 1.465\n",
      "  timestamp: 1643386930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1496000\n",
      "  training_iteration: 374\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:12 (running for 00:53:44.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         3208.89</td><td style=\"text-align: right;\">1496000</td><td style=\"text-align: right;\">0.0785146</td><td style=\"text-align: right;\">             9.54952</td><td style=\"text-align: right;\">              -10.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:17 (running for 00:53:49.11)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         3208.89</td><td style=\"text-align: right;\">1496000</td><td style=\"text-align: right;\">0.0785146</td><td style=\"text-align: right;\">             9.54952</td><td style=\"text-align: right;\">              -10.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.549516350030899\n",
      "  episode_reward_mean: 0.07945513337850571\n",
      "  episode_reward_min: -10.4600171148777\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.16596511819309764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019632895557106244\n",
      "          policy_loss: -0.0826236507144346\n",
      "          total_loss: 2.2517963974006845\n",
      "          vf_explained_var: 0.9087779296341763\n",
      "          vf_loss: 2.304602578410538\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1500000\n",
      "    num_agent_steps_trained: 1500000\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.37272727272727\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10182172443995562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08964727827269404\n",
      "    mean_inference_ms: 1.0001419322406304\n",
      "    mean_raw_obs_processing_ms: 0.08439219490585366\n",
      "  time_since_restore: 3216.688067674637\n",
      "  time_this_iter_s: 7.80301308631897\n",
      "  time_total_s: 3216.688067674637\n",
      "  timers:\n",
      "    learn_throughput: 703.12\n",
      "    learn_time_ms: 5688.932\n",
      "    load_throughput: 12271222.937\n",
      "    load_time_ms: 0.326\n",
      "    sample_throughput: 487.009\n",
      "    sample_time_ms: 8213.404\n",
      "    update_time_ms: 1.48\n",
      "  timestamp: 1643386938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 375\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:23 (running for 00:53:54.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         3216.69</td><td style=\"text-align: right;\">1500000</td><td style=\"text-align: right;\">0.0794551</td><td style=\"text-align: right;\">             9.54952</td><td style=\"text-align: right;\">              -10.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.392013430595398\n",
      "  episode_reward_mean: 0.032074496001005176\n",
      "  episode_reward_min: -8.25605247169733\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.11821163763262091\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01936423092466042\n",
      "          policy_loss: -0.11208038036711514\n",
      "          total_loss: 1.7963073932920002\n",
      "          vf_explained_var: 0.9075726020079787\n",
      "          vf_loss: 1.8789783496649997\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1504000\n",
      "    num_agent_steps_trained: 1504000\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1504000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.681818181818183\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10179676695427768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08963127233501485\n",
      "    mean_inference_ms: 0.9999272328985365\n",
      "    mean_raw_obs_processing_ms: 0.08437284967363962\n",
      "  time_since_restore: 3224.6295158863068\n",
      "  time_this_iter_s: 7.941448211669922\n",
      "  time_total_s: 3224.6295158863068\n",
      "  timers:\n",
      "    learn_throughput: 708.553\n",
      "    learn_time_ms: 5645.31\n",
      "    load_throughput: 12348900.339\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 488.493\n",
      "    sample_time_ms: 8188.456\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1643386946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 376\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:29 (running for 00:54:00.90)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         3224.63</td><td style=\"text-align: right;\">1504000</td><td style=\"text-align: right;\">0.0320745</td><td style=\"text-align: right;\">             9.39201</td><td style=\"text-align: right;\">            -8.25605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1508000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.233966022729874\n",
      "  episode_reward_mean: -0.07635437652468681\n",
      "  episode_reward_min: -8.25605247169733\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.12393003969042692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019004132952986787\n",
      "          policy_loss: -0.11495126158860262\n",
      "          total_loss: 2.8829731716754377\n",
      "          vf_explained_var: 0.8920264922162537\n",
      "          vf_loss: 2.969061902229504\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1508000\n",
      "    num_agent_steps_trained: 1508000\n",
      "    num_steps_sampled: 1508000\n",
      "    num_steps_trained: 1508000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.981818181818184\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10176982288864439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08961370460748551\n",
      "    mean_inference_ms: 0.9996824809688962\n",
      "    mean_raw_obs_processing_ms: 0.08435034005958858\n",
      "  time_since_restore: 3232.4261016845703\n",
      "  time_this_iter_s: 7.79658579826355\n",
      "  time_total_s: 3232.4261016845703\n",
      "  timers:\n",
      "    learn_throughput: 710.925\n",
      "    learn_time_ms: 5626.469\n",
      "    load_throughput: 12174164.429\n",
      "    load_time_ms: 0.329\n",
      "    sample_throughput: 493.613\n",
      "    sample_time_ms: 8103.51\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1643386954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1508000\n",
      "  training_iteration: 377\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:35 (running for 00:54:06.71)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         3232.43</td><td style=\"text-align: right;\">1508000</td><td style=\"text-align: right;\">-0.0763544</td><td style=\"text-align: right;\">             8.23397</td><td style=\"text-align: right;\">            -8.25605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:40 (running for 00:54:11.72)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         3232.43</td><td style=\"text-align: right;\">1508000</td><td style=\"text-align: right;\">-0.0763544</td><td style=\"text-align: right;\">             8.23397</td><td style=\"text-align: right;\">            -8.25605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.233966022729874\n",
      "  episode_reward_mean: 0.04229121819138527\n",
      "  episode_reward_min: -8.25605247169733\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.17631701836562766\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020283051589924644\n",
      "          policy_loss: -0.12665764848871897\n",
      "          total_loss: 0.9844110111355461\n",
      "          vf_explained_var: 0.9418966408057879\n",
      "          vf_loss: 1.0802637801695896\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1512000\n",
      "    num_agent_steps_trained: 1512000\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.372727272727275\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10174150062266833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08959551905018333\n",
      "    mean_inference_ms: 0.9994338942843157\n",
      "    mean_raw_obs_processing_ms: 0.08432924351275047\n",
      "  time_since_restore: 3240.1739995479584\n",
      "  time_this_iter_s: 7.7478978633880615\n",
      "  time_total_s: 3240.1739995479584\n",
      "  timers:\n",
      "    learn_throughput: 716.965\n",
      "    learn_time_ms: 5579.072\n",
      "    load_throughput: 12116137.792\n",
      "    load_time_ms: 0.33\n",
      "    sample_throughput: 496.174\n",
      "    sample_time_ms: 8061.687\n",
      "    update_time_ms: 1.389\n",
      "  timestamp: 1643386961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 378\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:45 (running for 00:54:17.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         3240.17</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">0.0422912</td><td style=\"text-align: right;\">             8.23397</td><td style=\"text-align: right;\">            -8.25605</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1516000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.064008712768555\n",
      "  episode_reward_mean: -0.07881945312023163\n",
      "  episode_reward_min: -10.5432990193367\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.2161228249390279\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014501092794242474\n",
      "          policy_loss: -0.10929316742926516\n",
      "          total_loss: 2.2172394852386788\n",
      "          vf_explained_var: 0.9030027818936174\n",
      "          vf_loss: 2.2934973313923805\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1516000\n",
      "    num_agent_steps_trained: 1516000\n",
      "    num_steps_sampled: 1516000\n",
      "    num_steps_trained: 1516000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.845454545454547\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1017105513731373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08957203151055391\n",
      "    mean_inference_ms: 0.9991473831944839\n",
      "    mean_raw_obs_processing_ms: 0.08430542492317265\n",
      "  time_since_restore: 3247.889240026474\n",
      "  time_this_iter_s: 7.715240478515625\n",
      "  time_total_s: 3247.889240026474\n",
      "  timers:\n",
      "    learn_throughput: 722.423\n",
      "    learn_time_ms: 5536.922\n",
      "    load_throughput: 11961511.479\n",
      "    load_time_ms: 0.334\n",
      "    sample_throughput: 500.283\n",
      "    sample_time_ms: 7995.471\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1643386969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1516000\n",
      "  training_iteration: 379\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:51 (running for 00:54:23.22)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         3247.89</td><td style=\"text-align: right;\">1516000</td><td style=\"text-align: right;\">-0.0788195</td><td style=\"text-align: right;\">             8.06401</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:22:56 (running for 00:54:28.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         3247.89</td><td style=\"text-align: right;\">1516000</td><td style=\"text-align: right;\">-0.0788195</td><td style=\"text-align: right;\">             8.06401</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-22-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.064008712768555\n",
      "  episode_reward_mean: -0.10803843319416045\n",
      "  episode_reward_min: -10.5432990193367\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.3320570749781465\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015345800312122225\n",
      "          policy_loss: -0.11144766963978288\n",
      "          total_loss: 1.8281138117392597\n",
      "          vf_explained_var: 0.9234838779895537\n",
      "          vf_loss: 1.9046018229096486\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1520000\n",
      "    num_agent_steps_trained: 1520000\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.241666666666664\n",
      "    ram_util_percent: 34.08333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10168557896624193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08955176167725218\n",
      "    mean_inference_ms: 0.9988840412714767\n",
      "    mean_raw_obs_processing_ms: 0.08428309293703318\n",
      "  time_since_restore: 3255.715598344803\n",
      "  time_this_iter_s: 7.826358318328857\n",
      "  time_total_s: 3255.715598344803\n",
      "  timers:\n",
      "    learn_throughput: 723.514\n",
      "    learn_time_ms: 5528.576\n",
      "    load_throughput: 11865914.138\n",
      "    load_time_ms: 0.337\n",
      "    sample_throughput: 505.616\n",
      "    sample_time_ms: 7911.141\n",
      "    update_time_ms: 1.318\n",
      "  timestamp: 1643386977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 380\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:02 (running for 00:54:34.07)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         3255.72</td><td style=\"text-align: right;\">1520000</td><td style=\"text-align: right;\">-0.108038</td><td style=\"text-align: right;\">             8.06401</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1524000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-23-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.291177898645401\n",
      "  episode_reward_mean: 0.0058192524313926694\n",
      "  episode_reward_min: -10.5432990193367\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.32581704844350134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014867173521080052\n",
      "          policy_loss: -0.09544812669722183\n",
      "          total_loss: 1.6877809863099429\n",
      "          vf_explained_var: 0.9309805667528542\n",
      "          vf_loss: 1.7493598262428918\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1524000\n",
      "    num_agent_steps_trained: 1524000\n",
      "    num_steps_sampled: 1524000\n",
      "    num_steps_trained: 1524000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.10909090909091\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10165669514649213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08952755392448898\n",
      "    mean_inference_ms: 0.9985917783618052\n",
      "    mean_raw_obs_processing_ms: 0.08425873932416952\n",
      "  time_since_restore: 3263.4413545131683\n",
      "  time_this_iter_s: 7.7257561683654785\n",
      "  time_total_s: 3263.4413545131683\n",
      "  timers:\n",
      "    learn_throughput: 725.857\n",
      "    learn_time_ms: 5510.731\n",
      "    load_throughput: 12598344.973\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 505.691\n",
      "    sample_time_ms: 7909.968\n",
      "    update_time_ms: 1.302\n",
      "  timestamp: 1643386985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1524000\n",
      "  training_iteration: 381\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:08 (running for 00:54:39.82)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         3263.44</td><td style=\"text-align: right;\">1524000</td><td style=\"text-align: right;\">0.00581925</td><td style=\"text-align: right;\">             9.29118</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.291177898645401\n",
      "  episode_reward_mean: 0.04673211619257927\n",
      "  episode_reward_min: -10.5432990193367\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.380677963134342\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014943616390129581\n",
      "          policy_loss: -0.0997510807608725\n",
      "          total_loss: 1.037978698532047\n",
      "          vf_explained_var: 0.9386281301257431\n",
      "          vf_loss: 1.1036863492060733\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1528000\n",
      "    num_agent_steps_trained: 1528000\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1528000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.954545454545457\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10162768318687149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08950356462805756\n",
      "    mean_inference_ms: 0.9983002168999979\n",
      "    mean_raw_obs_processing_ms: 0.0842345419781896\n",
      "  time_since_restore: 3271.2115478515625\n",
      "  time_this_iter_s: 7.770193338394165\n",
      "  time_total_s: 3271.2115478515625\n",
      "  timers:\n",
      "    learn_throughput: 726.431\n",
      "    learn_time_ms: 5506.376\n",
      "    load_throughput: 12890676.911\n",
      "    load_time_ms: 0.31\n",
      "    sample_throughput: 507.802\n",
      "    sample_time_ms: 7877.089\n",
      "    update_time_ms: 1.302\n",
      "  timestamp: 1643386992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 382\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:13 (running for 00:54:45.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         3271.21</td><td style=\"text-align: right;\">1528000</td><td style=\"text-align: right;\">0.0467321</td><td style=\"text-align: right;\">             9.29118</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:18 (running for 00:54:50.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         3271.21</td><td style=\"text-align: right;\">1528000</td><td style=\"text-align: right;\">0.0467321</td><td style=\"text-align: right;\">             9.29118</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-23-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.291177898645401\n",
      "  episode_reward_mean: 0.04769358590245247\n",
      "  episode_reward_min: -10.5432990193367\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.37482649329649187\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014745920601863208\n",
      "          policy_loss: -0.08438026472495529\n",
      "          total_loss: 1.8335789834554759\n",
      "          vf_explained_var: 0.9059813830801235\n",
      "          vf_loss: 1.8843661959453295\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1532000\n",
      "    num_agent_steps_trained: 1532000\n",
      "    num_steps_sampled: 1532000\n",
      "    num_steps_trained: 1532000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.836363636363636\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10159916375703984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08948138855748301\n",
      "    mean_inference_ms: 0.9980131776324079\n",
      "    mean_raw_obs_processing_ms: 0.084211491243992\n",
      "  time_since_restore: 3279.250899076462\n",
      "  time_this_iter_s: 8.039351224899292\n",
      "  time_total_s: 3279.250899076462\n",
      "  timers:\n",
      "    learn_throughput: 726.075\n",
      "    learn_time_ms: 5509.07\n",
      "    load_throughput: 13071457.733\n",
      "    load_time_ms: 0.306\n",
      "    sample_throughput: 508.057\n",
      "    sample_time_ms: 7873.128\n",
      "    update_time_ms: 1.294\n",
      "  timestamp: 1643387000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1532000\n",
      "  training_iteration: 383\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:24 (running for 00:54:55.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         3279.25</td><td style=\"text-align: right;\">1532000</td><td style=\"text-align: right;\">0.0476936</td><td style=\"text-align: right;\">             9.29118</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:29 (running for 00:55:00.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         3279.25</td><td style=\"text-align: right;\">1532000</td><td style=\"text-align: right;\">0.0476936</td><td style=\"text-align: right;\">             9.29118</td><td style=\"text-align: right;\">            -10.5433</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-23-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.886230528354645\n",
      "  episode_reward_mean: 0.049831618070602414\n",
      "  episode_reward_min: -10.528481841087341\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.4554986888763084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01416150783026069\n",
      "          policy_loss: -0.09815091190519191\n",
      "          total_loss: 2.529094844399476\n",
      "          vf_explained_var: 0.8949731997264329\n",
      "          vf_loss: 2.594984073119779\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1536000\n",
      "    num_agent_steps_trained: 1536000\n",
      "    num_steps_sampled: 1536000\n",
      "    num_steps_trained: 1536000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.383333333333333\n",
      "    ram_util_percent: 34.14166666666666\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10157642100439372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08946131023693012\n",
      "    mean_inference_ms: 0.9977555094075155\n",
      "    mean_raw_obs_processing_ms: 0.08419141909579249\n",
      "  time_since_restore: 3287.2988934516907\n",
      "  time_this_iter_s: 8.047994375228882\n",
      "  time_total_s: 3287.2988934516907\n",
      "  timers:\n",
      "    learn_throughput: 727.184\n",
      "    learn_time_ms: 5500.673\n",
      "    load_throughput: 13106176.08\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 508.177\n",
      "    sample_time_ms: 7871.276\n",
      "    update_time_ms: 1.29\n",
      "  timestamp: 1643387009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1536000\n",
      "  training_iteration: 384\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:34 (running for 00:55:05.74)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">          3287.3</td><td style=\"text-align: right;\">1536000</td><td style=\"text-align: right;\">0.0498316</td><td style=\"text-align: right;\">             10.8862</td><td style=\"text-align: right;\">            -10.5285</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-23-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.886230528354645\n",
      "  episode_reward_mean: 0.00043207094073295595\n",
      "  episode_reward_min: -10.528481841087341\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.42972831199326184\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015218346739334395\n",
      "          policy_loss: -0.12155898566938617\n",
      "          total_loss: 2.6655333243983907\n",
      "          vf_explained_var: 0.8707208362958765\n",
      "          vf_loss: 2.752423016887198\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1540000\n",
      "    num_agent_steps_trained: 1540000\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.345454545454544\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10155471874784597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08944270414726663\n",
      "    mean_inference_ms: 0.9975335308006257\n",
      "    mean_raw_obs_processing_ms: 0.08417237509661436\n",
      "  time_since_restore: 3295.385064125061\n",
      "  time_this_iter_s: 8.086170673370361\n",
      "  time_total_s: 3295.385064125061\n",
      "  timers:\n",
      "    learn_throughput: 727.245\n",
      "    learn_time_ms: 5500.208\n",
      "    load_throughput: 13333240.086\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 506.842\n",
      "    sample_time_ms: 7892.008\n",
      "    update_time_ms: 1.266\n",
      "  timestamp: 1643387017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 385\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:39 (running for 00:55:10.84)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         3295.39</td><td style=\"text-align: right;\">1540000</td><td style=\"text-align: right;\">0.000432071</td><td style=\"text-align: right;\">             10.8862</td><td style=\"text-align: right;\">            -10.5285</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:44 (running for 00:55:15.85)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         3295.39</td><td style=\"text-align: right;\">1540000</td><td style=\"text-align: right;\">0.000432071</td><td style=\"text-align: right;\">             10.8862</td><td style=\"text-align: right;\">            -10.5285</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-23-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.886230528354645\n",
      "  episode_reward_mean: -0.013463233411312104\n",
      "  episode_reward_min: -10.528481841087341\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.370794354637544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015494053741362625\n",
      "          policy_loss: -0.11233203346310283\n",
      "          total_loss: 1.4644786155066623\n",
      "          vf_explained_var: 0.9320129795741009\n",
      "          vf_loss: 1.5415132520419936\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1544000\n",
      "    num_agent_steps_trained: 1544000\n",
      "    num_steps_sampled: 1544000\n",
      "    num_steps_trained: 1544000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.499999999999996\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1015343718927844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08942823367877459\n",
      "    mean_inference_ms: 0.9973290162189073\n",
      "    mean_raw_obs_processing_ms: 0.08415475887805215\n",
      "  time_since_restore: 3303.20511674881\n",
      "  time_this_iter_s: 7.820052623748779\n",
      "  time_total_s: 3303.20511674881\n",
      "  timers:\n",
      "    learn_throughput: 727.942\n",
      "    learn_time_ms: 5494.946\n",
      "    load_throughput: 13340661.578\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 507.344\n",
      "    sample_time_ms: 7884.2\n",
      "    update_time_ms: 1.28\n",
      "  timestamp: 1643387024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1544000\n",
      "  training_iteration: 386\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:50 (running for 00:55:21.69)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         3303.21</td><td style=\"text-align: right;\">1544000</td><td style=\"text-align: right;\">-0.0134632</td><td style=\"text-align: right;\">             10.8862</td><td style=\"text-align: right;\">            -10.5285</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1548000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-23-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.886230528354645\n",
      "  episode_reward_mean: 0.04266904652118683\n",
      "  episode_reward_min: -10.528481841087341\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.42212011269343797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014428559514777995\n",
      "          policy_loss: -0.07062284513526866\n",
      "          total_loss: 1.5779807694661883\n",
      "          vf_explained_var: 0.9204028204564125\n",
      "          vf_loss: 1.6157335559206625\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1548000\n",
      "    num_agent_steps_trained: 1548000\n",
      "    num_steps_sampled: 1548000\n",
      "    num_steps_trained: 1548000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.683333333333334\n",
      "    ram_util_percent: 34.14166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10151751507559278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08941570468289822\n",
      "    mean_inference_ms: 0.9971503008503149\n",
      "    mean_raw_obs_processing_ms: 0.08413875074394733\n",
      "  time_since_restore: 3311.056318283081\n",
      "  time_this_iter_s: 7.85120153427124\n",
      "  time_total_s: 3311.056318283081\n",
      "  timers:\n",
      "    learn_throughput: 728.868\n",
      "    learn_time_ms: 5487.962\n",
      "    load_throughput: 13685631.781\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 506.893\n",
      "    sample_time_ms: 7891.212\n",
      "    update_time_ms: 1.276\n",
      "  timestamp: 1643387032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1548000\n",
      "  training_iteration: 387\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:23:55 (running for 00:55:27.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         3311.06</td><td style=\"text-align: right;\">1548000</td><td style=\"text-align: right;\">0.042669</td><td style=\"text-align: right;\">             10.8862</td><td style=\"text-align: right;\">            -10.5285</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.886230528354645\n",
      "  episode_reward_mean: -0.0618439457938075\n",
      "  episode_reward_min: -9.612776160240173\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.44605158276615603\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015812221360150272\n",
      "          policy_loss: -0.11469038263719368\n",
      "          total_loss: 1.4694341193997271\n",
      "          vf_explained_var: 0.931034231121822\n",
      "          vf_loss: 1.5481022877558586\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1552000\n",
      "    num_agent_steps_trained: 1552000\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1552000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.754545454545454\n",
      "    ram_util_percent: 34.118181818181824\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10149968178718304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08940259169077663\n",
      "    mean_inference_ms: 0.9969650545708177\n",
      "    mean_raw_obs_processing_ms: 0.08411922378748325\n",
      "  time_since_restore: 3318.7207159996033\n",
      "  time_this_iter_s: 7.664397716522217\n",
      "  time_total_s: 3318.7207159996033\n",
      "  timers:\n",
      "    learn_throughput: 729.502\n",
      "    learn_time_ms: 5483.191\n",
      "    load_throughput: 13860885.658\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 507.549\n",
      "    sample_time_ms: 7881.007\n",
      "    update_time_ms: 1.274\n",
      "  timestamp: 1643387040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 388\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:01 (running for 00:55:33.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         3318.72</td><td style=\"text-align: right;\">1552000</td><td style=\"text-align: right;\">-0.0618439</td><td style=\"text-align: right;\">             10.8862</td><td style=\"text-align: right;\">            -9.61278</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:06 (running for 00:55:38.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         3318.72</td><td style=\"text-align: right;\">1552000</td><td style=\"text-align: right;\">-0.0618439</td><td style=\"text-align: right;\">             10.8862</td><td style=\"text-align: right;\">            -9.61278</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1556000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.026678323745728\n",
      "  episode_reward_mean: -0.06280487418174743\n",
      "  episode_reward_min: -9.612776160240173\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.5220698787560386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014222205145741587\n",
      "          policy_loss: -0.0974374542140492\n",
      "          total_loss: 1.8679598422823673\n",
      "          vf_explained_var: 0.9336990706382259\n",
      "          vf_loss: 1.9329973367513509\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1556000\n",
      "    num_agent_steps_trained: 1556000\n",
      "    num_steps_sampled: 1556000\n",
      "    num_steps_trained: 1556000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.44545454545454\n",
      "    ram_util_percent: 34.13636363636365\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10147388127433533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08938411527489583\n",
      "    mean_inference_ms: 0.9967220354869997\n",
      "    mean_raw_obs_processing_ms: 0.08409537141455142\n",
      "  time_since_restore: 3326.489703655243\n",
      "  time_this_iter_s: 7.768987655639648\n",
      "  time_total_s: 3326.489703655243\n",
      "  timers:\n",
      "    learn_throughput: 726.753\n",
      "    learn_time_ms: 5503.935\n",
      "    load_throughput: 14044212.289\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 508.866\n",
      "    sample_time_ms: 7860.617\n",
      "    update_time_ms: 1.277\n",
      "  timestamp: 1643387048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1556000\n",
      "  training_iteration: 389\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:12 (running for 00:55:44.03)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         3326.49</td><td style=\"text-align: right;\">1556000</td><td style=\"text-align: right;\">-0.0628049</td><td style=\"text-align: right;\">             8.02668</td><td style=\"text-align: right;\">            -9.61278</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.026678323745728\n",
      "  episode_reward_mean: 0.03377803936600685\n",
      "  episode_reward_min: -7.774210095405579\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.48494755852446764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014904620874579447\n",
      "          policy_loss: -0.11530170895795147\n",
      "          total_loss: 0.6717603961201084\n",
      "          vf_explained_var: 0.9640327439513258\n",
      "          vf_loss: 0.7531075146009204\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_agent_steps_trained: 1560000\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.554545454545455\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10144404370250111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08936398975533447\n",
      "    mean_inference_ms: 0.9964493532041316\n",
      "    mean_raw_obs_processing_ms: 0.08406961646868691\n",
      "  time_since_restore: 3334.637871980667\n",
      "  time_this_iter_s: 8.148168325424194\n",
      "  time_total_s: 3334.637871980667\n",
      "  timers:\n",
      "    learn_throughput: 722.534\n",
      "    learn_time_ms: 5536.074\n",
      "    load_throughput: 13901082.111\n",
      "    load_time_ms: 0.288\n",
      "    sample_throughput: 507.54\n",
      "    sample_time_ms: 7881.146\n",
      "    update_time_ms: 1.307\n",
      "  timestamp: 1643387056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 390\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:17 (running for 00:55:49.20)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         3334.64</td><td style=\"text-align: right;\">1560000</td><td style=\"text-align: right;\">0.033778</td><td style=\"text-align: right;\">             8.02668</td><td style=\"text-align: right;\">            -7.77421</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:22 (running for 00:55:54.21)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         3334.64</td><td style=\"text-align: right;\">1560000</td><td style=\"text-align: right;\">0.033778</td><td style=\"text-align: right;\">             8.02668</td><td style=\"text-align: right;\">            -7.77421</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1564000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-24\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.026678323745728\n",
      "  episode_reward_mean: -0.0711554804444313\n",
      "  episode_reward_min: -7.774210095405579\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.5267702981509188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0156290387237832\n",
      "          policy_loss: -0.11578564247772379\n",
      "          total_loss: 2.202863535148803\n",
      "          vf_explained_var: 0.8923737133702924\n",
      "          vf_loss: 2.283044273254051\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1564000\n",
      "    num_agent_steps_trained: 1564000\n",
      "    num_steps_sampled: 1564000\n",
      "    num_steps_trained: 1564000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.099999999999998\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10141269680690748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08934013367735083\n",
      "    mean_inference_ms: 0.996151402874705\n",
      "    mean_raw_obs_processing_ms: 0.08404229200907387\n",
      "  time_since_restore: 3342.563127756119\n",
      "  time_this_iter_s: 7.92525577545166\n",
      "  time_total_s: 3342.563127756119\n",
      "  timers:\n",
      "    learn_throughput: 719.443\n",
      "    learn_time_ms: 5559.859\n",
      "    load_throughput: 14129371.737\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 505.708\n",
      "    sample_time_ms: 7909.695\n",
      "    update_time_ms: 1.307\n",
      "  timestamp: 1643387064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1564000\n",
      "  training_iteration: 391\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:28 (running for 00:56:00.15)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         3342.56</td><td style=\"text-align: right;\">1564000</td><td style=\"text-align: right;\">-0.0711555</td><td style=\"text-align: right;\">             8.02668</td><td style=\"text-align: right;\">            -7.77421</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.026678323745728\n",
      "  episode_reward_mean: -0.09494509741663933\n",
      "  episode_reward_min: -7.774210095405579\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.57869626669794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014962160507073307\n",
      "          policy_loss: -0.1179256379022263\n",
      "          total_loss: 2.4530824744292805\n",
      "          vf_explained_var: 0.8899721734626319\n",
      "          vf_loss: 2.536922449594544\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1568000\n",
      "    num_agent_steps_trained: 1568000\n",
      "    num_steps_sampled: 1568000\n",
      "    num_steps_trained: 1568000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.299999999999997\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10138519654550776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08932229358242293\n",
      "    mean_inference_ms: 0.9959025171552903\n",
      "    mean_raw_obs_processing_ms: 0.08401951597678078\n",
      "  time_since_restore: 3350.7287249565125\n",
      "  time_this_iter_s: 8.165597200393677\n",
      "  time_total_s: 3350.7287249565125\n",
      "  timers:\n",
      "    learn_throughput: 719.051\n",
      "    learn_time_ms: 5562.889\n",
      "    load_throughput: 14109171.642\n",
      "    load_time_ms: 0.284\n",
      "    sample_throughput: 501.883\n",
      "    sample_time_ms: 7969.988\n",
      "    update_time_ms: 1.314\n",
      "  timestamp: 1643387072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1568000\n",
      "  training_iteration: 392\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:33 (running for 00:56:05.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         3350.73</td><td style=\"text-align: right;\">1568000</td><td style=\"text-align: right;\">-0.0949451</td><td style=\"text-align: right;\">             8.02668</td><td style=\"text-align: right;\">            -7.77421</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:38 (running for 00:56:10.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         3350.73</td><td style=\"text-align: right;\">1568000</td><td style=\"text-align: right;\">-0.0949451</td><td style=\"text-align: right;\">             8.02668</td><td style=\"text-align: right;\">            -7.77421</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1572000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.525610312819481\n",
      "  episode_reward_mean: 0.011003726534545422\n",
      "  episode_reward_min: -9.698743790388107\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.5811713316587992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014021978608048465\n",
      "          policy_loss: -0.0888822104619135\n",
      "          total_loss: 1.774479963523536\n",
      "          vf_explained_var: 0.9186405650390091\n",
      "          vf_loss: 1.831418355081671\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1572000\n",
      "    num_agent_steps_trained: 1572000\n",
      "    num_steps_sampled: 1572000\n",
      "    num_steps_trained: 1572000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.591666666666665\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10136035456806422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08930499010018363\n",
      "    mean_inference_ms: 0.995678407691064\n",
      "    mean_raw_obs_processing_ms: 0.0839988396500446\n",
      "  time_since_restore: 3358.829020500183\n",
      "  time_this_iter_s: 8.100295543670654\n",
      "  time_total_s: 3358.829020500183\n",
      "  timers:\n",
      "    learn_throughput: 719.101\n",
      "    learn_time_ms: 5562.504\n",
      "    load_throughput: 14007861.735\n",
      "    load_time_ms: 0.286\n",
      "    sample_throughput: 501.274\n",
      "    sample_time_ms: 7979.661\n",
      "    update_time_ms: 1.357\n",
      "  timestamp: 1643387080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1572000\n",
      "  training_iteration: 393\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:43 (running for 00:56:15.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         3358.83</td><td style=\"text-align: right;\">1572000</td><td style=\"text-align: right;\">0.0110037</td><td style=\"text-align: right;\">             8.52561</td><td style=\"text-align: right;\">            -9.69874</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:48 (running for 00:56:20.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         3358.83</td><td style=\"text-align: right;\">1572000</td><td style=\"text-align: right;\">0.0110037</td><td style=\"text-align: right;\">             8.52561</td><td style=\"text-align: right;\">            -9.69874</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.525610312819481\n",
      "  episode_reward_mean: 0.05389194011688232\n",
      "  episode_reward_min: -9.698743790388107\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.6117024572946692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016566991578458645\n",
      "          policy_loss: -0.10259238611906767\n",
      "          total_loss: 1.7171350355410286\n",
      "          vf_explained_var: 0.9227819099862088\n",
      "          vf_loss: 1.7819857369507512\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1576000\n",
      "    num_agent_steps_trained: 1576000\n",
      "    num_steps_sampled: 1576000\n",
      "    num_steps_trained: 1576000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.94545454545455\n",
      "    ram_util_percent: 34.10909090909092\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10133886743029295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08929034405366729\n",
      "    mean_inference_ms: 0.9954935538072155\n",
      "    mean_raw_obs_processing_ms: 0.0839820423044866\n",
      "  time_since_restore: 3366.846338033676\n",
      "  time_this_iter_s: 8.017317533493042\n",
      "  time_total_s: 3366.846338033676\n",
      "  timers:\n",
      "    learn_throughput: 718.053\n",
      "    learn_time_ms: 5570.617\n",
      "    load_throughput: 14117482.329\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 501.959\n",
      "    sample_time_ms: 7968.783\n",
      "    update_time_ms: 1.384\n",
      "  timestamp: 1643387088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1576000\n",
      "  training_iteration: 394\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:53 (running for 00:56:25.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         3366.85</td><td style=\"text-align: right;\">1576000</td><td style=\"text-align: right;\">0.0538919</td><td style=\"text-align: right;\">             8.52561</td><td style=\"text-align: right;\">            -9.69874</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-24-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.525610312819481\n",
      "  episode_reward_mean: 0.06437048599123955\n",
      "  episode_reward_min: -9.698743790388107\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.5358453063554661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014215893948437218\n",
      "          policy_loss: -0.09123937887169661\n",
      "          total_loss: 1.5226379855508385\n",
      "          vf_explained_var: 0.9234040435924324\n",
      "          vf_loss: 1.5814917857768716\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1580000\n",
      "    num_agent_steps_trained: 1580000\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.35833333333333\n",
      "    ram_util_percent: 34.116666666666674\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1013126841638838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08927270833042478\n",
      "    mean_inference_ms: 0.9952777786316375\n",
      "    mean_raw_obs_processing_ms: 0.08396539499157572\n",
      "  time_since_restore: 3374.7995676994324\n",
      "  time_this_iter_s: 7.953229665756226\n",
      "  time_total_s: 3374.7995676994324\n",
      "  timers:\n",
      "    learn_throughput: 715.719\n",
      "    learn_time_ms: 5588.782\n",
      "    load_throughput: 14113919.408\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 503.44\n",
      "    sample_time_ms: 7945.343\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1643387096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 395\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:24:59 (running for 00:56:31.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">          3374.8</td><td style=\"text-align: right;\">1580000</td><td style=\"text-align: right;\">0.0643705</td><td style=\"text-align: right;\">             8.52561</td><td style=\"text-align: right;\">            -9.69874</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:04 (running for 00:56:36.48)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">          3374.8</td><td style=\"text-align: right;\">1580000</td><td style=\"text-align: right;\">0.0643705</td><td style=\"text-align: right;\">             8.52561</td><td style=\"text-align: right;\">            -9.69874</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-25-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.525610312819481\n",
      "  episode_reward_mean: 0.058851931095123294\n",
      "  episode_reward_min: -9.698743790388107\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.6214887111097254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015683113544697706\n",
      "          policy_loss: -0.11624615656021701\n",
      "          total_loss: 1.4774901550017818\n",
      "          vf_explained_var: 0.9257191271551194\n",
      "          vf_loss: 1.5580082227786383\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1584000\n",
      "    num_agent_steps_trained: 1584000\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.700000000000003\n",
      "    ram_util_percent: 34.118181818181824\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10128775856736075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08925779474192036\n",
      "    mean_inference_ms: 0.9950800607265989\n",
      "    mean_raw_obs_processing_ms: 0.08394969418818615\n",
      "  time_since_restore: 3382.8047311306\n",
      "  time_this_iter_s: 8.005163431167603\n",
      "  time_total_s: 3382.8047311306\n",
      "  timers:\n",
      "    learn_throughput: 712.863\n",
      "    learn_time_ms: 5611.174\n",
      "    load_throughput: 14184321.948\n",
      "    load_time_ms: 0.282\n",
      "    sample_throughput: 502.545\n",
      "    sample_time_ms: 7959.494\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1643387104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 396\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:09 (running for 00:56:41.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">          3382.8</td><td style=\"text-align: right;\">1584000</td><td style=\"text-align: right;\">0.0588519</td><td style=\"text-align: right;\">             8.52561</td><td style=\"text-align: right;\">            -9.69874</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.525610312819481\n",
      "  episode_reward_mean: 0.08424371019005776\n",
      "  episode_reward_min: -9.698743790388107\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.6870257097867227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01609018449258641\n",
      "          policy_loss: -0.0926744427191474\n",
      "          total_loss: 2.300654298102643\n",
      "          vf_explained_var: 0.8959446802575101\n",
      "          vf_loss: 2.3566732921587525\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1588000\n",
      "    num_agent_steps_trained: 1588000\n",
      "    num_steps_sampled: 1588000\n",
      "    num_steps_trained: 1588000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.758333333333333\n",
      "    ram_util_percent: 34.12500000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1012642326513275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08924124424834882\n",
      "    mean_inference_ms: 0.9948846750021875\n",
      "    mean_raw_obs_processing_ms: 0.08393495972580287\n",
      "  time_since_restore: 3390.7857954502106\n",
      "  time_this_iter_s: 7.981064319610596\n",
      "  time_total_s: 3390.7857954502106\n",
      "  timers:\n",
      "    learn_throughput: 714.429\n",
      "    learn_time_ms: 5598.877\n",
      "    load_throughput: 14249376.592\n",
      "    load_time_ms: 0.281\n",
      "    sample_throughput: 499.512\n",
      "    sample_time_ms: 8007.813\n",
      "    update_time_ms: 1.35\n",
      "  timestamp: 1643387112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1588000\n",
      "  training_iteration: 397\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:15 (running for 00:56:47.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         3390.79</td><td style=\"text-align: right;\">1588000</td><td style=\"text-align: right;\">0.0842437</td><td style=\"text-align: right;\">             8.52561</td><td style=\"text-align: right;\">            -9.69874</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-25-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.100027590990067\n",
      "  episode_reward_mean: -0.023489682972431182\n",
      "  episode_reward_min: -6.97973969578743\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.693165676183598\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015141127493749853\n",
      "          policy_loss: -0.10901440211962307\n",
      "          total_loss: 2.12235980713039\n",
      "          vf_explained_var: 0.8977638133110538\n",
      "          vf_loss: 2.196880832995458\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1592000\n",
      "    num_agent_steps_trained: 1592000\n",
      "    num_steps_sampled: 1592000\n",
      "    num_steps_trained: 1592000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.772727272727277\n",
      "    ram_util_percent: 34.145454545454555\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10123856025226549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08922314225096357\n",
      "    mean_inference_ms: 0.9946679757231911\n",
      "    mean_raw_obs_processing_ms: 0.08391898432942535\n",
      "  time_since_restore: 3398.5460720062256\n",
      "  time_this_iter_s: 7.760276556015015\n",
      "  time_total_s: 3398.5460720062256\n",
      "  timers:\n",
      "    learn_throughput: 713.314\n",
      "    learn_time_ms: 5607.631\n",
      "    load_throughput: 14207143.704\n",
      "    load_time_ms: 0.282\n",
      "    sample_throughput: 500.258\n",
      "    sample_time_ms: 7995.869\n",
      "    update_time_ms: 1.339\n",
      "  timestamp: 1643387120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1592000\n",
      "  training_iteration: 398\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:21 (running for 00:56:53.29)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         3398.55</td><td style=\"text-align: right;\">1592000</td><td style=\"text-align: right;\">-0.0234897</td><td style=\"text-align: right;\">             8.10003</td><td style=\"text-align: right;\">            -6.97974</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:26 (running for 00:56:58.29)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         3398.55</td><td style=\"text-align: right;\">1592000</td><td style=\"text-align: right;\">-0.0234897</td><td style=\"text-align: right;\">             8.10003</td><td style=\"text-align: right;\">            -6.97974</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.100027590990067\n",
      "  episode_reward_mean: -0.02743364989757538\n",
      "  episode_reward_min: -7.732244908809662\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.6607765862057285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015291225762353288\n",
      "          policy_loss: -0.1007784558512691\n",
      "          total_loss: 2.225523269581433\n",
      "          vf_explained_var: 0.9143625128012831\n",
      "          vf_loss: 2.2914663935901336\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1596000\n",
      "    num_agent_steps_trained: 1596000\n",
      "    num_steps_sampled: 1596000\n",
      "    num_steps_trained: 1596000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.645454545454545\n",
      "    ram_util_percent: 34.17272727272727\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10121012658928315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08920285026221057\n",
      "    mean_inference_ms: 0.9944178499256247\n",
      "    mean_raw_obs_processing_ms: 0.08389922171281583\n",
      "  time_since_restore: 3406.2568349838257\n",
      "  time_this_iter_s: 7.710762977600098\n",
      "  time_total_s: 3406.2568349838257\n",
      "  timers:\n",
      "    learn_throughput: 714.266\n",
      "    learn_time_ms: 5600.151\n",
      "    load_throughput: 14081933.859\n",
      "    load_time_ms: 0.284\n",
      "    sample_throughput: 499.601\n",
      "    sample_time_ms: 8006.387\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1643387128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1596000\n",
      "  training_iteration: 399\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:32 (running for 00:57:04.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         3406.26</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\">-0.0274336</td><td style=\"text-align: right;\">             8.10003</td><td style=\"text-align: right;\">            -7.73224</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-25-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.100027590990067\n",
      "  episode_reward_mean: -0.025308350473642348\n",
      "  episode_reward_min: -7.732244908809662\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.690054563585148\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015766021053518653\n",
      "          policy_loss: -0.10446724519534137\n",
      "          total_loss: 2.170157657835197\n",
      "          vf_explained_var: 0.8984464829326958\n",
      "          vf_loss: 2.2387079372361143\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1600000\n",
      "    num_agent_steps_trained: 1600000\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.12727272727273\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10118745045923998\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08918740440840342\n",
      "    mean_inference_ms: 0.9942192187332114\n",
      "    mean_raw_obs_processing_ms: 0.08388100643547006\n",
      "  time_since_restore: 3414.4053103923798\n",
      "  time_this_iter_s: 8.148475408554077\n",
      "  time_total_s: 3414.4053103923798\n",
      "  timers:\n",
      "    learn_throughput: 715.551\n",
      "    learn_time_ms: 5590.1\n",
      "    load_throughput: 14216774.85\n",
      "    load_time_ms: 0.281\n",
      "    sample_throughput: 499.434\n",
      "    sample_time_ms: 8009.072\n",
      "    update_time_ms: 1.326\n",
      "  timestamp: 1643387136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 400\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:37 (running for 00:57:09.19)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         3414.41</td><td style=\"text-align: right;\">1600000</td><td style=\"text-align: right;\">-0.0253084</td><td style=\"text-align: right;\">             8.10003</td><td style=\"text-align: right;\">            -7.73224</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:42 (running for 00:57:14.20)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         3414.41</td><td style=\"text-align: right;\">1600000</td><td style=\"text-align: right;\">-0.0253084</td><td style=\"text-align: right;\">             8.10003</td><td style=\"text-align: right;\">            -7.73224</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1604000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-25-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.9181933999061584\n",
      "  episode_reward_mean: 0.0595798422396183\n",
      "  episode_reward_min: -7.732244908809662\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7930397196482587\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015269765181729022\n",
      "          policy_loss: -0.08401161836865809\n",
      "          total_loss: 1.002370939136643\n",
      "          vf_explained_var: 0.9410648421574664\n",
      "          vf_loss: 1.051596124726598\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1604000\n",
      "    num_agent_steps_trained: 1604000\n",
      "    num_steps_sampled: 1604000\n",
      "    num_steps_trained: 1604000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.041666666666664\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10116523272676786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08917368711557919\n",
      "    mean_inference_ms: 0.9940238131525533\n",
      "    mean_raw_obs_processing_ms: 0.08386448406374335\n",
      "  time_since_restore: 3422.3509175777435\n",
      "  time_this_iter_s: 7.9456071853637695\n",
      "  time_total_s: 3422.3509175777435\n",
      "  timers:\n",
      "    learn_throughput: 716.794\n",
      "    learn_time_ms: 5580.402\n",
      "    load_throughput: 14231246.077\n",
      "    load_time_ms: 0.281\n",
      "    sample_throughput: 499.331\n",
      "    sample_time_ms: 8010.717\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1643387144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1604000\n",
      "  training_iteration: 401\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:48 (running for 00:57:20.17)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         3422.35</td><td style=\"text-align: right;\">1604000</td><td style=\"text-align: right;\">0.0595798</td><td style=\"text-align: right;\">             7.91819</td><td style=\"text-align: right;\">            -7.73224</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-25-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.45020616054535\n",
      "  episode_reward_mean: -0.09838065177202225\n",
      "  episode_reward_min: -10.418251067399979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.747185051024601\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014668498401373773\n",
      "          policy_loss: -0.11121034054285897\n",
      "          total_loss: 2.60631993829513\n",
      "          vf_explained_var: 0.88404008925602\n",
      "          vf_loss: 2.684113609310119\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1608000\n",
      "    num_agent_steps_trained: 1608000\n",
      "    num_steps_sampled: 1608000\n",
      "    num_steps_trained: 1608000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.79090909090909\n",
      "    ram_util_percent: 34.163636363636364\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10114249525076507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08915974622161765\n",
      "    mean_inference_ms: 0.9938200879310028\n",
      "    mean_raw_obs_processing_ms: 0.08384508618609952\n",
      "  time_since_restore: 3430.334722995758\n",
      "  time_this_iter_s: 7.983805418014526\n",
      "  time_total_s: 3430.334722995758\n",
      "  timers:\n",
      "    learn_throughput: 718.551\n",
      "    learn_time_ms: 5566.759\n",
      "    load_throughput: 14187920.507\n",
      "    load_time_ms: 0.282\n",
      "    sample_throughput: 500.184\n",
      "    sample_time_ms: 7997.065\n",
      "    update_time_ms: 1.329\n",
      "  timestamp: 1643387152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1608000\n",
      "  training_iteration: 402\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:53 (running for 00:57:25.17)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         3430.33</td><td style=\"text-align: right;\">1608000</td><td style=\"text-align: right;\">-0.0983807</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:25:58 (running for 00:57:30.17)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         3430.33</td><td style=\"text-align: right;\">1608000</td><td style=\"text-align: right;\">-0.0983807</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1612000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.45020616054535\n",
      "  episode_reward_mean: 0.03073559731245041\n",
      "  episode_reward_min: -10.418251067399979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7453960485355828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015499109413629081\n",
      "          policy_loss: -0.10600058477752472\n",
      "          total_loss: 0.6877103011564462\n",
      "          vf_explained_var: 0.9566988427792826\n",
      "          vf_loss: 0.7584019783563832\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1612000\n",
      "    num_agent_steps_trained: 1612000\n",
      "    num_steps_sampled: 1612000\n",
      "    num_steps_trained: 1612000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.545454545454547\n",
      "    ram_util_percent: 34.13636363636363\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1011192750863677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08914530770325266\n",
      "    mean_inference_ms: 0.9936027278408451\n",
      "    mean_raw_obs_processing_ms: 0.0838250731534294\n",
      "  time_since_restore: 3438.0798349380493\n",
      "  time_this_iter_s: 7.74511194229126\n",
      "  time_total_s: 3438.0798349380493\n",
      "  timers:\n",
      "    learn_throughput: 720.809\n",
      "    learn_time_ms: 5549.323\n",
      "    load_throughput: 14294296.669\n",
      "    load_time_ms: 0.28\n",
      "    sample_throughput: 502.191\n",
      "    sample_time_ms: 7965.096\n",
      "    update_time_ms: 1.272\n",
      "  timestamp: 1643387160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1612000\n",
      "  training_iteration: 403\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:04 (running for 00:57:35.93)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         3438.08</td><td style=\"text-align: right;\">1612000</td><td style=\"text-align: right;\">0.0307356</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:09 (running for 00:57:40.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         3438.08</td><td style=\"text-align: right;\">1612000</td><td style=\"text-align: right;\">0.0307356</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.45020616054535\n",
      "  episode_reward_mean: -0.0387124951928854\n",
      "  episode_reward_min: -10.418251067399979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7773716583527545\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014820275547783016\n",
      "          policy_loss: -0.12354473821738715\n",
      "          total_loss: 1.348148035029793\n",
      "          vf_explained_var: 0.9306096172460946\n",
      "          vf_loss: 1.4379303290078076\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1616000\n",
      "    num_agent_steps_trained: 1616000\n",
      "    num_steps_sampled: 1616000\n",
      "    num_steps_trained: 1616000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.07333333333333\n",
      "    ram_util_percent: 34.2\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10110191792627667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08913629461529435\n",
      "    mean_inference_ms: 0.9934435725969886\n",
      "    mean_raw_obs_processing_ms: 0.08381000835324315\n",
      "  time_since_restore: 3448.0619094371796\n",
      "  time_this_iter_s: 9.982074499130249\n",
      "  time_total_s: 3448.0619094371796\n",
      "  timers:\n",
      "    learn_throughput: 697.592\n",
      "    learn_time_ms: 5734.013\n",
      "    load_throughput: 14128181.895\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 502.612\n",
      "    sample_time_ms: 7958.422\n",
      "    update_time_ms: 1.259\n",
      "  timestamp: 1643387170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1616000\n",
      "  training_iteration: 404\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:14 (running for 00:57:45.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         3448.06</td><td style=\"text-align: right;\">1616000</td><td style=\"text-align: right;\">-0.0387125</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.45020616054535\n",
      "  episode_reward_mean: -0.026755122542381285\n",
      "  episode_reward_min: -10.418251067399979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7662278347758836\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01582221968771837\n",
      "          policy_loss: -0.10410805826516001\n",
      "          total_loss: 0.8379057612370259\n",
      "          vf_explained_var: 0.9561305130040774\n",
      "          vf_loss: 0.9059688182687887\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1620000\n",
      "    num_agent_steps_trained: 1620000\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.50909090909091\n",
      "    ram_util_percent: 34.20909090909091\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1010819474056818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08912612845850024\n",
      "    mean_inference_ms: 0.9932742925769021\n",
      "    mean_raw_obs_processing_ms: 0.08379486096238675\n",
      "  time_since_restore: 3456.256057024002\n",
      "  time_this_iter_s: 8.19414758682251\n",
      "  time_total_s: 3456.256057024002\n",
      "  timers:\n",
      "    learn_throughput: 696.96\n",
      "    learn_time_ms: 5739.208\n",
      "    load_throughput: 13514754.31\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 490.086\n",
      "    sample_time_ms: 8161.828\n",
      "    update_time_ms: 1.295\n",
      "  timestamp: 1643387178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 405\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:19 (running for 00:57:51.15)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         3456.26</td><td style=\"text-align: right;\">1620000</td><td style=\"text-align: right;\">-0.0267551</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:24 (running for 00:57:56.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         3456.26</td><td style=\"text-align: right;\">1620000</td><td style=\"text-align: right;\">-0.0267551</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.45020616054535\n",
      "  episode_reward_mean: -0.011679193526506424\n",
      "  episode_reward_min: -10.418251067399979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7248534971026964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015446395736930353\n",
      "          policy_loss: -0.08039148992217916\n",
      "          total_loss: 1.183031845666964\n",
      "          vf_explained_var: 0.9453708609586121\n",
      "          vf_loss: 1.2282345155554433\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1624000\n",
      "    num_agent_steps_trained: 1624000\n",
      "    num_steps_sampled: 1624000\n",
      "    num_steps_trained: 1624000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.400000000000002\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10106397840993159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08911465652270835\n",
      "    mean_inference_ms: 0.9931210461341061\n",
      "    mean_raw_obs_processing_ms: 0.08377993399568463\n",
      "  time_since_restore: 3464.087029695511\n",
      "  time_this_iter_s: 7.830972671508789\n",
      "  time_total_s: 3464.087029695511\n",
      "  timers:\n",
      "    learn_throughput: 699.918\n",
      "    learn_time_ms: 5714.955\n",
      "    load_throughput: 13483256.449\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 489.354\n",
      "    sample_time_ms: 8174.045\n",
      "    update_time_ms: 1.322\n",
      "  timestamp: 1643387186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1624000\n",
      "  training_iteration: 406\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:30 (running for 00:58:02.00)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         3464.09</td><td style=\"text-align: right;\">1624000</td><td style=\"text-align: right;\">-0.0116792</td><td style=\"text-align: right;\">             9.45021</td><td style=\"text-align: right;\">            -10.4183</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1628000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.465004563331604\n",
      "  episode_reward_mean: 0.10365992426872253\n",
      "  episode_reward_min: -11.146483048796654\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.8264417982229623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015298398075031026\n",
      "          policy_loss: -0.07945747609019921\n",
      "          total_loss: 2.201173129410393\n",
      "          vf_explained_var: 0.9208595761688807\n",
      "          vf_loss: 2.245778950203651\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1628000\n",
      "    num_agent_steps_trained: 1628000\n",
      "    num_steps_sampled: 1628000\n",
      "    num_steps_trained: 1628000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.754545454545454\n",
      "    ram_util_percent: 34.13636363636364\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1010497456884415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08910128365795465\n",
      "    mean_inference_ms: 0.9929623620546384\n",
      "    mean_raw_obs_processing_ms: 0.0837652489539539\n",
      "  time_since_restore: 3472.2263033390045\n",
      "  time_this_iter_s: 8.139273643493652\n",
      "  time_total_s: 3472.2263033390045\n",
      "  timers:\n",
      "    learn_throughput: 697.464\n",
      "    learn_time_ms: 5735.064\n",
      "    load_throughput: 13236462.327\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 491.114\n",
      "    sample_time_ms: 8144.748\n",
      "    update_time_ms: 1.339\n",
      "  timestamp: 1643387194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1628000\n",
      "  training_iteration: 407\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:35 (running for 00:58:07.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         3472.23</td><td style=\"text-align: right;\">1628000</td><td style=\"text-align: right;\"> 0.10366</td><td style=\"text-align: right;\">               6.465</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:40 (running for 00:58:12.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         3472.23</td><td style=\"text-align: right;\">1628000</td><td style=\"text-align: right;\"> 0.10366</td><td style=\"text-align: right;\">               6.465</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.47147150337696\n",
      "  episode_reward_mean: 0.018168266415596008\n",
      "  episode_reward_min: -11.146483048796654\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7827446004075389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015158342942659575\n",
      "          policy_loss: -0.0987615199238863\n",
      "          total_loss: 2.443978782173907\n",
      "          vf_explained_var: 0.8935607534582897\n",
      "          vf_loss: 2.508207706209793\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1632000\n",
      "    num_agent_steps_trained: 1632000\n",
      "    num_steps_sampled: 1632000\n",
      "    num_steps_trained: 1632000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.136363636363637\n",
      "    ram_util_percent: 34.13636363636364\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10103587962844796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08908831082498044\n",
      "    mean_inference_ms: 0.9928066133946511\n",
      "    mean_raw_obs_processing_ms: 0.08375072781489809\n",
      "  time_since_restore: 3480.0919563770294\n",
      "  time_this_iter_s: 7.865653038024902\n",
      "  time_total_s: 3480.0919563770294\n",
      "  timers:\n",
      "    learn_throughput: 695.504\n",
      "    learn_time_ms: 5751.228\n",
      "    load_throughput: 13232286.458\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 490.236\n",
      "    sample_time_ms: 8159.338\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1643387202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1632000\n",
      "  training_iteration: 408\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:46 (running for 00:58:18.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         3480.09</td><td style=\"text-align: right;\">1632000</td><td style=\"text-align: right;\">0.0181683</td><td style=\"text-align: right;\">             9.47147</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1636000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.47147150337696\n",
      "  episode_reward_mean: 0.021434179469943047\n",
      "  episode_reward_min: -11.146483048796654\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7528170954476121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014826152189786564\n",
      "          policy_loss: -0.0888382283359846\n",
      "          total_loss: 2.6115745816610114\n",
      "          vf_explained_var: 0.8888565402518036\n",
      "          vf_loss: 2.6666369757104302\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1636000\n",
      "    num_agent_steps_trained: 1636000\n",
      "    num_steps_sampled: 1636000\n",
      "    num_steps_trained: 1636000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.491666666666664\n",
      "    ram_util_percent: 34.13333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10102441709065718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08907531051836502\n",
      "    mean_inference_ms: 0.9926580449748136\n",
      "    mean_raw_obs_processing_ms: 0.08373680206741767\n",
      "  time_since_restore: 3488.2116963863373\n",
      "  time_this_iter_s: 8.119740009307861\n",
      "  time_total_s: 3488.2116963863373\n",
      "  timers:\n",
      "    learn_throughput: 694.718\n",
      "    learn_time_ms: 5757.728\n",
      "    load_throughput: 13290989.464\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 487.19\n",
      "    sample_time_ms: 8210.349\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1643387210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1636000\n",
      "  training_iteration: 409\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:51 (running for 00:58:23.19)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         3488.21</td><td style=\"text-align: right;\">1636000</td><td style=\"text-align: right;\">0.0214342</td><td style=\"text-align: right;\">             9.47147</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:26:56 (running for 00:58:28.20)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         3488.21</td><td style=\"text-align: right;\">1636000</td><td style=\"text-align: right;\">0.0214342</td><td style=\"text-align: right;\">             9.47147</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.47147150337696\n",
      "  episode_reward_mean: -0.030996256172657013\n",
      "  episode_reward_min: -11.146483048796654\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.7997654151211503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015302304731200193\n",
      "          policy_loss: -0.09457491917801039\n",
      "          total_loss: 1.6417333311095874\n",
      "          vf_explained_var: 0.9158282749114498\n",
      "          vf_loss: 1.7014476798154334\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1640000\n",
      "    num_agent_steps_trained: 1640000\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.909090909090907\n",
      "    ram_util_percent: 34.10909090909092\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1010093196214293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08905974712367676\n",
      "    mean_inference_ms: 0.9924653606924495\n",
      "    mean_raw_obs_processing_ms: 0.08372291580906666\n",
      "  time_since_restore: 3495.988450527191\n",
      "  time_this_iter_s: 7.776754140853882\n",
      "  time_total_s: 3495.988450527191\n",
      "  timers:\n",
      "    learn_throughput: 695.93\n",
      "    learn_time_ms: 5747.708\n",
      "    load_throughput: 13337479.927\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 488.42\n",
      "    sample_time_ms: 8189.676\n",
      "    update_time_ms: 1.438\n",
      "  timestamp: 1643387218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 410\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:02 (running for 00:58:33.99)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         3495.99</td><td style=\"text-align: right;\">1640000</td><td style=\"text-align: right;\">-0.0309963</td><td style=\"text-align: right;\">             9.47147</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1644000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.47147150337696\n",
      "  episode_reward_mean: -0.10901803709566593\n",
      "  episode_reward_min: -11.146483048796654\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.8887683095470551\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015538870708164875\n",
      "          policy_loss: -0.12483845840418531\n",
      "          total_loss: 1.693622193977322\n",
      "          vf_explained_var: 0.924503715256209\n",
      "          vf_loss: 1.7830611695685694\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1644000\n",
      "    num_agent_steps_trained: 1644000\n",
      "    num_steps_sampled: 1644000\n",
      "    num_steps_trained: 1644000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.263636363636362\n",
      "    ram_util_percent: 34.13636363636363\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10099494773945995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0890444544726526\n",
      "    mean_inference_ms: 0.9922909686193213\n",
      "    mean_raw_obs_processing_ms: 0.08370936582827275\n",
      "  time_since_restore: 3503.8871665000916\n",
      "  time_this_iter_s: 7.898715972900391\n",
      "  time_total_s: 3503.8871665000916\n",
      "  timers:\n",
      "    learn_throughput: 698.124\n",
      "    learn_time_ms: 5729.639\n",
      "    load_throughput: 13160665.202\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 488.22\n",
      "    sample_time_ms: 8193.029\n",
      "    update_time_ms: 1.448\n",
      "  timestamp: 1643387226\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1644000\n",
      "  training_iteration: 411\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:08 (running for 00:58:39.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         3503.89</td><td style=\"text-align: right;\">1644000</td><td style=\"text-align: right;\">-0.109018</td><td style=\"text-align: right;\">             9.47147</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:13 (running for 00:58:44.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         3503.89</td><td style=\"text-align: right;\">1644000</td><td style=\"text-align: right;\">-0.109018</td><td style=\"text-align: right;\">             9.47147</td><td style=\"text-align: right;\">            -11.1465</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-27-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.47147150337696\n",
      "  episode_reward_mean: -0.10483303874731063\n",
      "  episode_reward_min: -10.285030901432037\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.8603064459177756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015559056192144854\n",
      "          policy_loss: -0.1252652382027478\n",
      "          total_loss: 1.5061395280225884\n",
      "          vf_explained_var: 0.9341707055927605\n",
      "          vf_loss: 1.5959592853422446\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1648000\n",
      "    num_agent_steps_trained: 1648000\n",
      "    num_steps_sampled: 1648000\n",
      "    num_steps_trained: 1648000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.472727272727273\n",
      "    ram_util_percent: 34.10909090909092\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10097000697760235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08902564190862951\n",
      "    mean_inference_ms: 0.9920638405182893\n",
      "    mean_raw_obs_processing_ms: 0.08369219104295869\n",
      "  time_since_restore: 3511.5864539146423\n",
      "  time_this_iter_s: 7.699287414550781\n",
      "  time_total_s: 3511.5864539146423\n",
      "  timers:\n",
      "    learn_throughput: 697.756\n",
      "    learn_time_ms: 5732.666\n",
      "    load_throughput: 13135934.858\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 491.248\n",
      "    sample_time_ms: 8142.535\n",
      "    update_time_ms: 1.461\n",
      "  timestamp: 1643387233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1648000\n",
      "  training_iteration: 412\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:18 (running for 00:58:50.63)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         3511.59</td><td style=\"text-align: right;\">1648000</td><td style=\"text-align: right;\">-0.104833</td><td style=\"text-align: right;\">             9.47147</td><td style=\"text-align: right;\">             -10.285</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1652000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.142776638269424\n",
      "  episode_reward_mean: -0.12419577524065971\n",
      "  episode_reward_min: -9.668078392744064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.910390588160484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01474569295461176\n",
      "          policy_loss: -0.12268625452453572\n",
      "          total_loss: 2.1418181341013782\n",
      "          vf_explained_var: 0.9101055136931839\n",
      "          vf_loss: 2.230911845902121\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1652000\n",
      "    num_agent_steps_trained: 1652000\n",
      "    num_steps_sampled: 1652000\n",
      "    num_steps_trained: 1652000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.741666666666664\n",
      "    ram_util_percent: 34.13333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10094703832359254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08900912638193854\n",
      "    mean_inference_ms: 0.9918643645134813\n",
      "    mean_raw_obs_processing_ms: 0.08367671493803837\n",
      "  time_since_restore: 3519.5339179039\n",
      "  time_this_iter_s: 7.9474639892578125\n",
      "  time_total_s: 3519.5339179039\n",
      "  timers:\n",
      "    learn_throughput: 697.045\n",
      "    learn_time_ms: 5738.514\n",
      "    load_throughput: 13039962.692\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 490.179\n",
      "    sample_time_ms: 8160.282\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1643387241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1652000\n",
      "  training_iteration: 413\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:24 (running for 00:58:56.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         3519.53</td><td style=\"text-align: right;\">1652000</td><td style=\"text-align: right;\">-0.124196</td><td style=\"text-align: right;\">             10.1428</td><td style=\"text-align: right;\">            -9.66808</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:29 (running for 00:59:01.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         3519.53</td><td style=\"text-align: right;\">1652000</td><td style=\"text-align: right;\">-0.124196</td><td style=\"text-align: right;\">             10.1428</td><td style=\"text-align: right;\">            -9.66808</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.142776638269424\n",
      "  episode_reward_mean: -0.005651460289955139\n",
      "  episode_reward_min: -9.668078392744064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.8597753922144572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015387124850814345\n",
      "          policy_loss: -0.09466076123087556\n",
      "          total_loss: 1.5564263464650139\n",
      "          vf_explained_var: 0.9272956450139322\n",
      "          vf_loss: 1.6160333132391336\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1656000\n",
      "    num_agent_steps_trained: 1656000\n",
      "    num_steps_sampled: 1656000\n",
      "    num_steps_trained: 1656000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.836363636363636\n",
      "    ram_util_percent: 34.14545454545454\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10092131020799609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08899370772625292\n",
      "    mean_inference_ms: 0.99164767345795\n",
      "    mean_raw_obs_processing_ms: 0.08365904149584852\n",
      "  time_since_restore: 3527.670615673065\n",
      "  time_this_iter_s: 8.136697769165039\n",
      "  time_total_s: 3527.670615673065\n",
      "  timers:\n",
      "    learn_throughput: 719.502\n",
      "    learn_time_ms: 5559.401\n",
      "    load_throughput: 13183416.627\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 490.131\n",
      "    sample_time_ms: 8161.089\n",
      "    update_time_ms: 1.497\n",
      "  timestamp: 1643387250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1656000\n",
      "  training_iteration: 414\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:35 (running for 00:59:06.76)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         3527.67</td><td style=\"text-align: right;\">1656000</td><td style=\"text-align: right;\">-0.00565146</td><td style=\"text-align: right;\">             10.1428</td><td style=\"text-align: right;\">            -9.66808</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-27-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.142776638269424\n",
      "  episode_reward_mean: 0.10172877997159958\n",
      "  episode_reward_min: -9.668078392744064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.9514215025850522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014763328818210462\n",
      "          policy_loss: -0.08228167528386718\n",
      "          total_loss: 1.1995894769574666\n",
      "          vf_explained_var: 0.9410578671962985\n",
      "          vf_loss: 1.2482384414682466\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1660000\n",
      "    num_agent_steps_trained: 1660000\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1660000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.408333333333335\n",
      "    ram_util_percent: 34.11666666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1009005728729293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08897964778931952\n",
      "    mean_inference_ms: 0.9914477757379943\n",
      "    mean_raw_obs_processing_ms: 0.0836397285395195\n",
      "  time_since_restore: 3535.6806774139404\n",
      "  time_this_iter_s: 8.010061740875244\n",
      "  time_total_s: 3535.6806774139404\n",
      "  timers:\n",
      "    learn_throughput: 720.311\n",
      "    learn_time_ms: 5553.158\n",
      "    load_throughput: 13743930.532\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 501.877\n",
      "    sample_time_ms: 7970.077\n",
      "    update_time_ms: 1.478\n",
      "  timestamp: 1643387258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 415\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:40 (running for 00:59:11.79)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         3535.68</td><td style=\"text-align: right;\">1660000</td><td style=\"text-align: right;\">0.101729</td><td style=\"text-align: right;\">             10.1428</td><td style=\"text-align: right;\">            -9.66808</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:45 (running for 00:59:16.80)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         3535.68</td><td style=\"text-align: right;\">1660000</td><td style=\"text-align: right;\">0.101729</td><td style=\"text-align: right;\">             10.1428</td><td style=\"text-align: right;\">            -9.66808</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-27-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.142776638269424\n",
      "  episode_reward_mean: 0.07279610253870487\n",
      "  episode_reward_min: -9.668078392744064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.015783755561357\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015638565703206705\n",
      "          policy_loss: -0.07532966234871456\n",
      "          total_loss: 1.9008228871230317\n",
      "          vf_explained_var: 0.9121408535588172\n",
      "          vf_loss: 1.940525931848954\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1664000\n",
      "    num_agent_steps_trained: 1664000\n",
      "    num_steps_sampled: 1664000\n",
      "    num_steps_trained: 1664000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.200000000000003\n",
      "    ram_util_percent: 34.154545454545456\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10087968890502541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0889650355303857\n",
      "    mean_inference_ms: 0.991228740661271\n",
      "    mean_raw_obs_processing_ms: 0.08362178482904387\n",
      "  time_since_restore: 3543.460497379303\n",
      "  time_this_iter_s: 7.779819965362549\n",
      "  time_total_s: 3543.460497379303\n",
      "  timers:\n",
      "    learn_throughput: 720.976\n",
      "    learn_time_ms: 5548.038\n",
      "    load_throughput: 13755198.819\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 502.274\n",
      "    sample_time_ms: 7963.786\n",
      "    update_time_ms: 1.452\n",
      "  timestamp: 1643387265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1664000\n",
      "  training_iteration: 416\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:50 (running for 00:59:22.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         3543.46</td><td style=\"text-align: right;\">1664000</td><td style=\"text-align: right;\">0.0727961</td><td style=\"text-align: right;\">             10.1428</td><td style=\"text-align: right;\">            -9.66808</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1668000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-27-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.142776638269424\n",
      "  episode_reward_mean: 0.08499695926904678\n",
      "  episode_reward_min: -9.668078392744064\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.9908963937592763\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015678155804943375\n",
      "          policy_loss: -0.10439957653632968\n",
      "          total_loss: 2.171615012686309\n",
      "          vf_explained_var: 0.9019958693494079\n",
      "          vf_loss: 2.240297795539742\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1668000\n",
      "    num_agent_steps_trained: 1668000\n",
      "    num_steps_sampled: 1668000\n",
      "    num_steps_trained: 1668000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.63636363636363\n",
      "    ram_util_percent: 34.127272727272725\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10086018893516581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08895209386310513\n",
      "    mean_inference_ms: 0.9910353650681063\n",
      "    mean_raw_obs_processing_ms: 0.08360487434081787\n",
      "  time_since_restore: 3551.4072058200836\n",
      "  time_this_iter_s: 7.94670844078064\n",
      "  time_total_s: 3551.4072058200836\n",
      "  timers:\n",
      "    learn_throughput: 720.69\n",
      "    learn_time_ms: 5550.234\n",
      "    load_throughput: 13990340.227\n",
      "    load_time_ms: 0.286\n",
      "    sample_throughput: 503.95\n",
      "    sample_time_ms: 7937.301\n",
      "    update_time_ms: 1.438\n",
      "  timestamp: 1643387273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1668000\n",
      "  training_iteration: 417\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:27:56 (running for 00:59:28.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         3551.41</td><td style=\"text-align: right;\">1668000</td><td style=\"text-align: right;\">0.084997</td><td style=\"text-align: right;\">             10.1428</td><td style=\"text-align: right;\">            -9.66808</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.999996930360794\n",
      "  episode_reward_mean: 0.0618656562268734\n",
      "  episode_reward_min: -7.4838893711566925\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -0.9754570277147395\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01583930408370698\n",
      "          policy_loss: -0.12632406700843124\n",
      "          total_loss: 1.3831094842564355\n",
      "          vf_explained_var: 0.9279600586942447\n",
      "          vf_loss: 1.4733496376423425\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1672000\n",
      "    num_agent_steps_trained: 1672000\n",
      "    num_steps_sampled: 1672000\n",
      "    num_steps_trained: 1672000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.016666666666662\n",
      "    ram_util_percent: 34.15\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1008406036393302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08893856421833762\n",
      "    mean_inference_ms: 0.9908465891280588\n",
      "    mean_raw_obs_processing_ms: 0.08358966720646219\n",
      "  time_since_restore: 3559.322313785553\n",
      "  time_this_iter_s: 7.91510796546936\n",
      "  time_total_s: 3559.322313785553\n",
      "  timers:\n",
      "    learn_throughput: 721.975\n",
      "    learn_time_ms: 5540.359\n",
      "    load_throughput: 13962396.804\n",
      "    load_time_ms: 0.286\n",
      "    sample_throughput: 502.864\n",
      "    sample_time_ms: 7954.44\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1643387281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1672000\n",
      "  training_iteration: 418\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:02 (running for 00:59:34.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         3559.32</td><td style=\"text-align: right;\">1672000</td><td style=\"text-align: right;\">0.0618657</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">            -7.48389</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:07 (running for 00:59:39.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         3559.32</td><td style=\"text-align: right;\">1672000</td><td style=\"text-align: right;\">0.0618657</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">            -7.48389</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1676000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.999996930360794\n",
      "  episode_reward_mean: 0.04364494502544403\n",
      "  episode_reward_min: -7.4838893711566925\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.0957091407750243\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016594091471241215\n",
      "          policy_loss: -0.10838772784718262\n",
      "          total_loss: 1.3366852504802087\n",
      "          vf_explained_var: 0.9244550517810288\n",
      "          vf_loss: 1.4072695611385249\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1676000\n",
      "    num_agent_steps_trained: 1676000\n",
      "    num_steps_sampled: 1676000\n",
      "    num_steps_trained: 1676000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.325\n",
      "    ram_util_percent: 34.13333333333335\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1008235855388756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08892463869579678\n",
      "    mean_inference_ms: 0.9906887317309244\n",
      "    mean_raw_obs_processing_ms: 0.08357728271161274\n",
      "  time_since_restore: 3567.5895183086395\n",
      "  time_this_iter_s: 8.267204523086548\n",
      "  time_total_s: 3567.5895183086395\n",
      "  timers:\n",
      "    learn_throughput: 720.692\n",
      "    learn_time_ms: 5550.22\n",
      "    load_throughput: 14012541.552\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 503.179\n",
      "    sample_time_ms: 7949.457\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1643387290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1676000\n",
      "  training_iteration: 419\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:13 (running for 00:59:44.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         3567.59</td><td style=\"text-align: right;\">1676000</td><td style=\"text-align: right;\">0.0436449</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">            -7.48389</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.287649512290955\n",
      "  episode_reward_mean: -0.08325399816036225\n",
      "  episode_reward_min: -7.4838893711566925\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.0969283591675503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016612547981148973\n",
      "          policy_loss: -0.12024750499735756\n",
      "          total_loss: 2.4359452166249076\n",
      "          vf_explained_var: 0.8813404525479963\n",
      "          vf_loss: 2.518347257039239\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1680000\n",
      "    num_agent_steps_trained: 1680000\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.236363636363635\n",
      "    ram_util_percent: 34.118181818181824\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10080343433638346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08890918529844168\n",
      "    mean_inference_ms: 0.9905245468679703\n",
      "    mean_raw_obs_processing_ms: 0.08356377049744143\n",
      "  time_since_restore: 3575.458864212036\n",
      "  time_this_iter_s: 7.8693459033966064\n",
      "  time_total_s: 3575.458864212036\n",
      "  timers:\n",
      "    learn_throughput: 719.737\n",
      "    learn_time_ms: 5557.584\n",
      "    load_throughput: 14065405.768\n",
      "    load_time_ms: 0.284\n",
      "    sample_throughput: 502.424\n",
      "    sample_time_ms: 7961.401\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1643387297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 420\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:19 (running for 00:59:50.67)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         3575.46</td><td style=\"text-align: right;\">1680000</td><td style=\"text-align: right;\">-0.083254</td><td style=\"text-align: right;\">             8.28765</td><td style=\"text-align: right;\">            -7.48389</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:24 (running for 00:59:55.68)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         3575.46</td><td style=\"text-align: right;\">1680000</td><td style=\"text-align: right;\">-0.083254</td><td style=\"text-align: right;\">             8.28765</td><td style=\"text-align: right;\">            -7.48389</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1684000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.287649512290955\n",
      "  episode_reward_mean: 0.0265470552444458\n",
      "  episode_reward_min: -9.821950018405914\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.1412912780238735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015381700631266471\n",
      "          policy_loss: -0.0757114005785796\n",
      "          total_loss: 1.7259941597719506\n",
      "          vf_explained_var: 0.9403925028539473\n",
      "          vf_loss: 1.766664126651582\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1684000\n",
      "    num_agent_steps_trained: 1684000\n",
      "    num_steps_sampled: 1684000\n",
      "    num_steps_trained: 1684000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.163636363636364\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10077838755507498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08889031924514919\n",
      "    mean_inference_ms: 0.990317612990475\n",
      "    mean_raw_obs_processing_ms: 0.0835451648866756\n",
      "  time_since_restore: 3582.986801147461\n",
      "  time_this_iter_s: 7.527936935424805\n",
      "  time_total_s: 3582.986801147461\n",
      "  timers:\n",
      "    learn_throughput: 720.144\n",
      "    learn_time_ms: 5554.443\n",
      "    load_throughput: 14225212.82\n",
      "    load_time_ms: 0.281\n",
      "    sample_throughput: 504.104\n",
      "    sample_time_ms: 7934.866\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1643387305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1684000\n",
      "  training_iteration: 421\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:29 (running for 01:00:01.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         3582.99</td><td style=\"text-align: right;\">1684000</td><td style=\"text-align: right;\">0.0265471</td><td style=\"text-align: right;\">             8.28765</td><td style=\"text-align: right;\">            -9.82195</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.287649512290955\n",
      "  episode_reward_mean: -0.06903924584388733\n",
      "  episode_reward_min: -9.821950018405914\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.148667537332863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014625198728286368\n",
      "          policy_loss: -0.10958828173267345\n",
      "          total_loss: 0.9602431033712922\n",
      "          vf_explained_var: 0.9462739334952447\n",
      "          vf_loss: 1.0365133574052203\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1688000\n",
      "    num_agent_steps_trained: 1688000\n",
      "    num_steps_sampled: 1688000\n",
      "    num_steps_trained: 1688000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.249999999999996\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10075062240141239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0888686396393138\n",
      "    mean_inference_ms: 0.9900736048145284\n",
      "    mean_raw_obs_processing_ms: 0.08352435658544946\n",
      "  time_since_restore: 3590.5185446739197\n",
      "  time_this_iter_s: 7.53174352645874\n",
      "  time_total_s: 3590.5185446739197\n",
      "  timers:\n",
      "    learn_throughput: 721.241\n",
      "    learn_time_ms: 5546.0\n",
      "    load_throughput: 14386225.347\n",
      "    load_time_ms: 0.278\n",
      "    sample_throughput: 504.826\n",
      "    sample_time_ms: 7923.528\n",
      "    update_time_ms: 1.324\n",
      "  timestamp: 1643387313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1688000\n",
      "  training_iteration: 422\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:35 (running for 01:00:06.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         3590.52</td><td style=\"text-align: right;\">1688000</td><td style=\"text-align: right;\">-0.0690392</td><td style=\"text-align: right;\">             8.28765</td><td style=\"text-align: right;\">            -9.82195</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:40 (running for 01:00:11.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         3590.52</td><td style=\"text-align: right;\">1688000</td><td style=\"text-align: right;\">-0.0690392</td><td style=\"text-align: right;\">             8.28765</td><td style=\"text-align: right;\">            -9.82195</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1692000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.287649512290955\n",
      "  episode_reward_mean: 0.02626507878303528\n",
      "  episode_reward_min: -9.821950018405914\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.1324259021589833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016152433282572237\n",
      "          policy_loss: -0.11842500158154996\n",
      "          total_loss: 1.092070811402343\n",
      "          vf_explained_var: 0.9462310953806805\n",
      "          vf_loss: 1.1736985575808312\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1692000\n",
      "    num_agent_steps_trained: 1692000\n",
      "    num_steps_sampled: 1692000\n",
      "    num_steps_trained: 1692000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.972727272727273\n",
      "    ram_util_percent: 34.11818181818182\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10072020218709767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0888443543276534\n",
      "    mean_inference_ms: 0.9897896727991408\n",
      "    mean_raw_obs_processing_ms: 0.08349960696077748\n",
      "  time_since_restore: 3597.986681699753\n",
      "  time_this_iter_s: 7.46813702583313\n",
      "  time_total_s: 3597.986681699753\n",
      "  timers:\n",
      "    learn_throughput: 724.86\n",
      "    learn_time_ms: 5518.311\n",
      "    load_throughput: 14368975.677\n",
      "    load_time_ms: 0.278\n",
      "    sample_throughput: 506.685\n",
      "    sample_time_ms: 7894.453\n",
      "    update_time_ms: 1.306\n",
      "  timestamp: 1643387320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1692000\n",
      "  training_iteration: 423\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:45 (running for 01:00:17.27)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         3597.99</td><td style=\"text-align: right;\">1692000</td><td style=\"text-align: right;\">0.0262651</td><td style=\"text-align: right;\">             8.28765</td><td style=\"text-align: right;\">            -9.82195</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.287649512290955\n",
      "  episode_reward_mean: 0.028758253455162048\n",
      "  episode_reward_min: -9.821950018405914\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.1849311808745067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015979198576860393\n",
      "          policy_loss: -0.08096451367081334\n",
      "          total_loss: 1.387150188225011\n",
      "          vf_explained_var: 0.9336869844185409\n",
      "          vf_loss: 1.4317120923069857\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1696000\n",
      "    num_agent_steps_trained: 1696000\n",
      "    num_steps_sampled: 1696000\n",
      "    num_steps_trained: 1696000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.372727272727275\n",
      "    ram_util_percent: 34.10909090909092\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10068236617171532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08881528447974646\n",
      "    mean_inference_ms: 0.9894254802048362\n",
      "    mean_raw_obs_processing_ms: 0.08346877900746623\n",
      "  time_since_restore: 3605.3379230499268\n",
      "  time_this_iter_s: 7.35124135017395\n",
      "  time_total_s: 3605.3379230499268\n",
      "  timers:\n",
      "    learn_throughput: 731.542\n",
      "    learn_time_ms: 5467.904\n",
      "    load_throughput: 14470602.036\n",
      "    load_time_ms: 0.276\n",
      "    sample_throughput: 510.289\n",
      "    sample_time_ms: 7838.696\n",
      "    update_time_ms: 1.294\n",
      "  timestamp: 1643387327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1696000\n",
      "  training_iteration: 424\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:50 (running for 01:00:22.64)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         3605.34</td><td style=\"text-align: right;\">1696000</td><td style=\"text-align: right;\">0.0287583</td><td style=\"text-align: right;\">             8.28765</td><td style=\"text-align: right;\">            -9.82195</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.3767974972724915\n",
      "  episode_reward_mean: -0.07186055064201355\n",
      "  episode_reward_min: -9.821950018405914\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8500\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.2630741230262224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01552508634957149\n",
      "          policy_loss: -0.12467186630312954\n",
      "          total_loss: 2.2956946818583637\n",
      "          vf_explained_var: 0.9116876134949345\n",
      "          vf_loss: 2.3849984573100205\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1700000\n",
      "    num_agent_steps_trained: 1700000\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.06\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10064339591319398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08878595755663628\n",
      "    mean_inference_ms: 0.98904093894728\n",
      "    mean_raw_obs_processing_ms: 0.08343579325976469\n",
      "  time_since_restore: 3612.9439113140106\n",
      "  time_this_iter_s: 7.605988264083862\n",
      "  time_total_s: 3612.9439113140106\n",
      "  timers:\n",
      "    learn_throughput: 734.701\n",
      "    learn_time_ms: 5444.39\n",
      "    load_throughput: 14592690.267\n",
      "    load_time_ms: 0.274\n",
      "    sample_throughput: 514.713\n",
      "    sample_time_ms: 7771.327\n",
      "    update_time_ms: 1.282\n",
      "  timestamp: 1643387335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 425\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:28:56 (running for 01:00:28.26)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         3612.94</td><td style=\"text-align: right;\">1700000</td><td style=\"text-align: right;\">-0.0718606</td><td style=\"text-align: right;\">              7.3768</td><td style=\"text-align: right;\">            -9.82195</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:01 (running for 01:00:33.27)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         3612.94</td><td style=\"text-align: right;\">1700000</td><td style=\"text-align: right;\">-0.0718606</td><td style=\"text-align: right;\">              7.3768</td><td style=\"text-align: right;\">            -9.82195</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-29-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.466769486665726\n",
      "  episode_reward_mean: 0.025141668766736985\n",
      "  episode_reward_min: -8.524196773767471\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8520\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.1983847701421348\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015316397660871724\n",
      "          policy_loss: -0.09821941902039832\n",
      "          total_loss: 2.231505541850422\n",
      "          vf_explained_var: 0.9160932336443214\n",
      "          vf_loss: 2.294832281539998\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1704000\n",
      "    num_agent_steps_trained: 1704000\n",
      "    num_steps_sampled: 1704000\n",
      "    num_steps_trained: 1704000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.472727272727273\n",
      "    ram_util_percent: 34.163636363636364\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10060573075281475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08876130678120774\n",
      "    mean_inference_ms: 0.9886627730953269\n",
      "    mean_raw_obs_processing_ms: 0.08340339379368344\n",
      "  time_since_restore: 3620.4357323646545\n",
      "  time_this_iter_s: 7.491821050643921\n",
      "  time_total_s: 3620.4357323646545\n",
      "  timers:\n",
      "    learn_throughput: 735.883\n",
      "    learn_time_ms: 5435.646\n",
      "    load_throughput: 14671811.106\n",
      "    load_time_ms: 0.273\n",
      "    sample_throughput: 517.628\n",
      "    sample_time_ms: 7727.562\n",
      "    update_time_ms: 1.27\n",
      "  timestamp: 1643387343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1704000\n",
      "  training_iteration: 426\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:07 (running for 01:00:38.78)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         3620.44</td><td style=\"text-align: right;\">1704000</td><td style=\"text-align: right;\">0.0251417</td><td style=\"text-align: right;\">             8.46677</td><td style=\"text-align: right;\">             -8.5242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1708000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-29-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.466769486665726\n",
      "  episode_reward_mean: 0.03177078068256378\n",
      "  episode_reward_min: -8.524196773767471\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8540\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3078830728607793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015771522843881215\n",
      "          policy_loss: -0.10264673945544067\n",
      "          total_loss: 1.044822759973386\n",
      "          vf_explained_var: 0.933984451524673\n",
      "          vf_loss: 1.1115399970002071\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1708000\n",
      "    num_agent_steps_trained: 1708000\n",
      "    num_steps_sampled: 1708000\n",
      "    num_steps_trained: 1708000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.227272727272727\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10057140717233802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08873954934523973\n",
      "    mean_inference_ms: 0.9883215363144686\n",
      "    mean_raw_obs_processing_ms: 0.08337370774049532\n",
      "  time_since_restore: 3627.981960773468\n",
      "  time_this_iter_s: 7.546228408813477\n",
      "  time_total_s: 3627.981960773468\n",
      "  timers:\n",
      "    learn_throughput: 741.274\n",
      "    learn_time_ms: 5396.114\n",
      "    load_throughput: 14670528.157\n",
      "    load_time_ms: 0.273\n",
      "    sample_throughput: 518.256\n",
      "    sample_time_ms: 7718.2\n",
      "    update_time_ms: 1.301\n",
      "  timestamp: 1643387350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1708000\n",
      "  training_iteration: 427\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:12 (running for 01:00:44.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         3627.98</td><td style=\"text-align: right;\">1708000</td><td style=\"text-align: right;\">0.0317708</td><td style=\"text-align: right;\">             8.46677</td><td style=\"text-align: right;\">             -8.5242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:17 (running for 01:00:49.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         3627.98</td><td style=\"text-align: right;\">1708000</td><td style=\"text-align: right;\">0.0317708</td><td style=\"text-align: right;\">             8.46677</td><td style=\"text-align: right;\">             -8.5242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.466769486665726\n",
      "  episode_reward_mean: 0.030120776295661927\n",
      "  episode_reward_min: -8.524196773767471\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8560\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.2974805772945446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014941152701123355\n",
      "          policy_loss: -0.09938559095144912\n",
      "          total_loss: 2.0720833887956958\n",
      "          vf_explained_var: 0.9030211953706638\n",
      "          vf_loss: 2.137431170282665\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1712000\n",
      "    num_agent_steps_trained: 1712000\n",
      "    num_steps_sampled: 1712000\n",
      "    num_steps_trained: 1712000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.766666666666666\n",
      "    ram_util_percent: 34.15833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10053825743962644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08871825075188115\n",
      "    mean_inference_ms: 0.9879926349710888\n",
      "    mean_raw_obs_processing_ms: 0.08334460231624656\n",
      "  time_since_restore: 3636.501842021942\n",
      "  time_this_iter_s: 8.519881248474121\n",
      "  time_total_s: 3636.501842021942\n",
      "  timers:\n",
      "    learn_throughput: 730.909\n",
      "    learn_time_ms: 5472.634\n",
      "    load_throughput: 14535796.222\n",
      "    load_time_ms: 0.275\n",
      "    sample_throughput: 522.007\n",
      "    sample_time_ms: 7662.733\n",
      "    update_time_ms: 1.359\n",
      "  timestamp: 1643387359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1712000\n",
      "  training_iteration: 428\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:23 (running for 01:00:54.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">          3636.5</td><td style=\"text-align: right;\">1712000</td><td style=\"text-align: right;\">0.0301208</td><td style=\"text-align: right;\">             8.46677</td><td style=\"text-align: right;\">             -8.5242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:28 (running for 01:00:59.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">          3636.5</td><td style=\"text-align: right;\">1712000</td><td style=\"text-align: right;\">0.0301208</td><td style=\"text-align: right;\">             8.46677</td><td style=\"text-align: right;\">             -8.5242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1716000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.748364686965942\n",
      "  episode_reward_mean: -0.03288050800561905\n",
      "  episode_reward_min: -8.524196773767471\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8580\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.1997453390270152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016119605223574855\n",
      "          policy_loss: -0.10215355737174871\n",
      "          total_loss: 1.7748460629964948\n",
      "          vf_explained_var: 0.9049941186622906\n",
      "          vf_loss: 1.8402771345710243\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1716000\n",
      "    num_agent_steps_trained: 1716000\n",
      "    num_steps_sampled: 1716000\n",
      "    num_steps_trained: 1716000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.364285714285707\n",
      "    ram_util_percent: 34.24285714285715\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1005259150138603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08871494115544987\n",
      "    mean_inference_ms: 0.987895058085129\n",
      "    mean_raw_obs_processing_ms: 0.08333371253413169\n",
      "  time_since_restore: 3646.377193927765\n",
      "  time_this_iter_s: 9.875351905822754\n",
      "  time_total_s: 3646.377193927765\n",
      "  timers:\n",
      "    learn_throughput: 720.684\n",
      "    learn_time_ms: 5550.286\n",
      "    load_throughput: 13119499.531\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 511.282\n",
      "    sample_time_ms: 7823.477\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1643387369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1716000\n",
      "  training_iteration: 429\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:34 (running for 01:01:05.80)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         3646.38</td><td style=\"text-align: right;\">1716000</td><td style=\"text-align: right;\">-0.0328805</td><td style=\"text-align: right;\">             8.74836</td><td style=\"text-align: right;\">             -8.5242</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-29-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.748364686965942\n",
      "  episode_reward_mean: 0.15713504552841187\n",
      "  episode_reward_min: -9.024903506040573\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8600\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.2767788959446773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017201395344080904\n",
      "          policy_loss: -0.1001381993964715\n",
      "          total_loss: 1.6301706934753324\n",
      "          vf_explained_var: 0.9323949872165598\n",
      "          vf_loss: 1.691121967469332\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1720000\n",
      "    num_agent_steps_trained: 1720000\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.10909090909091\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1005129251872211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0887100379752377\n",
      "    mean_inference_ms: 0.9878026238423152\n",
      "    mean_raw_obs_processing_ms: 0.0833236021201287\n",
      "  time_since_restore: 3654.079046726227\n",
      "  time_this_iter_s: 7.701852798461914\n",
      "  time_total_s: 3654.079046726227\n",
      "  timers:\n",
      "    learn_throughput: 721.557\n",
      "    learn_time_ms: 5543.57\n",
      "    load_throughput: 13004585.691\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 506.926\n",
      "    sample_time_ms: 7890.697\n",
      "    update_time_ms: 1.329\n",
      "  timestamp: 1643387376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 430\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:39 (running for 01:01:11.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         3654.08</td><td style=\"text-align: right;\">1720000</td><td style=\"text-align: right;\">0.157135</td><td style=\"text-align: right;\">             8.74836</td><td style=\"text-align: right;\">             -9.0249</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1724000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.748364686965942\n",
      "  episode_reward_mean: -0.08298724874854088\n",
      "  episode_reward_min: -9.024903506040573\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8620\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.1589253776816912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015966665104060706\n",
      "          policy_loss: -0.11484311813518645\n",
      "          total_loss: 1.038826866005297\n",
      "          vf_explained_var: 0.9405196444321704\n",
      "          vf_loss: 1.1172959313117048\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1724000\n",
      "    num_agent_steps_trained: 1724000\n",
      "    num_steps_sampled: 1724000\n",
      "    num_steps_trained: 1724000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.618181818181817\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10050241124769244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08870216060694393\n",
      "    mean_inference_ms: 0.9877301634836733\n",
      "    mean_raw_obs_processing_ms: 0.0833142489538392\n",
      "  time_since_restore: 3661.885827064514\n",
      "  time_this_iter_s: 7.8067803382873535\n",
      "  time_total_s: 3661.885827064514\n",
      "  timers:\n",
      "    learn_throughput: 719.574\n",
      "    learn_time_ms: 5558.841\n",
      "    load_throughput: 13016693.304\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 506.552\n",
      "    sample_time_ms: 7896.528\n",
      "    update_time_ms: 1.335\n",
      "  timestamp: 1643387384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1724000\n",
      "  training_iteration: 431\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:45 (running for 01:01:17.34)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         3661.89</td><td style=\"text-align: right;\">1724000</td><td style=\"text-align: right;\">-0.0829872</td><td style=\"text-align: right;\">             8.74836</td><td style=\"text-align: right;\">             -9.0249</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:50 (running for 01:01:22.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         3661.89</td><td style=\"text-align: right;\">1724000</td><td style=\"text-align: right;\">-0.0829872</td><td style=\"text-align: right;\">             8.74836</td><td style=\"text-align: right;\">             -9.0249</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-29-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.748364686965942\n",
      "  episode_reward_mean: -0.000773073136806488\n",
      "  episode_reward_min: -10.730199635028839\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8640\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.281063655948126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01577896582293331\n",
      "          policy_loss: -0.11223432723800802\n",
      "          total_loss: 2.192134979486175\n",
      "          vf_explained_var: 0.899536862873262\n",
      "          vf_loss: 2.2684228637446\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1728000\n",
      "    num_agent_steps_trained: 1728000\n",
      "    num_steps_sampled: 1728000\n",
      "    num_steps_trained: 1728000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.563636363636363\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10049070856143441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08869334987702367\n",
      "    mean_inference_ms: 0.9876402143557325\n",
      "    mean_raw_obs_processing_ms: 0.08330383900573456\n",
      "  time_since_restore: 3669.563333272934\n",
      "  time_this_iter_s: 7.6775062084198\n",
      "  time_total_s: 3669.563333272934\n",
      "  timers:\n",
      "    learn_throughput: 718.946\n",
      "    learn_time_ms: 5563.697\n",
      "    load_throughput: 12865963.19\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 504.943\n",
      "    sample_time_ms: 7921.686\n",
      "    update_time_ms: 1.357\n",
      "  timestamp: 1643387392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1728000\n",
      "  training_iteration: 432\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:29:56 (running for 01:01:28.05)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         3669.56</td><td style=\"text-align: right;\">1728000</td><td style=\"text-align: right;\">-0.000773073</td><td style=\"text-align: right;\">             8.74836</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1732000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.748364686965942\n",
      "  episode_reward_mean: 0.016755200326442718\n",
      "  episode_reward_min: -10.730199635028839\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8660\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3921583839001195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015266237301961492\n",
      "          policy_loss: -0.08699721468572495\n",
      "          total_loss: 1.428363469198987\n",
      "          vf_explained_var: 0.9451104149382602\n",
      "          vf_loss: 1.4805822879196175\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1732000\n",
      "    num_agent_steps_trained: 1732000\n",
      "    num_steps_sampled: 1732000\n",
      "    num_steps_trained: 1732000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.016666666666666\n",
      "    ram_util_percent: 34.15000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10048434262014563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08868902902230999\n",
      "    mean_inference_ms: 0.9876088347578524\n",
      "    mean_raw_obs_processing_ms: 0.08329986946160792\n",
      "  time_since_restore: 3677.6548035144806\n",
      "  time_this_iter_s: 8.09147024154663\n",
      "  time_total_s: 3677.6548035144806\n",
      "  timers:\n",
      "    learn_throughput: 715.912\n",
      "    learn_time_ms: 5587.279\n",
      "    load_throughput: 12631543.442\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 502.163\n",
      "    sample_time_ms: 7965.547\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1643387400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1732000\n",
      "  training_iteration: 433\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:01 (running for 01:01:33.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         3677.65</td><td style=\"text-align: right;\">1732000</td><td style=\"text-align: right;\">0.0167552</td><td style=\"text-align: right;\">             8.74836</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:06 (running for 01:01:38.16)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         3677.65</td><td style=\"text-align: right;\">1732000</td><td style=\"text-align: right;\">0.0167552</td><td style=\"text-align: right;\">             8.74836</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.116976946592331\n",
      "  episode_reward_mean: -0.03406418174505234\n",
      "  episode_reward_min: -10.730199635028839\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8680\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3167279987565932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016134498243081392\n",
      "          policy_loss: -0.11957056341612692\n",
      "          total_loss: 1.0317382469465355\n",
      "          vf_explained_var: 0.9429346122408426\n",
      "          vf_loss: 1.1145524052682743\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1736000\n",
      "    num_agent_steps_trained: 1736000\n",
      "    num_steps_sampled: 1736000\n",
      "    num_steps_trained: 1736000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.441666666666666\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10046220308571367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08866875803156987\n",
      "    mean_inference_ms: 0.9873889481122805\n",
      "    mean_raw_obs_processing_ms: 0.08328141072266539\n",
      "  time_since_restore: 3685.852056980133\n",
      "  time_this_iter_s: 8.197253465652466\n",
      "  time_total_s: 3685.852056980133\n",
      "  timers:\n",
      "    learn_throughput: 708.183\n",
      "    learn_time_ms: 5648.255\n",
      "    load_throughput: 12555916.779\n",
      "    load_time_ms: 0.319\n",
      "    sample_throughput: 499.213\n",
      "    sample_time_ms: 8012.615\n",
      "    update_time_ms: 1.423\n",
      "  timestamp: 1643387408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1736000\n",
      "  training_iteration: 434\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:11 (running for 01:01:43.38)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         3685.85</td><td style=\"text-align: right;\">1736000</td><td style=\"text-align: right;\">-0.0340642</td><td style=\"text-align: right;\">             7.11698</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:16 (running for 01:01:48.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         3685.85</td><td style=\"text-align: right;\">1736000</td><td style=\"text-align: right;\">-0.0340642</td><td style=\"text-align: right;\">             7.11698</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.116976946592331\n",
      "  episode_reward_mean: -0.02960410952568054\n",
      "  episode_reward_min: -10.730199635028839\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8700\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3404297690237723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016188659383140798\n",
      "          policy_loss: -0.10630346513253146\n",
      "          total_loss: 1.1280010868061674\n",
      "          vf_explained_var: 0.9488866780393868\n",
      "          vf_loss: 1.1974247640259164\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1740000\n",
      "    num_agent_steps_trained: 1740000\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.045454545454543\n",
      "    ram_util_percent: 34.10909090909092\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10044532598991768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08865468024328296\n",
      "    mean_inference_ms: 0.9872275188857913\n",
      "    mean_raw_obs_processing_ms: 0.08326890399176373\n",
      "  time_since_restore: 3694.057203769684\n",
      "  time_this_iter_s: 8.205146789550781\n",
      "  time_total_s: 3694.057203769684\n",
      "  timers:\n",
      "    learn_throughput: 704.937\n",
      "    learn_time_ms: 5674.269\n",
      "    load_throughput: 12436779.837\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 493.343\n",
      "    sample_time_ms: 8107.942\n",
      "    update_time_ms: 1.433\n",
      "  timestamp: 1643387416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 435\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:21 (running for 01:01:53.61)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         3694.06</td><td style=\"text-align: right;\">1740000</td><td style=\"text-align: right;\">-0.0296041</td><td style=\"text-align: right;\">             7.11698</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.116976946592331\n",
      "  episode_reward_mean: -0.0040411436557769775\n",
      "  episode_reward_min: -10.730199635028839\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8720\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3067817060537237\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014968199002300288\n",
      "          policy_loss: -0.08238501508409778\n",
      "          total_loss: 1.2778077318808765\n",
      "          vf_explained_var: 0.9440248699598415\n",
      "          vf_loss: 1.3260933334708855\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1744000\n",
      "    num_agent_steps_trained: 1744000\n",
      "    num_steps_sampled: 1744000\n",
      "    num_steps_trained: 1744000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.733333333333338\n",
      "    ram_util_percent: 34.18333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1004306443981093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08864366729191067\n",
      "    mean_inference_ms: 0.9871031280988474\n",
      "    mean_raw_obs_processing_ms: 0.08326007586536073\n",
      "  time_since_restore: 3702.224689245224\n",
      "  time_this_iter_s: 8.167485475540161\n",
      "  time_total_s: 3702.224689245224\n",
      "  timers:\n",
      "    learn_throughput: 699.984\n",
      "    learn_time_ms: 5714.418\n",
      "    load_throughput: 12402761.884\n",
      "    load_time_ms: 0.323\n",
      "    sample_throughput: 490.089\n",
      "    sample_time_ms: 8161.782\n",
      "    update_time_ms: 1.443\n",
      "  timestamp: 1643387425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1744000\n",
      "  training_iteration: 436\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:27 (running for 01:01:58.80)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         3702.22</td><td style=\"text-align: right;\">1744000</td><td style=\"text-align: right;\">-0.00404114</td><td style=\"text-align: right;\">             7.11698</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:32 (running for 01:02:03.80)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         3702.22</td><td style=\"text-align: right;\">1744000</td><td style=\"text-align: right;\">-0.00404114</td><td style=\"text-align: right;\">             7.11698</td><td style=\"text-align: right;\">            -10.7302</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1748000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.116976946592331\n",
      "  episode_reward_mean: -0.0414775513112545\n",
      "  episode_reward_min: -8.097986370325089\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8740\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3224712178271303\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01589977370790543\n",
      "          policy_loss: -0.11230910192894679\n",
      "          total_loss: 0.640978722890178\n",
      "          vf_explained_var: 0.9601601482078593\n",
      "          vf_loss: 0.7170661550215496\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1748000\n",
      "    num_agent_steps_trained: 1748000\n",
      "    num_steps_sampled: 1748000\n",
      "    num_steps_trained: 1748000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.058333333333334\n",
      "    ram_util_percent: 34.14166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1004211893888338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08863720746403882\n",
      "    mean_inference_ms: 0.9870359915872843\n",
      "    mean_raw_obs_processing_ms: 0.08325490745090076\n",
      "  time_since_restore: 3710.348675966263\n",
      "  time_this_iter_s: 8.123986721038818\n",
      "  time_total_s: 3710.348675966263\n",
      "  timers:\n",
      "    learn_throughput: 695.5\n",
      "    learn_time_ms: 5751.26\n",
      "    load_throughput: 12255983.636\n",
      "    load_time_ms: 0.326\n",
      "    sample_throughput: 486.436\n",
      "    sample_time_ms: 8223.069\n",
      "    update_time_ms: 1.417\n",
      "  timestamp: 1643387433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1748000\n",
      "  training_iteration: 437\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:37 (running for 01:02:08.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         3710.35</td><td style=\"text-align: right;\">1748000</td><td style=\"text-align: right;\">-0.0414776</td><td style=\"text-align: right;\">             7.11698</td><td style=\"text-align: right;\">            -8.09799</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.50325033068657\n",
      "  episode_reward_mean: -0.044602733254432675\n",
      "  episode_reward_min: -6.382707267999649\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8760\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3548978095413535\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01608878363328349\n",
      "          policy_loss: -0.10622276840631359\n",
      "          total_loss: 1.9662031262647361\n",
      "          vf_explained_var: 0.9096342714883948\n",
      "          vf_loss: 2.035773631097168\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1752000\n",
      "    num_agent_steps_trained: 1752000\n",
      "    num_steps_sampled: 1752000\n",
      "    num_steps_trained: 1752000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.316666666666666\n",
      "    ram_util_percent: 34.15\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10041352970540178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08863350392754796\n",
      "    mean_inference_ms: 0.986991483036654\n",
      "    mean_raw_obs_processing_ms: 0.08324868591236617\n",
      "  time_since_restore: 3718.628150701523\n",
      "  time_this_iter_s: 8.27947473526001\n",
      "  time_total_s: 3718.628150701523\n",
      "  timers:\n",
      "    learn_throughput: 703.882\n",
      "    learn_time_ms: 5682.767\n",
      "    load_throughput: 12252403.418\n",
      "    load_time_ms: 0.326\n",
      "    sample_throughput: 481.684\n",
      "    sample_time_ms: 8304.194\n",
      "    update_time_ms: 1.357\n",
      "  timestamp: 1643387441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1752000\n",
      "  training_iteration: 438\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:42 (running for 01:02:14.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         3718.63</td><td style=\"text-align: right;\">1752000</td><td style=\"text-align: right;\">-0.0446027</td><td style=\"text-align: right;\">             8.50325</td><td style=\"text-align: right;\">            -6.38271</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:47 (running for 01:02:19.25)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         3718.63</td><td style=\"text-align: right;\">1752000</td><td style=\"text-align: right;\">-0.0446027</td><td style=\"text-align: right;\">             8.50325</td><td style=\"text-align: right;\">            -6.38271</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1756000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.50325033068657\n",
      "  episode_reward_mean: 0.019073317646980285\n",
      "  episode_reward_min: -7.010995507240295\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8780\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3182074507077535\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014937036347787595\n",
      "          policy_loss: -0.08753030222430024\n",
      "          total_loss: 1.7774183308077296\n",
      "          vf_explained_var: 0.9413775418394356\n",
      "          vf_loss: 1.8309201801656394\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1756000\n",
      "    num_agent_steps_trained: 1756000\n",
      "    num_steps_sampled: 1756000\n",
      "    num_steps_trained: 1756000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.200000000000003\n",
      "    ram_util_percent: 34.163636363636364\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10040282879129914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08862930986724044\n",
      "    mean_inference_ms: 0.9869369499497376\n",
      "    mean_raw_obs_processing_ms: 0.08324082028653561\n",
      "  time_since_restore: 3726.6002147197723\n",
      "  time_this_iter_s: 7.972064018249512\n",
      "  time_total_s: 3726.6002147197723\n",
      "  timers:\n",
      "    learn_throughput: 714.006\n",
      "    learn_time_ms: 5602.197\n",
      "    load_throughput: 13454062.55\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 492.343\n",
      "    sample_time_ms: 8124.411\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1643387449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1756000\n",
      "  training_iteration: 439\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:53 (running for 01:02:25.24)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">          3726.6</td><td style=\"text-align: right;\">1756000</td><td style=\"text-align: right;\">0.0190733</td><td style=\"text-align: right;\">             8.50325</td><td style=\"text-align: right;\">              -7.011</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-30-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.50325033068657\n",
      "  episode_reward_mean: -0.04660125315189362\n",
      "  episode_reward_min: -8.647316098213196\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8800\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.4205724477767945\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016027335404990854\n",
      "          policy_loss: -0.11665725300629293\n",
      "          total_loss: 2.13027824116345\n",
      "          vf_explained_var: 0.9102771652642117\n",
      "          vf_loss: 2.210423208660977\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1760000\n",
      "    num_agent_steps_trained: 1760000\n",
      "    num_steps_sampled: 1760000\n",
      "    num_steps_trained: 1760000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.625\n",
      "    ram_util_percent: 34.12500000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10039366676386749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08862526627391819\n",
      "    mean_inference_ms: 0.9868982726013548\n",
      "    mean_raw_obs_processing_ms: 0.08323365671057861\n",
      "  time_since_restore: 3734.7956850528717\n",
      "  time_this_iter_s: 8.195470333099365\n",
      "  time_total_s: 3734.7956850528717\n",
      "  timers:\n",
      "    learn_throughput: 713.033\n",
      "    learn_time_ms: 5609.836\n",
      "    load_throughput: 13140050.125\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 494.666\n",
      "    sample_time_ms: 8086.26\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1643387457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1760000\n",
      "  training_iteration: 440\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:30:58 (running for 01:02:30.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">          3734.8</td><td style=\"text-align: right;\">1760000</td><td style=\"text-align: right;\">-0.0466013</td><td style=\"text-align: right;\">             8.50325</td><td style=\"text-align: right;\">            -8.64732</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:03 (running for 01:02:35.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">          3734.8</td><td style=\"text-align: right;\">1760000</td><td style=\"text-align: right;\">-0.0466013</td><td style=\"text-align: right;\">             8.50325</td><td style=\"text-align: right;\">            -8.64732</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1764000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-31-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.50325033068657\n",
      "  episode_reward_mean: 0.06525974810123443\n",
      "  episode_reward_min: -8.647316098213196\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8820\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.4080200044057702\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01594278485735328\n",
      "          policy_loss: -0.09421783662125749\n",
      "          total_loss: 2.2909636975947008\n",
      "          vf_explained_var: 0.9004814221653887\n",
      "          vf_loss: 2.3488618820745457\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1764000\n",
      "    num_agent_steps_trained: 1764000\n",
      "    num_steps_sampled: 1764000\n",
      "    num_steps_trained: 1764000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.96363636363636\n",
      "    ram_util_percent: 34.163636363636364\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10038315232435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08862004851834954\n",
      "    mean_inference_ms: 0.9868296246787663\n",
      "    mean_raw_obs_processing_ms: 0.08322396083260573\n",
      "  time_since_restore: 3742.8227500915527\n",
      "  time_this_iter_s: 8.02706503868103\n",
      "  time_total_s: 3742.8227500915527\n",
      "  timers:\n",
      "    learn_throughput: 710.825\n",
      "    learn_time_ms: 5627.262\n",
      "    load_throughput: 13128739.338\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 493.939\n",
      "    sample_time_ms: 8098.171\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1643387465\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1764000\n",
      "  training_iteration: 441\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:08 (running for 01:02:40.50)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         3742.82</td><td style=\"text-align: right;\">1764000</td><td style=\"text-align: right;\">0.0652597</td><td style=\"text-align: right;\">             8.50325</td><td style=\"text-align: right;\">            -8.64732</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:13 (running for 01:02:45.51)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         3742.82</td><td style=\"text-align: right;\">1764000</td><td style=\"text-align: right;\">0.0652597</td><td style=\"text-align: right;\">             8.50325</td><td style=\"text-align: right;\">            -8.64732</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-31-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.206384599208832\n",
      "  episode_reward_mean: 0.07570643082261086\n",
      "  episode_reward_min: -15.104756772518158\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8840\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3118253261171362\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014725503810903325\n",
      "          policy_loss: -0.09641105602345158\n",
      "          total_loss: 3.273199524836845\n",
      "          vf_explained_var: 0.8732085882976491\n",
      "          vf_loss: 3.336064016298261\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1768000\n",
      "    num_agent_steps_trained: 1768000\n",
      "    num_steps_sampled: 1768000\n",
      "    num_steps_trained: 1768000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.950000000000003\n",
      "    ram_util_percent: 34.183333333333344\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1003726486369048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08861556877331882\n",
      "    mean_inference_ms: 0.986763096624424\n",
      "    mean_raw_obs_processing_ms: 0.08321479053932046\n",
      "  time_since_restore: 3751.066576719284\n",
      "  time_this_iter_s: 8.243826627731323\n",
      "  time_total_s: 3751.066576719284\n",
      "  timers:\n",
      "    learn_throughput: 707.876\n",
      "    learn_time_ms: 5650.706\n",
      "    load_throughput: 13268914.9\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 490.862\n",
      "    sample_time_ms: 8148.933\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1643387474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1768000\n",
      "  training_iteration: 442\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:19 (running for 01:02:50.77)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         3751.07</td><td style=\"text-align: right;\">1768000</td><td style=\"text-align: right;\">0.0757064</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1772000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.206384599208832\n",
      "  episode_reward_mean: -0.039328972399234774\n",
      "  episode_reward_min: -15.104756772518158\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8860\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3771127848215001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01772909931918796\n",
      "          policy_loss: -0.11509883432619034\n",
      "          total_loss: 1.949972380406063\n",
      "          vf_explained_var: 0.9133199095085103\n",
      "          vf_loss: 2.0246821049560784\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1772000\n",
      "    num_agent_steps_trained: 1772000\n",
      "    num_steps_sampled: 1772000\n",
      "    num_steps_trained: 1772000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.174999999999997\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10035755136265542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0886074915996304\n",
      "    mean_inference_ms: 0.9866418461840272\n",
      "    mean_raw_obs_processing_ms: 0.08320257455644624\n",
      "  time_since_restore: 3759.1857571601868\n",
      "  time_this_iter_s: 8.11918044090271\n",
      "  time_total_s: 3759.1857571601868\n",
      "  timers:\n",
      "    learn_throughput: 705.25\n",
      "    learn_time_ms: 5671.749\n",
      "    load_throughput: 13535470.754\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 490.539\n",
      "    sample_time_ms: 8154.291\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1643387482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1772000\n",
      "  training_iteration: 443\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:24 (running for 01:02:55.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         3759.19</td><td style=\"text-align: right;\">1772000</td><td style=\"text-align: right;\">-0.039329</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:29 (running for 01:03:00.92)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         3759.19</td><td style=\"text-align: right;\">1772000</td><td style=\"text-align: right;\">-0.039329</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.206384599208832\n",
      "  episode_reward_mean: -0.000991102159023285\n",
      "  episode_reward_min: -15.104756772518158\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8880\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3450135027849546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016812820912757334\n",
      "          policy_loss: -0.12144735914364617\n",
      "          total_loss: 1.4328727869489681\n",
      "          vf_explained_var: 0.9266292859149236\n",
      "          vf_loss: 1.5160184483294206\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1776000\n",
      "    num_agent_steps_trained: 1776000\n",
      "    num_steps_sampled: 1776000\n",
      "    num_steps_trained: 1776000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.383333333333333\n",
      "    ram_util_percent: 34.15833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1003460461781408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08860258084079999\n",
      "    mean_inference_ms: 0.9865531877491845\n",
      "    mean_raw_obs_processing_ms: 0.083194689630685\n",
      "  time_since_restore: 3767.5910120010376\n",
      "  time_this_iter_s: 8.40525484085083\n",
      "  time_total_s: 3767.5910120010376\n",
      "  timers:\n",
      "    learn_throughput: 704.281\n",
      "    learn_time_ms: 5679.553\n",
      "    load_throughput: 13543119.148\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 488.486\n",
      "    sample_time_ms: 8188.565\n",
      "    update_time_ms: 1.413\n",
      "  timestamp: 1643387490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1776000\n",
      "  training_iteration: 444\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:34 (running for 01:03:06.35)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">      reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         3767.59</td><td style=\"text-align: right;\">1776000</td><td style=\"text-align: right;\">-0.000991102</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.206384599208832\n",
      "  episode_reward_mean: 0.032140309661626815\n",
      "  episode_reward_min: -15.104756772518158\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8900\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.4109362873979794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015468191639058852\n",
      "          policy_loss: -0.09092303336728164\n",
      "          total_loss: 1.6201268246317262\n",
      "          vf_explained_var: 0.9067900276953175\n",
      "          vf_loss: 1.6758113866451607\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1780000\n",
      "    num_agent_steps_trained: 1780000\n",
      "    num_steps_sampled: 1780000\n",
      "    num_steps_trained: 1780000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.200000000000003\n",
      "    ram_util_percent: 34.12727272727273\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10033533913751887\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08859812821305642\n",
      "    mean_inference_ms: 0.9864669562589424\n",
      "    mean_raw_obs_processing_ms: 0.08318527501734126\n",
      "  time_since_restore: 3775.8492827415466\n",
      "  time_this_iter_s: 8.258270740509033\n",
      "  time_total_s: 3775.8492827415466\n",
      "  timers:\n",
      "    learn_throughput: 704.778\n",
      "    learn_time_ms: 5675.543\n",
      "    load_throughput: 13387500.798\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 487.434\n",
      "    sample_time_ms: 8206.24\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1643387498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1780000\n",
      "  training_iteration: 445\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:39 (running for 01:03:11.63)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         3775.85</td><td style=\"text-align: right;\">1780000</td><td style=\"text-align: right;\">0.0321403</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:44 (running for 01:03:16.63)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         3775.85</td><td style=\"text-align: right;\">1780000</td><td style=\"text-align: right;\">0.0321403</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-31-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.206384599208832\n",
      "  episode_reward_mean: -0.018206699192523955\n",
      "  episode_reward_min: -15.104756772518158\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8920\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.304392679916915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015745517693496563\n",
      "          policy_loss: -0.08296991504919064\n",
      "          total_loss: 1.0666023464943533\n",
      "          vf_explained_var: 0.9308660309160909\n",
      "          vf_loss: 1.11370199756116\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1784000\n",
      "    num_agent_steps_trained: 1784000\n",
      "    num_steps_sampled: 1784000\n",
      "    num_steps_trained: 1784000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.575\n",
      "    ram_util_percent: 34.12500000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10032213741761638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08859360871230512\n",
      "    mean_inference_ms: 0.986375785638711\n",
      "    mean_raw_obs_processing_ms: 0.083176426713579\n",
      "  time_since_restore: 3784.0927357673645\n",
      "  time_this_iter_s: 8.243453025817871\n",
      "  time_total_s: 3784.0927357673645\n",
      "  timers:\n",
      "    learn_throughput: 701.503\n",
      "    learn_time_ms: 5702.039\n",
      "    load_throughput: 13416406.238\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 488.812\n",
      "    sample_time_ms: 8183.113\n",
      "    update_time_ms: 1.451\n",
      "  timestamp: 1643387507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1784000\n",
      "  training_iteration: 446\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:50 (running for 01:03:21.89)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         3784.09</td><td style=\"text-align: right;\">1784000</td><td style=\"text-align: right;\">-0.0182067</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:31:55 (running for 01:03:26.90)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         3784.09</td><td style=\"text-align: right;\">1784000</td><td style=\"text-align: right;\">-0.0182067</td><td style=\"text-align: right;\">             15.2064</td><td style=\"text-align: right;\">            -15.1048</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1788000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-31-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.084215804934502\n",
      "  episode_reward_mean: 0.0024139419198036193\n",
      "  episode_reward_min: -7.843338891863823\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8940\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.2964329567006838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015542745805830387\n",
      "          policy_loss: -0.10162628754893298\n",
      "          total_loss: 1.6902548947218825\n",
      "          vf_explained_var: 0.918945901432345\n",
      "          vf_loss: 1.7564728593593963\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1788000\n",
      "    num_agent_steps_trained: 1788000\n",
      "    num_steps_sampled: 1788000\n",
      "    num_steps_trained: 1788000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.28333333333333\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1003061483523615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08858480276518783\n",
      "    mean_inference_ms: 0.9862451053976892\n",
      "    mean_raw_obs_processing_ms: 0.08316467567806056\n",
      "  time_since_restore: 3792.1899251937866\n",
      "  time_this_iter_s: 8.09718942642212\n",
      "  time_total_s: 3792.1899251937866\n",
      "  timers:\n",
      "    learn_throughput: 699.208\n",
      "    learn_time_ms: 5720.761\n",
      "    load_throughput: 13600207.523\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 488.511\n",
      "    sample_time_ms: 8188.149\n",
      "    update_time_ms: 1.541\n",
      "  timestamp: 1643387515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1788000\n",
      "  training_iteration: 447\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:00 (running for 01:03:32.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         3792.19</td><td style=\"text-align: right;\">1788000</td><td style=\"text-align: right;\">0.00241394</td><td style=\"text-align: right;\">             8.08422</td><td style=\"text-align: right;\">            -7.84334</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-32-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.084215804934502\n",
      "  episode_reward_mean: -0.022674050629138947\n",
      "  episode_reward_min: -7.843338891863823\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8960\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.2993980035986952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015995275120062172\n",
      "          policy_loss: -0.12562526537365812\n",
      "          total_loss: 1.6742401513513878\n",
      "          vf_explained_var: 0.9188186443621112\n",
      "          vf_loss: 1.7634261800156485\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1792000\n",
      "    num_agent_steps_trained: 1792000\n",
      "    num_steps_sampled: 1792000\n",
      "    num_steps_trained: 1792000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.745454545454546\n",
      "    ram_util_percent: 34.145454545454555\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10029279958138348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08857638234960499\n",
      "    mean_inference_ms: 0.9861188378281699\n",
      "    mean_raw_obs_processing_ms: 0.08315444337038645\n",
      "  time_since_restore: 3800.1326353549957\n",
      "  time_this_iter_s: 7.9427101612091064\n",
      "  time_total_s: 3800.1326353549957\n",
      "  timers:\n",
      "    learn_throughput: 700.02\n",
      "    learn_time_ms: 5714.119\n",
      "    load_throughput: 13611241.279\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 488.951\n",
      "    sample_time_ms: 8180.771\n",
      "    update_time_ms: 1.536\n",
      "  timestamp: 1643387523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1792000\n",
      "  training_iteration: 448\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:06 (running for 01:03:37.98)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         3800.13</td><td style=\"text-align: right;\">1792000</td><td style=\"text-align: right;\">-0.0226741</td><td style=\"text-align: right;\">             8.08422</td><td style=\"text-align: right;\">            -7.84334</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1796000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.084215804934502\n",
      "  episode_reward_mean: 0.022730500400066377\n",
      "  episode_reward_min: -11.451268702745438\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8980\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.3143131280458102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015497270393467424\n",
      "          policy_loss: -0.09897495362862584\n",
      "          total_loss: 2.161275589278328\n",
      "          vf_explained_var: 0.9129726325952878\n",
      "          vf_loss: 2.224945817427129\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1796000\n",
      "    num_agent_steps_trained: 1796000\n",
      "    num_steps_sampled: 1796000\n",
      "    num_steps_trained: 1796000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.925\n",
      "    ram_util_percent: 34.18333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10027715057104078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08856628608386721\n",
      "    mean_inference_ms: 0.9859812045835464\n",
      "    mean_raw_obs_processing_ms: 0.08314293223894369\n",
      "  time_since_restore: 3808.0758340358734\n",
      "  time_this_iter_s: 7.9431986808776855\n",
      "  time_total_s: 3808.0758340358734\n",
      "  timers:\n",
      "    learn_throughput: 701.656\n",
      "    learn_time_ms: 5700.8\n",
      "    load_throughput: 13387500.798\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 488.714\n",
      "    sample_time_ms: 8184.754\n",
      "    update_time_ms: 1.533\n",
      "  timestamp: 1643387531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1796000\n",
      "  training_iteration: 449\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:12 (running for 01:03:43.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         3808.08</td><td style=\"text-align: right;\">1796000</td><td style=\"text-align: right;\">0.0227305</td><td style=\"text-align: right;\">             8.08422</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:17 (running for 01:03:48.95)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         3808.08</td><td style=\"text-align: right;\">1796000</td><td style=\"text-align: right;\">0.0227305</td><td style=\"text-align: right;\">             8.08422</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-32-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.079760983586311\n",
      "  episode_reward_mean: -0.047607646137475965\n",
      "  episode_reward_min: -11.451268702745438\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9000\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.4266666576426517\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01605751727568916\n",
      "          policy_loss: -0.11268982546383975\n",
      "          total_loss: 1.3241637326158604\n",
      "          vf_explained_var: 0.9346719325870596\n",
      "          vf_loss: 1.400272522914794\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1800000\n",
      "    num_agent_steps_trained: 1800000\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.963636363636365\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.100257900094024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08855327699727315\n",
      "    mean_inference_ms: 0.9858174134756783\n",
      "    mean_raw_obs_processing_ms: 0.08313414679961235\n",
      "  time_since_restore: 3816.045668363571\n",
      "  time_this_iter_s: 7.969834327697754\n",
      "  time_total_s: 3816.045668363571\n",
      "  timers:\n",
      "    learn_throughput: 702.832\n",
      "    learn_time_ms: 5691.261\n",
      "    load_throughput: 13647780.037\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 490.33\n",
      "    sample_time_ms: 8157.777\n",
      "    update_time_ms: 1.516\n",
      "  timestamp: 1643387539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 450\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:23 (running for 01:03:54.94)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         3816.05</td><td style=\"text-align: right;\">1800000</td><td style=\"text-align: right;\">-0.0476076</td><td style=\"text-align: right;\">             7.07976</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1804000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.079760983586311\n",
      "  episode_reward_mean: 0.05383119374513626\n",
      "  episode_reward_min: -11.451268702745438\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9020\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.4197415271753906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015963835282295575\n",
      "          policy_loss: -0.07742800814129652\n",
      "          total_loss: 1.305536514796537\n",
      "          vf_explained_var: 0.9337143753164558\n",
      "          vf_loss: 1.3465969084451597\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1804000\n",
      "    num_agent_steps_trained: 1804000\n",
      "    num_steps_sampled: 1804000\n",
      "    num_steps_trained: 1804000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.983333333333334\n",
      "    ram_util_percent: 34.13333333333335\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10024419149481027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08854413527991349\n",
      "    mean_inference_ms: 0.9857076802836926\n",
      "    mean_raw_obs_processing_ms: 0.08312780375149673\n",
      "  time_since_restore: 3824.386576652527\n",
      "  time_this_iter_s: 8.340908288955688\n",
      "  time_total_s: 3824.386576652527\n",
      "  timers:\n",
      "    learn_throughput: 702.37\n",
      "    learn_time_ms: 5695.007\n",
      "    load_throughput: 13520199.855\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 489.237\n",
      "    sample_time_ms: 8175.996\n",
      "    update_time_ms: 1.529\n",
      "  timestamp: 1643387547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1804000\n",
      "  training_iteration: 451\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:28 (running for 01:04:00.30)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         3824.39</td><td style=\"text-align: right;\">1804000</td><td style=\"text-align: right;\">0.0538312</td><td style=\"text-align: right;\">             7.07976</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:33 (running for 01:04:05.31)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         3824.39</td><td style=\"text-align: right;\">1804000</td><td style=\"text-align: right;\">0.0538312</td><td style=\"text-align: right;\">             7.07976</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1808000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-32-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.8300586342811584\n",
      "  episode_reward_mean: -0.020344754159450532\n",
      "  episode_reward_min: -11.451268702745438\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9040\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.4620099791916468\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016662925244148034\n",
      "          policy_loss: -0.11560471613881408\n",
      "          total_loss: 1.2545834936226608\n",
      "          vf_explained_var: 0.938700960464375\n",
      "          vf_loss: 1.3322279773892896\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1808000\n",
      "    num_agent_steps_trained: 1808000\n",
      "    num_steps_sampled: 1808000\n",
      "    num_steps_trained: 1808000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.575\n",
      "    ram_util_percent: 34.14166666666667\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10023450945378155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08853772155897704\n",
      "    mean_inference_ms: 0.9856280256377287\n",
      "    mean_raw_obs_processing_ms: 0.08312475913790114\n",
      "  time_since_restore: 3832.6770939826965\n",
      "  time_this_iter_s: 8.290517330169678\n",
      "  time_total_s: 3832.6770939826965\n",
      "  timers:\n",
      "    learn_throughput: 701.013\n",
      "    learn_time_ms: 5706.03\n",
      "    load_throughput: 13524559.452\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 489.369\n",
      "    sample_time_ms: 8173.798\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1643387555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1808000\n",
      "  training_iteration: 452\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:38 (running for 01:04:10.62)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         3832.68</td><td style=\"text-align: right;\">1808000</td><td style=\"text-align: right;\">-0.0203448</td><td style=\"text-align: right;\">             6.83006</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:43 (running for 01:04:15.63)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         3832.68</td><td style=\"text-align: right;\">1808000</td><td style=\"text-align: right;\">-0.0203448</td><td style=\"text-align: right;\">             6.83006</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1812000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.954054176807404\n",
      "  episode_reward_mean: 0.08512056291103363\n",
      "  episode_reward_min: -11.451268702745438\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9060\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.4870149303508062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015466868998397021\n",
      "          policy_loss: -0.09839656890109583\n",
      "          total_loss: 1.2865327962099635\n",
      "          vf_explained_var: 0.9363940410716559\n",
      "          vf_loss: 1.349693901864912\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1812000\n",
      "    num_agent_steps_trained: 1812000\n",
      "    num_steps_sampled: 1812000\n",
      "    num_steps_trained: 1812000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.816666666666674\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10022759286583231\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08853265488270229\n",
      "    mean_inference_ms: 0.9855885183069206\n",
      "    mean_raw_obs_processing_ms: 0.08312279061647124\n",
      "  time_since_restore: 3841.0569365024567\n",
      "  time_this_iter_s: 8.379842519760132\n",
      "  time_total_s: 3841.0569365024567\n",
      "  timers:\n",
      "    learn_throughput: 700.687\n",
      "    learn_time_ms: 5708.681\n",
      "    load_throughput: 13509313.149\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 487.321\n",
      "    sample_time_ms: 8208.145\n",
      "    update_time_ms: 1.538\n",
      "  timestamp: 1643387564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1812000\n",
      "  training_iteration: 453\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:49 (running for 01:04:21.02)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         3841.06</td><td style=\"text-align: right;\">1812000</td><td style=\"text-align: right;\">0.0851206</td><td style=\"text-align: right;\">             6.95405</td><td style=\"text-align: right;\">            -11.4513</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1816000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-32-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.954054176807404\n",
      "  episode_reward_mean: 0.00016921162605285645\n",
      "  episode_reward_min: -8.628533899784088\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9080\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.524365347047006\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01671998450177231\n",
      "          policy_loss: -0.10824427519754697\n",
      "          total_loss: 1.7221535411521902\n",
      "          vf_explained_var: 0.9213166545796138\n",
      "          vf_loss: 1.792307596537535\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1816000\n",
      "    num_agent_steps_trained: 1816000\n",
      "    num_steps_sampled: 1816000\n",
      "    num_steps_trained: 1816000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.433333333333334\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10022258060355974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08852787451467588\n",
      "    mean_inference_ms: 0.9855549323683335\n",
      "    mean_raw_obs_processing_ms: 0.08311983033266288\n",
      "  time_since_restore: 3849.407814025879\n",
      "  time_this_iter_s: 8.350877523422241\n",
      "  time_total_s: 3849.407814025879\n",
      "  timers:\n",
      "    learn_throughput: 700.659\n",
      "    learn_time_ms: 5708.912\n",
      "    load_throughput: 13360847.336\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 487.498\n",
      "    sample_time_ms: 8205.16\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1643387572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1816000\n",
      "  training_iteration: 454\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:54 (running for 01:04:26.39)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         3849.41</td><td style=\"text-align: right;\">1816000</td><td style=\"text-align: right;\">0.000169212</td><td style=\"text-align: right;\">             6.95405</td><td style=\"text-align: right;\">            -8.62853</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:32:59 (running for 01:04:31.40)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         3849.41</td><td style=\"text-align: right;\">1816000</td><td style=\"text-align: right;\">0.000169212</td><td style=\"text-align: right;\">             6.95405</td><td style=\"text-align: right;\">            -8.62853</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.914423510432243\n",
      "  episode_reward_mean: -0.012833453118801116\n",
      "  episode_reward_min: -8.628533899784088\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9100\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.5731205560827768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01603542428103342\n",
      "          policy_loss: -0.12156027361690518\n",
      "          total_loss: 2.047028575088423\n",
      "          vf_explained_var: 0.919971844778266\n",
      "          vf_loss: 2.132058153205341\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1820000\n",
      "    num_agent_steps_trained: 1820000\n",
      "    num_steps_sampled: 1820000\n",
      "    num_steps_trained: 1820000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.53333333333333\n",
      "    ram_util_percent: 34.10833333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10022199651151313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08852612214752023\n",
      "    mean_inference_ms: 0.9855525284517216\n",
      "    mean_raw_obs_processing_ms: 0.08311599010215476\n",
      "  time_since_restore: 3857.9065730571747\n",
      "  time_this_iter_s: 8.498759031295776\n",
      "  time_total_s: 3857.9065730571747\n",
      "  timers:\n",
      "    learn_throughput: 698.193\n",
      "    learn_time_ms: 5729.072\n",
      "    load_throughput: 13369364.89\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 487.326\n",
      "    sample_time_ms: 8208.053\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1643387581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1820000\n",
      "  training_iteration: 455\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:05 (running for 01:04:36.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         3857.91</td><td style=\"text-align: right;\">1820000</td><td style=\"text-align: right;\">-0.0128335</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.62853</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1824000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.914423510432243\n",
      "  episode_reward_mean: -0.04987922817468643\n",
      "  episode_reward_min: -8.628533899784088\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9120\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.5716709557399955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01563612109104165\n",
      "          policy_loss: -0.1130714454337634\n",
      "          total_loss: 1.8158923603884716\n",
      "          vf_explained_var: 0.9126416824197257\n",
      "          vf_loss: 1.8933427518894594\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1824000\n",
      "    num_agent_steps_trained: 1824000\n",
      "    num_steps_sampled: 1824000\n",
      "    num_steps_trained: 1824000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.458333333333332\n",
      "    ram_util_percent: 34.15\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10023027485495416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08852639250638196\n",
      "    mean_inference_ms: 0.9855834919445681\n",
      "    mean_raw_obs_processing_ms: 0.08311633113077466\n",
      "  time_since_restore: 3866.2845029830933\n",
      "  time_this_iter_s: 8.377929925918579\n",
      "  time_total_s: 3866.2845029830933\n",
      "  timers:\n",
      "    learn_throughput: 702.937\n",
      "    learn_time_ms: 5690.412\n",
      "    load_throughput: 13243776.445\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 483.087\n",
      "    sample_time_ms: 8280.075\n",
      "    update_time_ms: 1.476\n",
      "  timestamp: 1643387589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1824000\n",
      "  training_iteration: 456\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:10 (running for 01:04:42.31)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         3866.28</td><td style=\"text-align: right;\">1824000</td><td style=\"text-align: right;\">-0.0498792</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.62853</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:15 (running for 01:04:47.31)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         3866.28</td><td style=\"text-align: right;\">1824000</td><td style=\"text-align: right;\">-0.0498792</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.62853</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1828000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.914423510432243\n",
      "  episode_reward_mean: -0.03979647696018219\n",
      "  episode_reward_min: -8.628533899784088\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9140\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.5342075287654835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016255588498768216\n",
      "          policy_loss: -0.09484267437089515\n",
      "          total_loss: 1.7450215403811746\n",
      "          vf_explained_var: 0.915661862780971\n",
      "          vf_loss: 1.8028319658971923\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1828000\n",
      "    num_agent_steps_trained: 1828000\n",
      "    num_steps_sampled: 1828000\n",
      "    num_steps_trained: 1828000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.80909090909091\n",
      "    ram_util_percent: 34.163636363636364\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10023469308937397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08852510082005896\n",
      "    mean_inference_ms: 0.9855959153963528\n",
      "    mean_raw_obs_processing_ms: 0.08311441436121564\n",
      "  time_since_restore: 3874.2577905654907\n",
      "  time_this_iter_s: 7.973287582397461\n",
      "  time_total_s: 3874.2577905654907\n",
      "  timers:\n",
      "    learn_throughput: 705.302\n",
      "    learn_time_ms: 5671.328\n",
      "    load_throughput: 13266816.385\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 484.985\n",
      "    sample_time_ms: 8247.679\n",
      "    update_time_ms: 1.509\n",
      "  timestamp: 1643387597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1828000\n",
      "  training_iteration: 457\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:20 (running for 01:04:52.32)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         3874.26</td><td style=\"text-align: right;\">1828000</td><td style=\"text-align: right;\">-0.0397965</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.62853</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:25 (running for 01:04:57.33)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         3874.26</td><td style=\"text-align: right;\">1828000</td><td style=\"text-align: right;\">-0.0397965</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.62853</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1832000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.914423510432243\n",
      "  episode_reward_mean: -0.04548098921775818\n",
      "  episode_reward_min: -8.107294738292694\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9160\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.5373190968267378\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015651846265482224\n",
      "          policy_loss: -0.1266095002852781\n",
      "          total_loss: 1.8859587301895704\n",
      "          vf_explained_var: 0.9123553840703862\n",
      "          vf_loss: 1.9769113704962755\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1832000\n",
      "    num_agent_steps_trained: 1832000\n",
      "    num_steps_sampled: 1832000\n",
      "    num_steps_trained: 1832000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.150000000000002\n",
      "    ram_util_percent: 34.18333333333334\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10023603655234953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08852411776640107\n",
      "    mean_inference_ms: 0.9855807272845898\n",
      "    mean_raw_obs_processing_ms: 0.08311121133043527\n",
      "  time_since_restore: 3882.3976440429688\n",
      "  time_this_iter_s: 8.139853477478027\n",
      "  time_total_s: 3882.3976440429688\n",
      "  timers:\n",
      "    learn_throughput: 703.731\n",
      "    learn_time_ms: 5683.99\n",
      "    load_throughput: 13456220.725\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 485.607\n",
      "    sample_time_ms: 8237.113\n",
      "    update_time_ms: 1.523\n",
      "  timestamp: 1643387605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1832000\n",
      "  training_iteration: 458\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:30 (running for 01:05:02.49)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">          3882.4</td><td style=\"text-align: right;\">1832000</td><td style=\"text-align: right;\">-0.045481</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.10729</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1836000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.914423510432243\n",
      "  episode_reward_mean: -0.0015981790423393249\n",
      "  episode_reward_min: -8.107294738292694\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9180\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6005343139812511\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016637921622005936\n",
      "          policy_loss: -0.08874728078603424\n",
      "          total_loss: 1.6259315283886426\n",
      "          vf_explained_var: 0.9210693179279246\n",
      "          vf_loss: 1.67677554831069\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1836000\n",
      "    num_agent_steps_trained: 1836000\n",
      "    num_steps_sampled: 1836000\n",
      "    num_steps_trained: 1836000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.345454545454544\n",
      "    ram_util_percent: 34.145454545454555\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10023685051673865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08852203357877692\n",
      "    mean_inference_ms: 0.9855601406484784\n",
      "    mean_raw_obs_processing_ms: 0.08310768921219626\n",
      "  time_since_restore: 3890.4489753246307\n",
      "  time_this_iter_s: 8.051331281661987\n",
      "  time_total_s: 3890.4489753246307\n",
      "  timers:\n",
      "    learn_throughput: 702.412\n",
      "    learn_time_ms: 5694.667\n",
      "    load_throughput: 13532195.515\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 484.834\n",
      "    sample_time_ms: 8250.251\n",
      "    update_time_ms: 1.541\n",
      "  timestamp: 1643387613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1836000\n",
      "  training_iteration: 459\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:35 (running for 01:05:07.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         3890.45</td><td style=\"text-align: right;\">1836000</td><td style=\"text-align: right;\">-0.00159818</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.10729</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:40 (running for 01:05:12.56)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         3890.45</td><td style=\"text-align: right;\">1836000</td><td style=\"text-align: right;\">-0.00159818</td><td style=\"text-align: right;\">             8.91442</td><td style=\"text-align: right;\">            -8.10729</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.484098017215729\n",
      "  episode_reward_mean: 0.06729344844818115\n",
      "  episode_reward_min: -8.107294738292694\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9200\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.5881768144587034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016454051089066737\n",
      "          policy_loss: -0.11735830674929323\n",
      "          total_loss: 2.2193946186152678\n",
      "          vf_explained_var: 0.8888754517801346\n",
      "          vf_loss: 2.2992685337781267\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1840000\n",
      "    num_agent_steps_trained: 1840000\n",
      "    num_steps_sampled: 1840000\n",
      "    num_steps_trained: 1840000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.099999999999998\n",
      "    ram_util_percent: 34.18333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10023435931064009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08851899053902489\n",
      "    mean_inference_ms: 0.9854999036142861\n",
      "    mean_raw_obs_processing_ms: 0.08310015172044567\n",
      "  time_since_restore: 3898.46492934227\n",
      "  time_this_iter_s: 8.01595401763916\n",
      "  time_total_s: 3898.46492934227\n",
      "  timers:\n",
      "    learn_throughput: 701.207\n",
      "    learn_time_ms: 5704.451\n",
      "    load_throughput: 13495186.615\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 484.504\n",
      "    sample_time_ms: 8255.867\n",
      "    update_time_ms: 1.576\n",
      "  timestamp: 1643387621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1840000\n",
      "  training_iteration: 460\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:45 (running for 01:05:17.59)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         3898.46</td><td style=\"text-align: right;\">1840000</td><td style=\"text-align: right;\">0.0672934</td><td style=\"text-align: right;\">              8.4841</td><td style=\"text-align: right;\">            -8.10729</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1844000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.484098017215729\n",
      "  episode_reward_mean: -0.043975749611854555\n",
      "  episode_reward_min: -8.107294738292694\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9220\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.5251949053938672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015846141664342943\n",
      "          policy_loss: -0.10025256865117098\n",
      "          total_loss: 1.2826575839626653\n",
      "          vf_explained_var: 0.9244467406503616\n",
      "          vf_loss: 1.3468106586726443\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1844000\n",
      "    num_agent_steps_trained: 1844000\n",
      "    num_steps_sampled: 1844000\n",
      "    num_steps_trained: 1844000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.172727272727272\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10021865653665568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08850946899557131\n",
      "    mean_inference_ms: 0.9853596057955943\n",
      "    mean_raw_obs_processing_ms: 0.08308550872456193\n",
      "  time_since_restore: 3906.2915461063385\n",
      "  time_this_iter_s: 7.8266167640686035\n",
      "  time_total_s: 3906.2915461063385\n",
      "  timers:\n",
      "    learn_throughput: 703.98\n",
      "    learn_time_ms: 5681.982\n",
      "    load_throughput: 13673362.673\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 485.63\n",
      "    sample_time_ms: 8236.721\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1643387629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1844000\n",
      "  training_iteration: 461\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:51 (running for 01:05:23.44)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         3906.29</td><td style=\"text-align: right;\">1844000</td><td style=\"text-align: right;\">-0.0439757</td><td style=\"text-align: right;\">              8.4841</td><td style=\"text-align: right;\">            -8.10729</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:33:56 (running for 01:05:28.45)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         3906.29</td><td style=\"text-align: right;\">1844000</td><td style=\"text-align: right;\">-0.0439757</td><td style=\"text-align: right;\">              8.4841</td><td style=\"text-align: right;\">            -8.10729</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1848000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-33-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.484098017215729\n",
      "  episode_reward_mean: 0.005780395567417145\n",
      "  episode_reward_min: -8.245839983224869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9240\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.5747852849703963\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016881669683994575\n",
      "          policy_loss: -0.1250325858295064\n",
      "          total_loss: 2.323071300140607\n",
      "          vf_explained_var: 0.8900373497957824\n",
      "          vf_loss: 2.4096453407679195\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1848000\n",
      "    num_agent_steps_trained: 1848000\n",
      "    num_steps_sampled: 1848000\n",
      "    num_steps_trained: 1848000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.427272727272726\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10020515512405127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08850225768539112\n",
      "    mean_inference_ms: 0.9852361095029106\n",
      "    mean_raw_obs_processing_ms: 0.08307262045225672\n",
      "  time_since_restore: 3914.202735185623\n",
      "  time_this_iter_s: 7.911189079284668\n",
      "  time_total_s: 3914.202735185623\n",
      "  timers:\n",
      "    learn_throughput: 708.662\n",
      "    learn_time_ms: 5644.436\n",
      "    load_throughput: 13571603.3\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 487.025\n",
      "    sample_time_ms: 8213.137\n",
      "    update_time_ms: 1.532\n",
      "  timestamp: 1643387637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1848000\n",
      "  training_iteration: 462\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:02 (running for 01:05:34.37)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">          3914.2</td><td style=\"text-align: right;\">1848000</td><td style=\"text-align: right;\">0.0057804</td><td style=\"text-align: right;\">              8.4841</td><td style=\"text-align: right;\">            -8.24584</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1852000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-34-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.596116825938225\n",
      "  episode_reward_mean: 0.09200167998671532\n",
      "  episode_reward_min: -8.245839983224869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9260\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.609886230832787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01504512965868367\n",
      "          policy_loss: -0.08967562600424493\n",
      "          total_loss: 1.8885248602950013\n",
      "          vf_explained_var: 0.926120610326849\n",
      "          vf_loss: 1.9439257989166885\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1852000\n",
      "    num_agent_steps_trained: 1852000\n",
      "    num_steps_sampled: 1852000\n",
      "    num_steps_trained: 1852000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.349999999999998\n",
      "    ram_util_percent: 34.15\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10019203063775704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0884964576713514\n",
      "    mean_inference_ms: 0.9851137779665322\n",
      "    mean_raw_obs_processing_ms: 0.0830608278452236\n",
      "  time_since_restore: 3922.228618621826\n",
      "  time_this_iter_s: 8.025883436203003\n",
      "  time_total_s: 3922.228618621826\n",
      "  timers:\n",
      "    learn_throughput: 711.645\n",
      "    learn_time_ms: 5620.782\n",
      "    load_throughput: 13570505.541\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 489.977\n",
      "    sample_time_ms: 8163.649\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1643387645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1852000\n",
      "  training_iteration: 463\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:07 (running for 01:05:39.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         3922.23</td><td style=\"text-align: right;\">1852000</td><td style=\"text-align: right;\">0.0920017</td><td style=\"text-align: right;\">             8.59612</td><td style=\"text-align: right;\">            -8.24584</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:12 (running for 01:05:44.42)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         3922.23</td><td style=\"text-align: right;\">1852000</td><td style=\"text-align: right;\">0.0920017</td><td style=\"text-align: right;\">             8.59612</td><td style=\"text-align: right;\">            -8.24584</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1856000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-34-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.596116825938225\n",
      "  episode_reward_mean: 0.020713650435209275\n",
      "  episode_reward_min: -8.245839983224869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9280\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6523627778535248\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01629113427930561\n",
      "          policy_loss: -0.09125328600927386\n",
      "          total_loss: 1.5290249020281819\n",
      "          vf_explained_var: 0.9296799355937588\n",
      "          vf_loss: 1.583164955174891\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1856000\n",
      "    num_agent_steps_trained: 1856000\n",
      "    num_steps_sampled: 1856000\n",
      "    num_steps_trained: 1856000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.358333333333334\n",
      "    ram_util_percent: 34.150000000000006\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10018159259836967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08849331435511917\n",
      "    mean_inference_ms: 0.9850178061371488\n",
      "    mean_raw_obs_processing_ms: 0.08305152913279619\n",
      "  time_since_restore: 3930.7014141082764\n",
      "  time_this_iter_s: 8.472795486450195\n",
      "  time_total_s: 3930.7014141082764\n",
      "  timers:\n",
      "    learn_throughput: 711.604\n",
      "    learn_time_ms: 5621.102\n",
      "    load_throughput: 13741679.089\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 490.691\n",
      "    sample_time_ms: 8151.77\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1643387654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1856000\n",
      "  training_iteration: 464\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:18 (running for 01:05:49.91)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">          3930.7</td><td style=\"text-align: right;\">1856000</td><td style=\"text-align: right;\">0.0207137</td><td style=\"text-align: right;\">             8.59612</td><td style=\"text-align: right;\">            -8.24584</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-34-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.596116825938225\n",
      "  episode_reward_mean: -0.09070731312036515\n",
      "  episode_reward_min: -8.245839983224869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9300\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6121629539356437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016259293909992527\n",
      "          policy_loss: -0.11168808318193882\n",
      "          total_loss: 1.6621272885656204\n",
      "          vf_explained_var: 0.9164975346416555\n",
      "          vf_loss: 1.7367746734410845\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1860000\n",
      "    num_agent_steps_trained: 1860000\n",
      "    num_steps_sampled: 1860000\n",
      "    num_steps_trained: 1860000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.24166666666667\n",
      "    ram_util_percent: 34.16666666666668\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10017322440048738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.088489027635329\n",
      "    mean_inference_ms: 0.9849411497070016\n",
      "    mean_raw_obs_processing_ms: 0.08304280923758249\n",
      "  time_since_restore: 3938.9930624961853\n",
      "  time_this_iter_s: 8.291648387908936\n",
      "  time_total_s: 3938.9930624961853\n",
      "  timers:\n",
      "    learn_throughput: 712.541\n",
      "    learn_time_ms: 5613.71\n",
      "    load_throughput: 13718083.401\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 491.466\n",
      "    sample_time_ms: 8138.912\n",
      "    update_time_ms: 1.488\n",
      "  timestamp: 1643387662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1860000\n",
      "  training_iteration: 465\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:23 (running for 01:05:55.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         3938.99</td><td style=\"text-align: right;\">1860000</td><td style=\"text-align: right;\">-0.0907073</td><td style=\"text-align: right;\">             8.59612</td><td style=\"text-align: right;\">            -8.24584</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:28 (running for 01:06:00.23)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         3938.99</td><td style=\"text-align: right;\">1860000</td><td style=\"text-align: right;\">-0.0907073</td><td style=\"text-align: right;\">             8.59612</td><td style=\"text-align: right;\">            -8.24584</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1864000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-34-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.06854322552681\n",
      "  episode_reward_mean: 0.078008653819561\n",
      "  episode_reward_min: -12.045217663049698\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9320\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6390314470055283\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014914496250199478\n",
      "          policy_loss: -0.08573557916472876\n",
      "          total_loss: 2.2264030862118926\n",
      "          vf_explained_var: 0.9076707374665045\n",
      "          vf_loss: 2.2781615751004347\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1864000\n",
      "    num_agent_steps_trained: 1864000\n",
      "    num_steps_sampled: 1864000\n",
      "    num_steps_trained: 1864000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.845454545454544\n",
      "    ram_util_percent: 34.145454545454555\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10016787523488216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08848701346149293\n",
      "    mean_inference_ms: 0.9848955259969199\n",
      "    mean_raw_obs_processing_ms: 0.08303634701992264\n",
      "  time_since_restore: 3947.145619869232\n",
      "  time_this_iter_s: 8.152557373046875\n",
      "  time_total_s: 3947.145619869232\n",
      "  timers:\n",
      "    learn_throughput: 711.352\n",
      "    learn_time_ms: 5623.098\n",
      "    load_throughput: 13832315.937\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 493.831\n",
      "    sample_time_ms: 8099.935\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1643387670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1864000\n",
      "  training_iteration: 466\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:33 (running for 01:06:05.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         3947.15</td><td style=\"text-align: right;\">1864000</td><td style=\"text-align: right;\">0.0780087</td><td style=\"text-align: right;\">             9.06854</td><td style=\"text-align: right;\">            -12.0452</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:38 (running for 01:06:10.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         3947.15</td><td style=\"text-align: right;\">1864000</td><td style=\"text-align: right;\">0.0780087</td><td style=\"text-align: right;\">             9.06854</td><td style=\"text-align: right;\">            -12.0452</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1868000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.06854322552681\n",
      "  episode_reward_mean: 0.05534156806766987\n",
      "  episode_reward_min: -12.045217663049698\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9340\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6423981565301136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016217813001051032\n",
      "          policy_loss: -0.1081096541577129\n",
      "          total_loss: 1.5105479435354312\n",
      "          vf_explained_var: 0.9195245685115937\n",
      "          vf_loss: 1.581711394467982\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1868000\n",
      "    num_agent_steps_trained: 1868000\n",
      "    num_steps_sampled: 1868000\n",
      "    num_steps_trained: 1868000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.816666666666666\n",
      "    ram_util_percent: 34.224999999999994\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10016277773138953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08848434121974318\n",
      "    mean_inference_ms: 0.9848578840711458\n",
      "    mean_raw_obs_processing_ms: 0.08302982113373376\n",
      "  time_since_restore: 3955.312974691391\n",
      "  time_this_iter_s: 8.167354822158813\n",
      "  time_total_s: 3955.312974691391\n",
      "  timers:\n",
      "    learn_throughput: 710.772\n",
      "    learn_time_ms: 5627.68\n",
      "    load_throughput: 13806135.616\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 492.308\n",
      "    sample_time_ms: 8125.0\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1643387678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1868000\n",
      "  training_iteration: 467\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:43 (running for 01:06:15.60)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         3955.31</td><td style=\"text-align: right;\">1868000</td><td style=\"text-align: right;\">0.0553416</td><td style=\"text-align: right;\">             9.06854</td><td style=\"text-align: right;\">            -12.0452</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1872000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-34-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.06854322552681\n",
      "  episode_reward_mean: -0.0906521038711071\n",
      "  episode_reward_min: -12.045217663049698\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9360\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.685520202241918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01633245353067647\n",
      "          policy_loss: -0.09784669132918239\n",
      "          total_loss: 0.8754391388602114\n",
      "          vf_explained_var: 0.9536290933368027\n",
      "          vf_loss: 0.9360784570656476\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1872000\n",
      "    num_agent_steps_trained: 1872000\n",
      "    num_steps_sampled: 1872000\n",
      "    num_steps_trained: 1872000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.945454545454545\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10015510817528142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0884774787770828\n",
      "    mean_inference_ms: 0.984807791583751\n",
      "    mean_raw_obs_processing_ms: 0.08302342896553057\n",
      "  time_since_restore: 3963.172240257263\n",
      "  time_this_iter_s: 7.859265565872192\n",
      "  time_total_s: 3963.172240257263\n",
      "  timers:\n",
      "    learn_throughput: 713.251\n",
      "    learn_time_ms: 5608.127\n",
      "    load_throughput: 13716961.818\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 492.671\n",
      "    sample_time_ms: 8119.01\n",
      "    update_time_ms: 1.4\n",
      "  timestamp: 1643387686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1872000\n",
      "  training_iteration: 468\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:49 (running for 01:06:21.47)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">         3963.17</td><td style=\"text-align: right;\">1872000</td><td style=\"text-align: right;\">-0.0906521</td><td style=\"text-align: right;\">             9.06854</td><td style=\"text-align: right;\">            -12.0452</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1876000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-34-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.06854322552681\n",
      "  episode_reward_mean: -0.08449758723378181\n",
      "  episode_reward_min: -12.045217663049698\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9380\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.616562625797846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017476697451429303\n",
      "          policy_loss: -0.1072685109531527\n",
      "          total_loss: 2.2516313384156876\n",
      "          vf_explained_var: 0.8938677575639499\n",
      "          vf_loss: 2.319085756192605\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1876000\n",
      "    num_agent_steps_trained: 1876000\n",
      "    num_steps_sampled: 1876000\n",
      "    num_steps_trained: 1876000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.73333333333333\n",
      "    ram_util_percent: 34.23333333333333\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10014378478703534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08846715952182717\n",
      "    mean_inference_ms: 0.9847095577542976\n",
      "    mean_raw_obs_processing_ms: 0.0830129974693165\n",
      "  time_since_restore: 3971.105633497238\n",
      "  time_this_iter_s: 7.933393239974976\n",
      "  time_total_s: 3971.105633497238\n",
      "  timers:\n",
      "    learn_throughput: 713.118\n",
      "    learn_time_ms: 5609.168\n",
      "    load_throughput: 13890723.63\n",
      "    load_time_ms: 0.288\n",
      "    sample_throughput: 494.669\n",
      "    sample_time_ms: 8086.211\n",
      "    update_time_ms: 1.379\n",
      "  timestamp: 1643387694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1876000\n",
      "  training_iteration: 469\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:34:55 (running for 01:06:27.43)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         3971.11</td><td style=\"text-align: right;\">1876000</td><td style=\"text-align: right;\">-0.0844976</td><td style=\"text-align: right;\">             9.06854</td><td style=\"text-align: right;\">            -12.0452</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:35:00 (running for 01:06:32.43)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         3971.11</td><td style=\"text-align: right;\">1876000</td><td style=\"text-align: right;\">-0.0844976</td><td style=\"text-align: right;\">             9.06854</td><td style=\"text-align: right;\">            -12.0452</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.06854322552681\n",
      "  episode_reward_mean: 0.055592353940010074\n",
      "  episode_reward_min: -12.045217663049698\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9400\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6883932204656704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01591686061796438\n",
      "          policy_loss: -0.10437664179261573\n",
      "          total_loss: 1.1356056015900966\n",
      "          vf_explained_var: 0.9296813909084566\n",
      "          vf_loss: 1.2037216390573209\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1880000\n",
      "    num_agent_steps_trained: 1880000\n",
      "    num_steps_sampled: 1880000\n",
      "    num_steps_trained: 1880000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.163636363636364\n",
      "    ram_util_percent: 34.20909090909091\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1001267742483219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08845360132110304\n",
      "    mean_inference_ms: 0.9845667170583405\n",
      "    mean_raw_obs_processing_ms: 0.08300033100964672\n",
      "  time_since_restore: 3979.00977230072\n",
      "  time_this_iter_s: 7.904138803482056\n",
      "  time_total_s: 3979.00977230072\n",
      "  timers:\n",
      "    learn_throughput: 712.437\n",
      "    learn_time_ms: 5614.529\n",
      "    load_throughput: 14118670.369\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 495.63\n",
      "    sample_time_ms: 8070.538\n",
      "    update_time_ms: 1.356\n",
      "  timestamp: 1643387702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1880000\n",
      "  training_iteration: 470\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:35:06 (running for 01:06:38.36)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         3979.01</td><td style=\"text-align: right;\">1880000</td><td style=\"text-align: right;\">0.0555924</td><td style=\"text-align: right;\">             9.06854</td><td style=\"text-align: right;\">            -12.0452</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1884000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.84403258562088\n",
      "  episode_reward_mean: -0.0338434624671936\n",
      "  episode_reward_min: -7.6666562259197235\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9420\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6960400167331902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016203211534579782\n",
      "          policy_loss: -0.08775672593945137\n",
      "          total_loss: 2.068600517680107\n",
      "          vf_explained_var: 0.9136267410170648\n",
      "          vf_loss: 2.119444296605164\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1884000\n",
      "    num_agent_steps_trained: 1884000\n",
      "    num_steps_sampled: 1884000\n",
      "    num_steps_trained: 1884000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.245454545454546\n",
      "    ram_util_percent: 34.199999999999996\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10011048072497776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08844097208031908\n",
      "    mean_inference_ms: 0.9844322873798349\n",
      "    mean_raw_obs_processing_ms: 0.0829882432306207\n",
      "  time_since_restore: 3987.03590798378\n",
      "  time_this_iter_s: 8.026135683059692\n",
      "  time_total_s: 3987.03590798378\n",
      "  timers:\n",
      "    learn_throughput: 712.903\n",
      "    learn_time_ms: 5610.858\n",
      "    load_throughput: 14070124.119\n",
      "    load_time_ms: 0.284\n",
      "    sample_throughput: 493.807\n",
      "    sample_time_ms: 8100.327\n",
      "    update_time_ms: 1.373\n",
      "  timestamp: 1643387710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1884000\n",
      "  training_iteration: 471\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:35:11 (running for 01:06:43.40)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         3987.04</td><td style=\"text-align: right;\">1884000</td><td style=\"text-align: right;\">-0.0338435</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -7.66666</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:35:16 (running for 01:06:48.41)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         3987.04</td><td style=\"text-align: right;\">1884000</td><td style=\"text-align: right;\">-0.0338435</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -7.66666</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1888000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-35-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.84403258562088\n",
      "  episode_reward_mean: -0.017376407459378244\n",
      "  episode_reward_min: -7.6666562259197235\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9440\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6285607744288702\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01711815940291473\n",
      "          policy_loss: -0.1119421864329006\n",
      "          total_loss: 1.4652886703535313\n",
      "          vf_explained_var: 0.9301779704709207\n",
      "          vf_loss: 1.5382335439805062\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1888000\n",
      "    num_agent_steps_trained: 1888000\n",
      "    num_steps_sampled: 1888000\n",
      "    num_steps_trained: 1888000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.84166666666667\n",
      "    ram_util_percent: 34.175000000000004\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10009258988339974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08842686428636255\n",
      "    mean_inference_ms: 0.9842716562575284\n",
      "    mean_raw_obs_processing_ms: 0.08297436113664315\n",
      "  time_since_restore: 3995.0003588199615\n",
      "  time_this_iter_s: 7.964450836181641\n",
      "  time_total_s: 3995.0003588199615\n",
      "  timers:\n",
      "    learn_throughput: 710.82\n",
      "    learn_time_ms: 5627.3\n",
      "    load_throughput: 14178328.404\n",
      "    load_time_ms: 0.282\n",
      "    sample_throughput: 494.732\n",
      "    sample_time_ms: 8085.182\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1643387718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1888000\n",
      "  training_iteration: 472\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:35:22 (running for 01:06:54.40)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">            3995</td><td style=\"text-align: right;\">1888000</td><td style=\"text-align: right;\">-0.0173764</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -7.66666</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:35:30 (running for 01:07:01.95)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">            3995</td><td style=\"text-align: right;\">1888000</td><td style=\"text-align: right;\">-0.0173764</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -7.66666</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1892000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_17-35-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.84403258562088\n",
      "  episode_reward_mean: 0.023923683762550354\n",
      "  episode_reward_min: -8.505185380578041\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9460\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6149383025784647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01669875940631122\n",
      "          policy_loss: -0.12134248005446567\n",
      "          total_loss: 1.8860744394291873\n",
      "          vf_explained_var: 0.9006338413684599\n",
      "          vf_loss: 1.9693750539934762\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1892000\n",
      "    num_agent_steps_trained: 1892000\n",
      "    num_steps_sampled: 1892000\n",
      "    num_steps_trained: 1892000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.484615384615385\n",
      "    ram_util_percent: 34.08461538461539\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10007691851148745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08841481066605258\n",
      "    mean_inference_ms: 0.9841415029116308\n",
      "    mean_raw_obs_processing_ms: 0.08295989412629576\n",
      "  time_since_restore: 4007.2286064624786\n",
      "  time_this_iter_s: 12.22824764251709\n",
      "  time_total_s: 4007.2286064624786\n",
      "  timers:\n",
      "    learn_throughput: 661.995\n",
      "    learn_time_ms: 6042.338\n",
      "    load_throughput: 14217979.661\n",
      "    load_time_ms: 0.281\n",
      "    sample_throughput: 493.358\n",
      "    sample_time_ms: 8107.705\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1643387730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1892000\n",
      "  training_iteration: 473\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 17:35:51 (running for 01:07:22.98)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         4007.23</td><td style=\"text-align: right;\">1892000</td><td style=\"text-align: right;\">0.0239237</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -8.50519</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 20:45:14 (running for 04:16:46.51)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         4007.23</td><td style=\"text-align: right;\">1892000</td><td style=\"text-align: right;\">0.0239237</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -8.50519</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 20:45:19 (running for 04:16:51.53)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         4007.23</td><td style=\"text-align: right;\">1892000</td><td style=\"text-align: right;\">0.0239237</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -8.50519</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 20:45:24 (running for 04:16:56.54)<br>Memory usage on this node: 3.2/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         4007.23</td><td style=\"text-align: right;\">1892000</td><td style=\"text-align: right;\">0.0239237</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -8.50519</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-28 20:45:29 (running for 04:17:01.55)<br>Memory usage on this node: 3.3/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/4.51 GiB heap, 0.0/2.26 GiB objects<br>Result logdir: /home/michelangelo/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_gym_xymodel:xymodel-v0_f123e_00000</td><td>RUNNING </td><td>172.22.93.212:18857</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         4007.23</td><td style=\"text-align: right;\">1892000</td><td style=\"text-align: right;\">0.0239237</td><td style=\"text-align: right;\">             7.84403</td><td style=\"text-align: right;\">            -8.50519</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_gym_xymodel:xymodel-v0_f123e_00000:\n",
      "  agent_timesteps_total: 1896000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-28_20-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.187867879867554\n",
      "  episode_reward_mean: 0.08404054790735245\n",
      "  episode_reward_min: -8.505185380578041\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9480\n",
      "  experiment_id: f1f8f48c61354efaa2905ae54374083d\n",
      "  hostname: DESKTOP-HPOSGEH\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: -1.6133925646863958\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015728522433590447\n",
      "          policy_loss: -0.0793695379148728\n",
      "          total_loss: 2.1144186633989537\n",
      "          vf_explained_var: 0.9064997525625331\n",
      "          vf_loss: 2.1579566666275585\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1896000\n",
      "    num_agent_steps_trained: 1896000\n",
      "    num_steps_sampled: 1896000\n",
      "    num_steps_trained: 1896000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 172.22.93.212\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.5\n",
      "    ram_util_percent: 33.66071428571428\n",
      "  pid: 18857\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10009766965016759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08843632310996917\n",
      "    mean_inference_ms: 3.385844827308396\n",
      "    mean_raw_obs_processing_ms: 0.08297960619054326\n",
      "  time_since_restore: 15408.664625406265\n",
      "  time_this_iter_s: 11401.436018943787\n",
      "  time_total_s: 15408.664625406265\n",
      "  timers:\n",
      "    learn_throughput: 585.002\n",
      "    learn_time_ms: 6837.58\n",
      "    load_throughput: 13326885.376\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 371.094\n",
      "    sample_time_ms: 10778.929\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1643399132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1896000\n",
      "  training_iteration: 474\n",
      "  trial_id: f123e_00000\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "  \"env\": \"gym_xymodel:xymodel-v0\",\n",
    "  # Change the following line to `“framework”: “tf”` to use tensorflow\n",
    "  \"framework\": \"torch\",\n",
    "  # \"model\": {\n",
    "  #   \"fcnet_hiddens\": [32],\n",
    "  #   \"fcnet_activation\": \"linear\",\n",
    "  # },\n",
    "  \"model\" : {\n",
    "      # Use None for making RLlib try to find a default filter setup given the\n",
    "    # observation space.\n",
    "    \"conv_filters\": None,\n",
    "    # Activation function descriptor.\n",
    "    # Supported values are: \"tanh\", \"relu\", \"swish\" (or \"silu\"),\n",
    "    # \"linear\" (or None).\n",
    "    \"conv_activation\": \"relu\",\n",
    "  },\n",
    "  \"horizon\": 200,\n",
    "}\n",
    "stop = {\"episode_reward_mean\": 195}\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "  num_gpus=1,\n",
    "  num_cpus=4,\n",
    "  include_dashboard=False,\n",
    "  ignore_reinit_error=True,\n",
    "  log_to_driver=False,\n",
    ")\n",
    "# execute training \n",
    "analysis = ray.tune.run(\n",
    "  \"PPO\",\n",
    "  config=config,\n",
    "  stop=stop,\n",
    "  checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_search_config = {\n",
    "#     \"env\": \"CartPole-v0\",\n",
    "#     \"framework\": \"torch\",\n",
    "\n",
    "#     # Hyperparameter tuning\n",
    "#     \"model\": {\n",
    "#       \"fcnet_hiddens\": ray.tune.grid_search([[32], [64]]),\n",
    "#       \"fcnet_activation\": ray.tune.grid_search([\"linear\", \"relu\"]),\n",
    "#     },\n",
    "#     \"lr\": ray.tune.uniform(1e-7, 1e-2)\n",
    "# }\n",
    "\n",
    "# # To explicitly stop or restart Ray, use the shutdown API.\n",
    "# ray.shutdown()\n",
    "\n",
    "# ray.init(\n",
    "#   num_cpus=8,\n",
    "#   include_dashboard=False,\n",
    "#   ignore_reinit_error=True,\n",
    "#   log_to_driver=False,\n",
    "# )\n",
    "\n",
    "# parameter_search_analysis = ray.tune.run(\n",
    "#   \"PPO\",\n",
    "#   config=parameter_search_config,\n",
    "#   stop=stop,\n",
    "#   num_samples=5,\n",
    "#   metric=\"timesteps_total\",\n",
    "#   mode=\"min\",\n",
    "# )\n",
    "\n",
    "# print(\n",
    "#   \"Best hyperparameters found:\",\n",
    "#   parameter_search_analysis.best_config,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d2462a55c97c25b2d4d0163340a42414ea8606c78af340d95537bd72592005a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
